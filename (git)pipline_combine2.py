# -*- coding: utf-8 -*-
"""
[ìˆ˜ì •] Streamlit App v2.5 â€” TAB1 ê²Œì´ì§€ë°” ê°€ë¡œ ë°°ì¹˜, TAB4 ë‹¨ì–´ì€í–‰ Chi2ë¡œ ë³µì›
- TAB1: [ìˆ˜ì •] Plotly ê²Œì´ì§€ ì°¨íŠ¸(S/I)ë¥¼ st.columnsë¡œ ê°€ë¡œ ë°°ì¹˜, [ì‹ ê·œ] ëª©í‘œ ì ìˆ˜ í…ìŠ¤íŠ¸ë¡œ ê°•ì¡°
- TAB2: [ìˆ˜ì •] í† í”½ ë‹¨ì–´ ì€í–‰ ìƒì„± í•¨ìˆ˜ë¥¼ LogRegì—ì„œ ë‹¤ì‹œ Chi2ë¡œ ë³µì› (ìƒ˜í”Œ ë¶€ì¡± ë¬¸ì œ í•´ê²°)
- TAB4: [ìˆ˜ì •] ë‹¨ì–´ ì€í–‰ ì„¤ëª…ì„ Chi2 ê¸°ì¤€ìœ¼ë¡œ ë³µì›
"""

import os, io, json, time, re as regx
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px # ì‹œê°í™” ì¶”ê°€
import plotly.graph_objects as go # [ì‹ ê·œ] ê²Œì´ì§€ ì°¨íŠ¸ìš©
import datetime 
from typing import List, Dict, Tuple

# ==== OpenAI SDK ====
from openai import OpenAI
from openai import APIError, RateLimitError

# ==== ML / NLP ====
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.preprocessing import RobustScaler
from sklearn.utils import shuffle as sk_shuffle
from sklearn.feature_selection import chi2
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split

# ==== íšŒê·€ ====
import statsmodels.api as sm

# [ì‹ ê·œ] í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì •ì˜
STOPWORDS_KO = [
    # ì¡°ì‚¬/ì–´ë¯¸ (ë§¤ìš° ë¹ˆë²ˆ)
    "ì…ë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ê°™ìŠµë‹ˆë‹¤", "ìˆìŠµë‹ˆë‹¤", "ìˆëŠ”", "ê²ƒì…ë‹ˆë‹¤", "í–ˆë‹¤", "ë“±", "ì´", "ê·¸", "ì €",
    "ìˆ˜", "ê²ƒ", "ë°", "A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z",
    "ì—ì„œ", "ìœ¼ë¡œ", "í•˜ëŠ”", "ì…ë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "ì„", "ë¥¼", "ì€", "ëŠ”", "ì´", "ê°€", "ì˜", "ì—", "ì™€", "ê³¼", "ë„",
    
    # ì¼ë°˜ ëª…ì‚¬ (ì‹ í˜¸ ë°©í•´)
    "ë¸”ë¡œê·¸", "í¬ìŠ¤íŒ…", "ì˜¤ëŠ˜", "ì´ë²ˆ", "ë‹¤ì–‘í•œ", "ê´€ë ¨", "ë‚´ìš©", "ì •ë³´", "ì •ë§", "ë°”ë¡œ", "ì§€ê¸ˆ", "ìƒê°",
    "ê²½ìš°", "ëŒ€í•´", "ëŒ€ë¶€ë¶„", "ë•Œë¬¸", "ê´€ë ¨", "ê°€ì§€", "í†µí•´", "ìœ„í•´", "ëŒ€í•œ", "í†µí•œ", "ë”°ë¼",
    
    # ìŠ¤í¬ë¦°ìƒ·ì—ì„œ ë³´ì¸ ë¬¸ì œ ë‹¨ì–´ë“¤
    "2024", "2023", "ai", "2024ë…„", "2023ë…„", "ìì‚´", "ê·¹ìš°", "ã…‹ã…‹", "ã…ã…"
]


# ================== CONFIG ==================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
USE_LLM = len(OPENAI_API_KEY) > 0
client = OpenAI(api_key=OPENAI_API_KEY) if USE_LLM else None
MODEL_CHAT = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o-mini-2024-07-18")

LLM_OK = False
if USE_LLM and client:
    try:
        client.models.list()
        LLM_OK = True
    except Exception:
        LLM_OK = False

# ========= Robust CSV Loader =========
def read_csv_robust(src, **kwargs) -> pd.DataFrame:
    """UploadedFile/bytes/path/file-like ëª¨ë‘ ì§€ì›. ì¸ì½”ë”©ê³¼ êµ¬ë¶„ì ìë™ ì¬ì‹œë„."""
    encodings = ["utf-8", "utf-8-sig", "cp949", "euc-kr", "latin1"]
    seps = [None, ",", "\t", ";"]

    # bytesë¡œ ì•ˆì „ ë³µì‚¬
    if hasattr(src, "getvalue"):        # Streamlit UploadedFile
        raw = src.getvalue()
    elif isinstance(src, (bytes, bytearray)):
        raw = bytes(src)
    elif isinstance(src, str):          # ê²½ë¡œ
        with open(src, "rb") as f:
            raw = f.read()
    else:                               # file-like
        raw = src.read()
        try:
            src.seek(0)
        except Exception:
            pass

    last_err = None
    for enc in encodings:
        for sep in seps:
            try:
                return pd.read_csv(io.BytesIO(raw), encoding=enc, sep=sep, engine="python", **kwargs)
            except Exception as e:
                last_err = e
                continue
    raise RuntimeError(f"CSV ë””ì½”ë”© ì‹¤íŒ¨: ë§ˆì§€ë§‰ ì˜¤ë¥˜={last_err}")

# ========= JSON íŒŒì„œ =========
def _parse_json_safely(txt: str):
    """ì½”ë“œíœìŠ¤/ì•ë’¤ ì“°ë ˆê¸°/í•œê¸€ BOM ì œê±°, ì²« JSON ê°ì²´/ë°°ì—´ë§Œ íŒŒì‹±"""
    if not isinstance(txt, str):
        raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
    t = txt.strip().lstrip("\ufeff")
    if t.startswith("```"):
        parts = t.split("```")
        if len(parts) >= 3:
            cand = parts[1] if parts[1].strip().startswith(("{","[")) else parts[2]
            t = cand
        else:
            t = t.replace("```","").strip()
    si, sj = t.find("["), t.rfind("]")
    oi, oj = t.find("{"), t.rfind("}")
    if 0 <= si < sj:
        return json.loads(t[si:sj+1])
    if 0 <= oi < oj:
        return json.loads(t[oi:oj+1])
    raise ValueError("LLM ì‘ë‹µì—ì„œ JSONì„ ì°¾ì§€ ëª»í•¨")

# ====== Candidate templates (í´ë°±) ======
NUM_RE  = regx.compile(r"\b(\d+|top\s*\d+|[0-9]+ë¶„)\b", regx.I)
TIME_BANK = ["ì˜¤ëŠ˜", "ì´ë²ˆ ì£¼", "ì£¼ë§", "ì§€ê¸ˆ", "ë°©ê¸ˆ", "ì´ë²ˆ ë‹¬", "10ì›”", "11ì›”", "12ì›”"]
HOWTO_BANK = ["ë°©-step", "ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤"]
ACTION_BANK = ["ì •ë¦¬", "ë¹„êµ", "ë¶„ì„", "ì„¤ëª…", "ì¶”ì²œ", "ì ê²€", "ì‹¤ë²•", "ê°€ì´ë“œ", "ì²´í¬ë¦¬ìŠ¤íŠ¸", "íŠœí† ë¦¬ì–¼", "Step-byí—˜"]
CTA_BANK = ["ì§ˆë¬¸", "ëŒ“ê¸€", "êµ¬ë…", "ê³µìœ ", "ì•Œë¦¼", "ì°¸ì—¬"]
LIST_BANK = ["Top 5", "Top 7", "3ê°€ì§€", "5ë¶„ ìš”ì•½", "í•œëˆˆì—"]
BRAND_HINT = ["í•œì–‘ëŒ€", "ì˜¤í”ˆAI", "ì¹´ì¹´ì˜¤", "êµ¬ê¸€", "MS", "ë„¤ì´ë²„"]
DEFAULT_CANDIDATES = TIME_BANK + HOWTO_BANK + ACTION_BANK + CTA_BANK + LIST_BANK + BRAND_HINT

# ========= Utility =========
def categorize_term(t: str) -> str:
    t_low = t.lower()
    if NUM_RE.search(t_low) or any(x in t for x in LIST_BANK): return "ìˆ«ì/ë¦¬ìŠ¤íŠ¸"
    if any(k in t for k in TIME_BANK): return "ì‹œê°„í‘œí˜„"
    if any(k in t for k in HOWTO_BANK): return "How-to/ê°€ì´ë“œ"
    if any(k in t for k in CTA_BANK): return "ì§ˆë¬¸/CTA"
    if any(k in t for k in ACTION_BANK): return "í–‰ë™ë™ì‚¬/í–‰ìœ„"
    if regx.match(r"[A-Z][a-zA-Z0-9]+", t) or "ëŒ€" in t or "ëŒ€í•™" in t or any(b in t for b in BRAND_HINT):
        return "ê³ ìœ ëª…ì‚¬/ë¸Œëœë“œ"
    return "ê¸°íƒ€"

# ==== article_id ìŠ¤í‚¤ë§ˆ ë³´ì •(ìµœì†Œ íŒ¨ì¹˜) ====
_CTRL = regx.compile(r"[\x00-\x1f\x7f]")

def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ì»¬ëŸ¼ëª… BOM/ê³µë°± ì œê±° + ì†Œë¬¸ì + ìŠ¤í˜ì´ìŠ¤â†’ì–¸ë”ìŠ¤ì½”ì–´"""
    df = df.copy()
    df.columns = (
        pd.Index(df.columns)
        .map(lambda c: str(c).lstrip("\ufeff").strip().lower().replace(" ", "_"))
    )
    return df

def coerce_article_id(df: pd.DataFrame) -> pd.DataFrame:
    """
    - ì»¬ëŸ¼ëª… ì •ê·œí™”
    - article_id ë³„ì¹­(id, doc_id, post_id ë“±)ì„ article_idë¡œ í†µì¼
    - ê°’ì˜ BOM/ì œì–´ë¬¸ì/ê³µë°± ì œê±° + ë¬¸ìì—´í™”
    """
    df = _normalize_columns(df.copy())
    aliases = ["article_id", "id", "doc_id", "post_id", "review_id", "news_id", "content_id"]
    found = None
    for a in aliases:
        if a in df.columns:
            found = a
            break
    if found is None:
        raise KeyError(f"CSVì— article_id ê³„ì—´ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (ì»¬ëŸ¼={list(df.columns)[:12]})")
    if found != "article_id":
        df = df.rename(columns={found: "article_id"})

    df["article_id"] = (
        df["article_id"]
        .astype(str)
        .str.replace("\ufeff", "", regex=False)
        .apply(lambda x: _CTRL.sub("", x))
        .str.strip()
    )
    return df

# ====== [ìˆ˜ì •] í† í”½ ë‹¨ì–´ ì€í–‰ (Chi2 ê¸°ë°˜ìœ¼ë¡œ ë³µì›) ======
def build_topic_term_bank(df: pd.DataFrame,
                          vect: CountVectorizer,
                          lda_model: LatentDirichletAllocation,
                          topn: int = 80,
                          min_samples: int = 10) -> dict: # [ìˆ˜ì •] ìµœì†Œ ìƒ˜í”Œ ìˆ˜ 10
    """
    [ìˆ˜ì •] LogReg ëŒ€ì‹  Chi2 (ì¹´ì´ì œê³±) í†µê³„ë¡œ ë³µì› (ìƒ˜í”Œ ë¶€ì¡± ë¬¸ì œ í•´ê²°)
    - 'good': í•´ë‹¹ í† í”½ì—ì„œ 'Good' ë¼ë²¨ê³¼ í†µê³„ì ìœ¼ë¡œ ê°€ì¥ ì—°ê´€ì„±ì´ ë†’ì€ ë‹¨ì–´
    - 'bad': í•´ë‹¹ í† í”½ì—ì„œ 'Bad' ë¼ë²¨ê³¼ í†µê³„ì ìœ¼ë¡œ ê°€ì¥ ì—°ê´€ì„±ì´ ë†’ì€ ë‹¨ì–´
    """
    bank = {}
    
    # [ìˆ˜ì •] 'topic' ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„
    if 'topic' not in df.columns:
        return bank

    vocab = np.array(vect.get_feature_names_out())
    X = vect.transform((df["title"].fillna("") + " " + df["content"].fillna("")).tolist())
    topics = df["topic"].astype(int).values
    labels = (df["quality_label"] == "good").astype(int).values

    for t in sorted(np.unique(topics)):
        idx_t = np.where(topics == t)[0]
        
        # [ìˆ˜ì •] ìƒ˜í”Œ ìˆ˜ ì²´í¬ ë° 'ì‹¤íŒ¨ ê¸°ë¡' ë‚¨ê¸°ê¸°
        if len(idx_t) < min_samples:
            bank[int(t)] = {
                "status": "error", 
                "message": f"ìƒ˜í”Œ ë¶€ì¡± (N={len(idx_t)}, ìµœì†Œ {min_samples} í•„ìš”)"
            }
            continue
            
        Xt = X[idx_t]
        yt = labels[idx_t]

        try:
            # 'Good' ë¼ë²¨ê³¼ 'Bad' ë¼ë²¨ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if len(np.unique(yt)) < 2:
                 bank[int(t)] = {"status": "error", "message": f"ë‹¨ì¼ ë¼ë²¨ë§Œ ì¡´ì¬ (N={len(idx_t)})"}
                 continue

            chi_good, _ = chi2(Xt, yt)
            order_g = np.argsort(chi_good)[::-1]
            good_terms = [(vocab[i], float(chi_good[i])) for i in order_g[:topn]]

            chi_bad, _ = chi2(Xt, 1 - yt) # 'Bad' ë¼ë²¨(1-yt)ê³¼ ì—°ê´€ì„±
            order_b = np.argsort(chi_bad)[::-1]
            bad_terms = [(vocab[i], float(chi_bad[i])) for i in order_b[:topn]]

            counts = np.asarray(Xt.sum(axis=0)).ravel()
            order_all = np.argsort(counts)[::-1][:topn]
            all_terms = [(vocab[i], float(counts[i])) for i in order_all]

            bank[int(t)] = {
                "good": good_terms, 
                "bad": bad_terms, 
                "all": all_terms,
                "status": "ok", 
                "message": f"ì„±ê³µ (N={len(idx_t)})"
            }
        except Exception as e:
             bank[int(t)] = {"status": "error", "message": f"Chi2 ê³„ì‚° ì˜¤ë¥˜: {e}"}
    return bank


# ====== Draft â†’ Topic ì¶”ë¡  ======
def infer_topic_for_text(txt: str,
                         vect: CountVectorizer,
                         lda_model: LatentDirichletAllocation) -> Tuple[int, np.ndarray]:
    Xd = vect.transform([txt if isinstance(txt, str) else ""])
    dist = lda_model.transform(Xd)[0]
    return int(dist.argmax()), dist

def get_topic_keywords_from_bank(bank: dict, topic_id: int, k_each: int = 30) -> Dict[str, List[Tuple[str, float]]]:
    """ì£¼ì œ IDì— í•´ë‹¹í•˜ëŠ” 'good'/'all' í‚¤ì›Œë“œë¥¼ (ë‹¨ì–´, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    if topic_id not in bank or bank[topic_id].get("status") != "ok": # [ìˆ˜ì •] status check
        return {"good": [], "all": []}
    
    goods = [(w, s) for w,s in bank[topic_id].get("good", [])[:k_each]]
    alls = [(w, s) for w,s in bank[topic_id].get("all", [])[:max(1, k_each//2)]]
    
    seen = set()
    unique_goods = []
    for w,s in goods:
        if w not in seen:
            unique_goods.append((w,s)); seen.add(w)
            
    unique_alls = []
    for w,s in alls:
        if w not in seen:
            unique_alls.append((w,s)); seen.add(w)

    return {"good": unique_goods, "all": unique_alls}


# ====== LLM Reranker ======
def llm_rerank_with_explanations(
    draft_title: str, 
    draft_body: str,  
    candidates: List[str],
    topk: int = 8,
    audience: str = "í˜¼í•©",
    tone: str = "ë¶„ì„ì ",
    model_name: str = MODEL_CHAT,
    temperature: float = 0.2,
) -> List[Dict]:
    if not USE_LLM or client is None or not LLM_OK:
        raise RuntimeError("APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: OPENAI_API_KEY/ë„¤íŠ¸ì›Œí¬/ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")

    cand = [c.strip() for c in candidates if str(c).strip()]
    cand_unique = list(dict.fromkeys(cand))[:500]
    if not cand_unique:
        raise RuntimeError("í›„ë³´ ë‹¨ì–´ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (í† í”½ ë‹¨ì–´ì€í–‰/ë¶„ë¥˜ê¸° íŒíŠ¸ ìƒì„± í™•ì¸)")

    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ì–´ ì½˜í…ì¸  í¸ì§‘ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. ë°˜ë“œì‹œ JSON ê°ì²´ë§Œ ì¶œë ¥í•œë‹¤. "
        "ì´ˆì•ˆì€ {'title': '...', 'body': '...'} JSON ê°ì²´ë¡œ ì œê³µëœë‹¤. 'title'ê³¼ 'body'ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ë¶„ì„í•´ì•¼ í•œë‹¤. "
        "ê°ì²´ëŠ” {'items': [...]} í˜•ì‹ì´ë©°, ê° í•­ëª©ì€ "
        "{term, why, where_to_add, insertion_example, expected_effect, cautions} í‚¤ë¥¼ ê°€ì§„ë‹¤. "
        "where_to_addëŠ” ë°˜ë“œì‹œ ['ì œëª©', 'ë³¸ë¬¸'] ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•œë‹¤. ('ì†Œì œëª©', 'ì²« 120ì' ë“± ë‹¤ë¥¸ ê°’ì€ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€) "
        "ë°˜ë“œì‹œ 'í›„ë³´ í’€'ì— ìˆëŠ” ë‹¨ì–´ë§Œ ì‚¬ìš©."
        "insertion_exampleì€ ë°˜ë“œì‹œ ì´ˆì•ˆì˜ ì‹¤ì œ ë¬¸ì¥ì„ ì°¾ì•„ 'ê¸°ì¡´/ìˆ˜ì •' (Before/After) í˜•ì‹ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•´ì•¼ í•œë‹¤."
    )
    user_payload = {
        "goal": f"ì´ˆì•ˆ ë¬¸ë§¥ì„ ë³´ì¡´í•˜ë©° í›„ë³´ í’€ì—ì„œë§Œ Top-{topk} ì„ ë³„",
        "constraints": [
            "í›„ë³´ ë°– ë‹¨ì–´/ë™ì˜ì–´ ê¸ˆì§€",
            "ë¬¸ë§¥ ì–´ê¸‹ë‚˜ëŠ” ì‚½ì… ì˜ˆì‹œ ê¸ˆì§€",
            "ì¤‘ë³µ ì˜ë¯¸ ì¶”ì²œ ìµœì†Œí™”",
            "where_to_addëŠ” 'ì œëª©' ë˜ëŠ” 'ë³¸ë¬¸'ë§Œ í—ˆìš©. 'ì†Œì œëª©' ê¸ˆì§€.",
            f"ë…ììˆ˜ì¤€={audience}",
            f"í†¤={tone}"
        ],
        "candidates": cand_unique,
        "draft": {
            "title": draft_title,
            "body": draft_body[:6000] 
        },
        "return_format": [
            {"term":"...", "why":"'ì œëª©' ë˜ëŠ” 'ë³¸ë¬¸'ì˜ ë¬¸ë§¥ê³¼ ì—°ê´€ì§€ì–´ ì„¤ëª…", "where_to_add":"ì œëª©",
             "insertion_example":"ì˜ˆ: 'ê¸°ì¡´: AI íŠ¸ë Œë“œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.' -> 'ìˆ˜ì •: [ì¶”ì²œë‹¨ì–´] AI íŠ¸ë Œë“œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.'", 
             "expected_effect":"...", "cautions":"..."}
        ]
    }
    try:
        resp = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role":"system","content": sys_prompt},
                {"role":"user","content": json.dumps(user_payload, ensure_ascii=False)}
            ],
            temperature=temperature,
            response_format={"type": "json_object"},
        )
        raw = (resp.choices[0].message.content or "").strip()
        st.session_state["llm_meta_last"] = {
            "model": getattr(resp, "model", model_name),
            "id": getattr(resp, "id", ""),
            "usage": getattr(resp, "usage", None),
            "raw": raw,
            "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
        }
        data_obj = _parse_json_safely(raw)
        data = data_obj.get("items") if isinstance(data_obj, dict) else data_obj
        if not isinstance(data, list):
            raise ValueError("JSON í˜•ì‹ ì˜¤ë¥˜: ë°°ì—´(items)ì´ ì•„ë‹˜")

        allowed = set(cand_unique)
        recs: List[Dict] = []
        for item in data:
            term = str(item.get("term","")).strip()
            where = str(item.get("where_to_add","")).strip()
            if not term or term not in allowed or where not in ['ì œëª©', 'ë³¸ë¬¸']: 
                continue
            recs.append({
                "term": term,
                "category": categorize_term(term),
                "why": str(item.get("why","")).strip(),
                "where_to_add": where,
                "insertion_example": str(item.get("insertion_example","")).strip(),
                "expected_effect": str(item.get("expected_effect","")).strip(),
                "cautions": str(item.get("cautions","")).strip(),
            })
            if len(recs) >= topk:
                break
        if not recs:
            preview = (raw[:200] + "â€¦") if raw else "(ë¹ˆ ì‘ë‹µ)"
            raise RuntimeError(f"API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨(í›„ë³´ ë‚´ ìœ íš¨ í•­ëª© 0): {preview}")
        return recs

    except (APIError, RateLimitError) as e:
        raise RuntimeError(f"API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    except Exception as e:
        if st.session_state.get("show_llm_raw"):
            st.text(st.session_state.get("llm_meta_last", {}).get("raw","(raw ì—†ìŒ)"))
        raise RuntimeError(f"API í˜¸ì¶œ/íŒŒì‹± ì‹¤íŒ¨: {e}")

# ========= ì„±ê³¼ ê¸°ë°˜ ë¶„ì„ (RobustScaler) =========
def build_engagement(df: pd.DataFrame, w_views=0.4, w_likes=0.4, w_comments=0.2) -> pd.DataFrame:
    """
    RobustScaler(ì¤‘ì•™ê°’/IQR) ê¸°ë°˜ ì •ê·œí™”:
      x_rob = (x - median) / IQR
    â€» ë²”ìœ„ê°€ [0,1]ë¡œ ê³ ì •ë˜ì§€ ì•ŠìŒ(ì´ìƒì¹˜ ì˜í–¥ ê°ì†Œ ëª©ì ).
    """
    df = df.copy()
    cols = ["views_total", "likes", "comments"]
    for c in cols:
        if c not in df.columns:
            df[c] = 0
        df[c] = df[c].fillna(0)

    scaler = RobustScaler(quantile_range=(25.0, 75.0))
    for c in cols:
        df[c + "_rob"] = scaler.fit_transform(df[[c]]).ravel()

    df["engagement"] = (
        w_views   * df["views_total_rob"] +
        w_likes   * df["likes_rob"] +
        w_comments* df["comments_rob"]
    )
    return df

def label_quality_by_quantile(df: pd.DataFrame, col="engagement", low_q=0.33, high_q=0.66) -> pd.DataFrame:
    df = df.copy()
    q_low, q_high = df[col].quantile([low_q, high_q])
    def _label(x):
        if x >= q_high: return "good"
        if x <= q_low: return "bad"
        return "medium"
    df["quality_label"] = df[col].apply(_label)
    return df

# ========= LDA(ì˜¨ë¼ì¸) =========
def run_lda_topics_streaming(
    texts: List[str],
    n_topics: int = 10,
    max_features: int = 5000,
    batch_size: int = 1000,
    n_epochs: int = 3,
):
    vect = CountVectorizer(
        min_df=0.01,  # ìµœì†Œ 1%ì˜ ë¬¸ì„œì—ëŠ” ë“±ì¥í•´ì•¼ í•¨ (í¬ê·€ ë‹¨ì–´ ì œê±°)
        max_df=0.90,   
        stop_words=STOPWORDS_KO
    )
    X = vect.fit_transform([t if isinstance(t, str) else "" for t in texts])

    lda = LatentDirichletAllocation(
        n_components=n_topics, learning_method="online",
        batch_size=batch_size, max_iter=1, random_state=42, evaluate_every=0,
    )

    n_samples = X.shape[0]
    n_batches = int(np.ceil(n_samples / batch_size))
    total_steps = n_epochs * n_batches

    prog = st.progress(0.0, text="LDA ì£¼ì œ ë¶„ì„ í•™ìŠµ ì¤‘â€¦")
    t0 = time.time(); step = 0

    idx_all = np.arange(n_samples)
    for epoch in range(n_epochs):
        idx_all = sk_shuffle(idx_all, random_state=42 + epoch)
        for b in range(n_batches):
            bs = idx_all[b * batch_size : (b + 1) * batch_size]
            Xb = X[bs]
            lda.partial_fit(Xb)

            step += 1
            frac = step / total_steps
            elapsed = time.time() - t0
            sec_per_step = elapsed / max(step, 1)
            remain = sec_per_step * (total_steps - step)
            prog.progress(
                frac, text=f"LDA í•™ìŠµ {frac*100:.1f}% | ê²½ê³¼ {elapsed:,.0f}s | ë‚¨ìŒ ~{remain:,.0f}s"
            )

    W = lda.transform(X)        # ë¬¸ì„œ-í† í”½ ë¶„í¬
    prog.empty()
    df_topic = pd.DataFrame({"topic": W.argmax(axis=1)})
    return df_topic, vect, lda, W

# --------- n-gram ì¤‘ìš”ë„ ---------
def top_ngrams_by_logreg(texts: List[str], labels: List[str], ngram_range=(1,2), k: int = 20) -> Dict[str, List[Tuple[str, float]]]:
    y = np.array([1 if l=="good" else 0 for l in labels])
    tfidf = TfidfVectorizer(
        ngram_range=ngram_range, 
        max_features=30000, 
        min_df=2,
        stop_words=STOPWORDS_KO 
    )
    X = tfidf.fit_transform([t if isinstance(t,str) else "" for t in texts])
    clf = LogisticRegression(max_iter=1000, solver="liblinear")
    clf.fit(X, y)
    coefs = clf.coef_[0]
    idx_sorted = np.argsort(coefs)
    vocab = np.array(tfidf.get_feature_names_out())
    top_pos = [(vocab[i], float(coefs[i])) for i in idx_sorted[::-1][:k]]
    top_neg = [(vocab[i], float(coefs[i])) for i in idx_sorted[:k]]
    return {"good_terms": top_pos, "bad_terms": top_neg}

# --------- ë¶ˆìš©ì–´/ETA í•™ìŠµ ìœ í‹¸ ---------
def train_logreg_with_progress(texts, labels, stoplist=None, ngram_range=(1,2),
                               epochs=3, batch_size=2000, k_show=20, seed=42):
    y = np.array([1 if l=="good" else 0 for l in labels])
    
    final_stop_words = list(STOPWORDS_KO)
    if stoplist:
        if isinstance(stoplist, (set, tuple, list)):
            final_stop_words.extend(list(stoplist))
        else:
            try: final_stop_words.extend(list(stoplist))
            except Exception: pass
    final_stop_words = list(set(final_stop_words)) 

    tfidf = TfidfVectorizer(
        ngram_range=ngram_range, max_features=30000, min_df=2,
        stop_words=final_stop_words 
    )
    X = tfidf.fit_transform([t if isinstance(t,str) else "" for t in texts])

    n = X.shape[0]
    idx_all = np.arange(n)
    n_batches = int(np.ceil(n / batch_size))
    total_steps = epochs * n_batches

    clf = SGDClassifier(loss="log_loss", learning_rate="optimal",
                        alpha=1e-5, random_state=seed, warm_start=True)
    classes = np.array([0,1])

    prog = st.progress(0.0, text="ì½˜í…ì¸  ë“±ê¸‰ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì¤‘â€¦")
    t0 = time.time(); step = 0

    for ep in range(epochs):
        idx_all = sk_shuffle(idx_all, random_state=seed + ep)
        for b in range(n_batches):
            bs = idx_all[b*batch_size : (b+1)*batch_size]
            Xb = X[bs]; yb = y[bs]
            clf.partial_fit(Xb, yb, classes=classes)

            step += 1
            frac = step/total_steps
            elapsed = time.time() - t0
            sec_per = elapsed / max(step,1)
            remain = sec_per * (total_steps - step)
            prog.progress(frac, text=f"ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ {frac*100:.1f}% | ê²½ê³¼ {elapsed:,.0f}s | ë‚¨ìŒ ~{remain:,.0f}s")

    prog.empty()

    coefs = clf.coef_[0]
    vocab = np.array(tfidf.get_feature_names_out())
    order = np.argsort(coefs)
    good_terms = [(vocab[i], float(coefs[i])) for i in order[::-1][:k_show]]
    bad_terms  = [(vocab[i], float(coefs[i]))  for i in order[:k_show]]

    return {
        "tfidf": tfidf, "clf": clf,
        "good_terms": good_terms, "bad_terms": bad_terms,
        "last_eta_info": {"elapsed": time.time()-t0}
    }

# === í† í”½ ìƒìœ„ë‹¨ì–´ & ë¼ë²¨ ===
def get_topic_top_words(lda, vect, topn=8):
    vocab = np.array(vect.get_feature_names_out())
    topics = {}
    for k, comp in enumerate(lda.components_):
        idx = np.argsort(comp)[::-1][:topn]
        topics[f"Topic {k}"] = [str(vocab[i]) for i in idx]
    return topics

def _heuristic_topic_name(words: list[str]) -> dict:
    w = " ".join(words)
    rules = [
        (["ì •ë¶€","êµ­íšŒ","ì˜ˆì‚°","ì •ì±…","ëŒ€í†µë ¹"], ("ì •ì¹˜/í–‰ì •", "ì •ë¶€Â·êµ­íšŒÂ·ì˜ˆì‚° ê´€ë ¨ ì´ìŠˆ")),
        (["ì†í¥ë¯¼","ë¦¬ê·¸","ê²½ê¸°","ê³¨","ì„ ìˆ˜","ìŠ¤í¬ì¸ "], ("ìŠ¤í¬ì¸ /ì¶•êµ¬", "ê²½ê¸°/ì„ ìˆ˜/ë¦¬ê·¸ ì¤‘ì‹¬ ê¸°ì‚¬")),
        (["AI","ì¸ê³µì§€ëŠ¥","ë¡œë´‡","ê¸°ìˆ ","ì‚°ì—…","ìë™í™”","ë°ì´í„°"], ("ê¸°ìˆ /AI", "AIÂ·ë¡œë´‡Â·ì‚°ì—… ìë™í™”")),
        (["ì£¼ì‹","í™˜ìœ¨","ë¶€ë™ì‚°","ê¸ˆë¦¬","ê²½ì œ"], ("ê²½ì œ/ê¸ˆìœµ", "ê±°ì‹œê²½ì œÂ·ì‹œì¥ ë™í–¥")),
        (["ì½”ë¡œë‚˜","ì˜ë£Œ","ê±´ê°•","ë³‘ì›"], ("ì˜ë£Œ/ê±´ê°•", "ì§ˆë³‘Â·ì˜ë£ŒÂ·í—¬ìŠ¤ì¼€ì–´")),
        (["ë„·í”Œë¦­ìŠ¤","ì˜í™”","ë“œë¼ë§ˆ","ì½˜í…ì¸ "], ("ë¬¸í™”/ì½˜í…ì¸ ", "ì˜í™”Â·ë°©ì†¡Â·í”Œë«í¼")),
    ]
    for keys, (nm, desc) in rules:
        if any(k in w for k in keys):
            return {"name": nm, "desc": desc}
    return {"name": "ì¼ë°˜/ì¢…í•©", "desc": "ê´‘ë²”ìœ„í•œ ì´ìŠˆ ë¬¶ìŒ"}

def llm_name_topics(topic_top_words: dict, model_name=MODEL_CHAT):
    if not USE_LLM or client is None or not LLM_OK:
        return {k: _heuristic_topic_name(v) for k, v in topic_top_words.items()}

    payload = {
        "topics": topic_top_words,
        "schema": {"Topic k": {"name": "ì§§ì€ ì´ë¦„", "desc": "í•œ ì¤„ ì„¤ëª…"}},
        "instruction": "ìœ„ 'topics'ì˜ ìƒìœ„ ë‹¨ì–´ë¥¼ ë³´ê³  ê° í† í”½ì— ëŒ€í•´ {name, desc}ë¥¼ ìƒì„±. "
                       "JSON ê°ì²´ë¡œ { 'Topic 0': {'name':'..','desc':'..'}, ... } í˜•ì‹ë§Œ ë°˜í™˜. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€."
    }
    try:
        r = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role":"system","content":"ë„ˆëŠ” ì£¼ì œ ë¼ë²¨ëŸ¬ë‹¤. JSON ê°ì²´ë§Œ ë°˜í™˜í•œë‹¤."},
                {"role":"user","content": json.dumps(payload, ensure_ascii=False)}
            ],
            temperature=0.2,
            response_format={"type": "json_object"},
        )
        txt = (r.choices[0].message.content or "").strip()
        data = _parse_json_safely(txt)
        if not isinstance(data, dict) or not data:
            raise ValueError("ë¹ˆ JSON")
        for k, words in topic_top_words.items():
            if k not in data or "name" not in data[k]:
                data[k] = _heuristic_topic_name(words)
        return data
    except Exception:
        return {k: _heuristic_topic_name(v) for k, v in topic_top_words.items()}

# [ìˆ˜ì •] ê°ì„± S/I ê³„ì‚° í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½ (cv, lexë¥¼ ì¸ìë¡œ ë°›ë„ë¡)
def compute_sentiment_SI(df_work: pd.DataFrame, cv: CountVectorizer, lex: dict) -> pd.DataFrame:
    """ê°„ë‹¨ í† í° ê¸°ì¤€ í‰ê· ê°ì„± S, í‰ê· ì ˆëŒ€ê°ì„± I (CV, Lexicon ì™¸ë¶€ ì£¼ì…)"""
    df = df_work.copy()
    texts = (df["title"].fillna("") + " " + df["content"].fillna("")).tolist()

    X = cv.transform(texts) 
    vocab = np.array(cv.get_feature_names_out())

    rows, cols = X.nonzero()
    tok_by_row: Dict[int, List[str]] = {}
    for r, c in zip(rows, cols):
        tok_by_row.setdefault(r, []).append(vocab[c])

    S, I = [], []
    for r in range(X.shape[0]):
        vals = [lex.get(t, 0.0) for t in tok_by_row.get(r, [])]
        if vals:
            S.append(float(np.mean(vals)))
            I.append(float(np.mean(np.abs(vals))))
        else:
            S.append(0.0); I.append(0.0)
    df["S"], df["I"] = S, I
    return df

# [ìˆ˜ì •] TAB1ì—ì„œ ì´ˆì•ˆ í…ìŠ¤íŠ¸ì˜ ê°ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ í—¬í¼ í•¨ìˆ˜
def get_sentiment_for_text(txt: str, senti_pack: dict) -> Tuple[float, float]:
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•´ S/I ì ìˆ˜ ê³„ì‚°"""
    if not senti_pack or not senti_pack.get('cv') or not senti_pack.get('lex') or not txt:
        return 0.0, 0.0
    
    try:
        cv = senti_pack['cv']
        lex = senti_pack['lex']
        
        X = cv.transform([txt])
        vocab = np.array(cv.get_feature_names_out())
        
        rows, cols = X.nonzero()
        if not np.any(cols): 
            return 0.0, 0.0
            
        vals = [lex.get(vocab[c], 0.0) for c in cols]
        if vals:
            s = float(np.mean(vals))
            i = float(np.mean(np.abs(vals)))
            return s, i
    except Exception:
        return 0.0, 0.0
    return 0.0, 0.0

# [ì‹ ê·œ] TAB1ì—ì„œ ìµœê·¼ 30ì¼ ì¸ê¸° ë‹¨ì–´ë¥¼ ë™ì ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def get_recent_popular_words(df_all_data: pd.DataFrame, 
                             end_date: datetime.date, 
                             topic_id: int = None, 
                             k: int = 10) -> List[str]:
    """íŠ¹ì • í† í”½/ê¸°ê°„/Goodë“±ê¸‰ ë¬¸ì„œì—ì„œ Top-K ë¹ˆë„ ë‹¨ì–´ ì¶”ì¶œ"""
    if df_all_data is None or df_all_data.empty or 'date' not in df_all_data.columns or 'topic' not in df_all_data.columns or 'quality_label' not in df_all_data.columns:
        return []
    
    try:
        df = df_all_data.copy()
        if not pd.api.types.is_datetime64_any_dtype(df['date']):
             df['date'] = pd.to_datetime(df['date'], errors='coerce')
        
        # ë‚ ì§œ í•„í„°ë§
        end_date_pd = pd.to_datetime(end_date)
        start_date_pd = end_date_pd - pd.Timedelta(days=30)
        
        df_filtered = df[
            (df['date'] >= start_date_pd) & 
            (df['date'] <= end_date_pd) &
            (df['quality_label'] == 'good')
        ]
        
        if topic_id is not None:
            df_filtered = df_filtered[df_filtered['topic'] == topic_id]
        
        if df_filtered.empty:
            return []
        
        texts = (df_filtered["title"].fillna("") + " " + df_filtered["content"].fillna("")).tolist()
        
        cv_recent = CountVectorizer(max_features=2000, stop_words=STOPWORDS_KO)
        X_recent = cv_recent.fit_transform(texts)
        
        word_counts = X_recent.sum(axis=0)
        words_freq = [(word, word_counts[0, idx]) for word, idx in cv_recent.vocabulary_.items()]
        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
        
        return [word for word, freq in words_freq[:k]]
        
    except Exception as e:
        st.error(f"[ìµœê·¼ ë‹¨ì–´ ë¶„ì„ ì˜¤ë¥˜] {e}. (ë°ì´í„°ì— 'topic' ë˜ëŠ” 'date' ì»¬ëŸ¼ì´ ì˜¬ë°”ë¥´ê²Œ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.)")
        return []


# ========= íšŒê·€ ìœ í‹¸ =========
def fit_ols(y, X):
    X = X.apply(pd.to_numeric, errors='coerce').fillna(0)
    y = y.apply(pd.to_numeric, errors='coerce').fillna(0)
    
    valid_idx = (y != 0) | (X != 0).any(axis=1)
    y_valid = y[valid_idx]
    X_valid = X[valid_idx]

    if len(y_valid) < 2:
        raise ValueError("ìœ íš¨í•œ ë°ì´í„° í¬ì¸íŠ¸ê°€ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. íšŒê·€ ë¶„ì„ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    Xc = sm.add_constant(X_valid, has_constant="add")
    model = sm.OLS(y_valid.astype(float), Xc, missing="drop")
    return model.fit()

def tidy_summary(res: sm.regression.linear_model.RegressionResultsWrapper, max_rows=200):
    s = []
    for name, coef, se, t, p in zip(res.params.index, res.params.values, res.bse.values, res.tvalues, res.pvalues):
        s.append({"term": name, "coef": float(coef), "se": float(se), "t": float(t), "p": float(p)})
    df = pd.DataFrame(s)
    if len(df) > max_rows:
        return df.head(max_rows)
    return df

# [ìˆ˜ì •] TAB4 'ë¶ˆìš©ì–´ ì˜ì‹¬ ë‹¨ì–´' ë¡œì§ ë³€ê²½: 'Bad'ê°€ ì•„ë‹Œ 'Generic' ë‹¨ì–´ ì¶”ì¶œ
def get_suspected_stopwords(df_all_data: pd.DataFrame, k: int = 50) -> List[str]:
    """í† í”½/ì„±ê³¼ì™€ ë¬´ê´€í•˜ê²Œ ê°€ì¥ ìì£¼ ì“°ì´ëŠ” ì¼ë°˜ ë‹¨ì–´(ë¶ˆìš©ì–´ í›„ë³´) ì¶”ì¶œ"""
    if df_all_data is None or df_all_data.empty:
        return []
    try:
        texts = (df_all_data["title"].fillna("") + " " + df_all_data["content"].fillna("")).tolist()
        
        # [ìˆ˜ì •] ë¶ˆìš©ì–´ *ì—†ì´* CV ì‹¤í–‰, min_df=0.1 (ìµœì†Œ 10%ì˜ ë¬¸ì„œì— ë“±ì¥)
        cv_nostop = CountVectorizer(max_features=k, 
                                    min_df=0.1, 
                                    ngram_range=(1,1)) 
        cv_nostop.fit(texts)
        common_words = cv_nostop.get_feature_names_out()
        
        # ì´ ì¤‘ì—ì„œ ì´ë¯¸ ìš°ë¦¬ ê¸°ë³¸ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê²ƒì€ ì œì™¸
        final_suspects = [w for w in common_words if w not in STOPWORDS_KO]
        return final_suspects
    except Exception as e:
        st.error(f"[ë¶ˆìš©ì–´ ì˜ì‹¬ ë‹¨ì–´ ì¶”ì¶œ ì˜¤ë¥˜] {e}")
        return []

# [ì‹ ê·œ] TAB1 ê°ì„± ê²Œì´ì§€ ì°¨íŠ¸ ìƒì„± í•¨ìˆ˜
def create_sentiment_gauge(s_val: float, s_target: float, i_val: float, i_target: float, lexicon_max: float = 1.0):
    """Plotlyì˜ Bullet Chartë¥¼ ì‚¬ìš©í•´ ê°ì„± ì ìˆ˜(S)ì™€ ê°•ë„(I) ê²Œì´ì§€ ìƒì„±"""
    
    fig = go.Figure()

    # [ìˆ˜ì •] ê²Œì´ì§€ë¥¼ ê°€ë¡œë¡œ ë°°ì¹˜ (domain x, yê°’ ìˆ˜ì •)
    # 1. ê°ì„± ì ìˆ˜ (S) ê²Œì´ì§€ (ì™¼ìª½)
    fig.add_trace(go.Indicator(
        mode = "gauge+number",
        value = s_val,
        domain = {'x': [0, 0.48], 'y': [0, 1]}, # [ìˆ˜ì •]
        title = {'text': "ğŸ’– ê°ì„± ì ìˆ˜ (S)", 'font': {'size': 18}},
        number = {'font': {'size': 24}},
        gauge = {
            'axis': {'range': [-lexicon_max, lexicon_max], 'tickwidth': 1, 'tickcolor': "darkblue"},
            'bar': {'color': "#0072F0" if s_val >= 0 else "#E63946", 'thickness': 0.4},
            'bgcolor': "white",
            'borderwidth': 2,
            'bordercolor': "#CCCCCC",
            'steps': [
                {'range': [-lexicon_max, -0.05], 'color': 'rgba(230, 57, 70, 0.1)'}, 
                {'range': [-0.05, 0.05], 'color': 'rgba(200, 200, 200, 0.2)'}, 
                {'range': [0.05, lexicon_max], 'color': 'rgba(0, 114, 240, 0.1)'}
            ],
            'threshold': {
                'line': {'color': "green", 'width': 3},
                'thickness': 0.75,
                'value': s_target
            }
        }
    ))

    # 2. ê°ì„± ê°•ë„ (I) ê²Œì´ì§€ (ì˜¤ë¥¸ìª½)
    fig.add_trace(go.Indicator(
        mode = "gauge+number",
        value = i_val,
        domain = {'x': [0.52, 1], 'y': [0, 1]}, # [ìˆ˜ì •]
        title = {'text': "ğŸ’– ê°ì„± ê°•ë„ (I)", 'font': {'size': 18}},
        number = {'font': {'size': 24}},
        gauge = {
            'axis': {'range': [0, lexicon_max], 'tickwidth': 1, 'tickcolor': "darkblue"},
            'bar': {'color': "#F4A261", 'thickness': 0.4},
            'bgcolor': "white",
            'borderwidth': 2,
            'bordercolor': "#CCCCCC",
            'steps': [
                {'range': [0, lexicon_max * 0.33], 'color': 'rgba(200, 200, 200, 0.2)'},
                {'range': [lexicon_max * 0.33, lexicon_max * 0.66], 'color': 'rgba(244, 162, 97, 0.1)'},
                {'range': [lexicon_max * 0.66, lexicon_max], 'color': 'rgba(244, 162, 97, 0.2)'},
            ],
            'threshold': {
                'line': {'color': "green", 'width': 3},
                'thickness': 0.75,
                'value': i_target
            }
        }
    ))

    fig.update_layout(
        height=180, # [ìˆ˜ì •] ì„¸ë¡œ í¬ê¸° ì¡°ì •
        margin=dict(l=20, r=20, t=40, b=10)
    )
    return fig


# ==================== (â˜…) MODE_CFG ====================
MODE_CFG = {
    "quick": {
        "sample_n": 5000,
        "lda_topics": 0,
        "max_features": 3000,
        "batch_size": 500,
        "n_epochs": 2,
        "clf_epochs": 1,
        "clf_batch": 500,
        "ngram_range": (1, 2),
    },
    "full": {
        "sample_n": None,
        "lda_topics": 0,
        "max_features": 5000,
        "batch_size": 1000,
        "n_epochs": 3,
        "clf_epochs": 3,
        "clf_batch": 1000,
        "ngram_range": (1, 3),
    },
}
# ==================================================================

# ========= Streamlit UI =========
st.set_page_config(page_title="ë¬¸ë§¥í˜• ì¶”ì²œ + ì„±ê³¼ ë¶„ì„ + ê°ì„±/íšŒê·€", page_icon="ğŸ“", layout="wide")
st.title("ğŸ“ ë¬¸ë§¥í˜• ìš©ì–´ ì¶”ì²œ + ğŸ“ˆ ì„±ê³¼ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

with st.sidebar:
    st.subheader("ê³µí†µ ì„¤ì •")
    audience = st.selectbox("ì£¼ìš” ë…ì ìˆ˜ì¤€", ["ì…ë¬¸ì", "ì „ë¬¸ê°€", "í˜¼í•©"], index=2)
    tone = st.selectbox("ì½˜í…ì¸  í†¤/ìŠ¤íƒ€ì¼", ["ì¹œê·¼", "ê³µì‹", "ë¶„ì„ì "], index=2)
    if LLM_OK: st.success("LLM ìƒíƒœ: âœ… ì—°ê²° OK")
    elif USE_LLM: st.error("LLM ìƒíƒœ: âŒ ì¸ì¦/ê¶Œí•œ/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜")
    else: st.error("LLM ìƒíƒœ: âŒ OPENAI_API_KEY ë¯¸ì„¤ì •")

def require_llm():
    if not LLM_OK:
        st.error("APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: OPENAI_API_KEY/ë„¤íŠ¸ì›Œí¬/ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

# [ì‹ ê·œ] TAB4 ì¶”ê°€
TAB1, TAB2, TAB3, TAB4 = st.tabs([
    "ğŸ’¡ ë¬¸ë§¥í˜• ìš©ì–´ ì¶”ì²œ", 
    "ğŸ“ˆ ì„±ê³¼/ì£¼ì œ/ê°ì„± ë¶„ì„", 
    "ğŸ› ï¸ íŒŒì¸íŠœë‹(ê³¨ê²©)",
    "ğŸ”¬ ëª¨ë¸ ê´€ë¦¬ì (Admin)" 
])

# ================= TAB1 =================
with TAB1:
    st.header("1) ì´ˆì•ˆ í…ìŠ¤íŠ¸ ì…ë ¥")
    
    draft_title = st.text_input("ì œëª© (ì„ íƒ)", placeholder="ì˜ˆ: ì´ë²ˆ ì£¼ AI íŠ¸ë Œë“œ Top 5")
    draft_body = st.text_area("ë³¸ë¬¸ (ì´ˆì•ˆ)", height=220,
                         placeholder="ì˜ˆ) 1. ì˜¤í”ˆAIì˜ ìƒˆ ëª¨ë¸ì´...")
    
    full_draft = draft_title.strip() + " " + draft_body.strip()

    c_date, c_check = st.columns([1, 1])
    with c_date:
        ref_date = st.date_input("ê¸°ì¤€ ë‚ ì§œ", datetime.date.today())
    with c_check:
        st.write("") 
        st.write("")
        all_dates = st.checkbox("ëª¨ë“  ë‚ ì§œ ì„ íƒí•˜ê¸° (ì „ì²´ ê¸°ê°„ ë¶„ì„)", value=True) 

    # [ìˆ˜ì •] 'LLM í† í° ë³´ê¸°' ì œê±°

    for _k, _v in [
        ("last_recs", None),
        ("last_recs_time", None),
        ("last_draft", ""),
        ("last_candidates", []),
        ("sentiment_pack", None), 
        ("df_for_analysis", None),
        ("analysis_done", False), 
    ]:
        st.session_state.setdefault(_k, _v)

    candidates = list(DEFAULT_CANDIDATES)
    topic_id_for_draft, topic_dist = None, None
    topic_name = "ë¯¸ë¶„ë¥˜" 

    topic_bank = st.session_state.get("topic_term_bank")
    lda_vect   = st.session_state.get("lda_vect")
    lda_model  = st.session_state.get("lda_model")
    clf_pack   = st.session_state.get("clf_pack")
    senti_pack = st.session_state.get('sentiment_pack')
    df_all_data = st.session_state.get('df_for_analysis') 

    # [ì‹ ê·œ] ë°ì´í„° ë¯¸ë¡œë“œ ì‹œ ê²½ê³ 
    if not st.session_state['analysis_done']:
        st.warning("âš ï¸ **ë°ì´í„° ë¯¸ë¡œë“œ:** TAB2ì—ì„œ CSV ì—…ë¡œë“œ ë° ë¶„ì„ì„ ì‹¤í–‰í•˜ë©´ ê³¼ê±° ë°ì´í„° ê¸°ë°˜ì˜ í™•ë¥ , ì£¼ì œ, í‚¤ì›Œë“œ ì¶”ì²œì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

    
    # --- 1. í† í”½ ì¶”ë¡  ë° íƒœê·¸ í‘œì‹œ ---
    if full_draft.strip() and topic_bank and lda_vect is not None and lda_model is not None:
        topic_id_for_draft, topic_dist = infer_topic_for_text(full_draft, lda_vect, lda_model)
        
        topic_name = f"í† í”½ {topic_id_for_draft}"
        topic_desc = "ë¶„ì„ëœ ì£¼ì œ"
        lbls = st.session_state.get("topic_labels", {})
        if f"Topic {topic_id_for_draft}" in lbls:
            meta = lbls[f"Topic {topic_id_for_draft}"]
            topic_name = meta.get('name', topic_name)
            topic_desc = meta.get('desc', topic_desc)

        st.markdown(f"**ì´ˆì•ˆì˜ ì˜ˆìƒ ì£¼ì œ:** <span style='background-color: #0072F0; color: white; padding: 3px 8px; border-radius: 15px; font-size: 0.9em; margin-left: 10px;'>{topic_name}</span>", unsafe_allow_html=True)
        st.caption(f"â”” {topic_desc} (í›„ë³´ ë‹¨ì–´ {len(candidates)}ê°œ)")
    
    # --- 2. ë“±ê¸‰/í™•ë¥  ë° í‚¤ì›Œë“œ ì¶”ì²œ (ë¶„ë¥˜ê¸° ë¡œë“œ ì‹œ) ---
    if full_draft.strip() and clf_pack is not None:
        tfidf = clf_pack["tfidf"]; clf = clf_pack["clf"]
        Xd = tfidf.transform([full_draft])
        proba_good = float(clf.predict_proba(Xd)[0,1])
        label = "ìƒ (Good)" if proba_good >= 0.5 else "í•˜ (Bad)"
        
        c1, c2 = st.columns(2)
        c1.metric("ì˜ˆìƒ ì½˜í…ì¸  ë§¤ë ¥ ë“±ê¸‰", label)
        c2.metric("ğŸ“ˆ ê³¼ê±° ë°ì´í„° ê¸°ë°˜ ì„±ê³¼ í™•ë¥ ", f"{proba_good*100:.1f}%")
        
        st.caption("â”” ê³¼ê±° ë°ì´í„°(TAB2)ë¡œ í•™ìŠµí•œ í†µê³„ ëª¨ë¸ì˜ ì˜ˆì¸¡ì¹˜ì…ë‹ˆë‹¤. LLM ì¶”ì²œ(ë¬¸ë§¥/í’ˆì§ˆ ì¤‘ì‹¬)ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # [ìˆ˜ì •] 'ëª¨ë“  ë‚ ì§œ' ì²´í¬ë°•ìŠ¤ì— ë”°ë¼ ë™ì  í‚¤ì›Œë“œ ì¶”ì²œ
        if all_dates:
            # --- "ëª¨ë“  ë‚ ì§œ" ì„ íƒ ì‹œ (ê¸°ì¡´ ë¡œì§) ---
            if topic_id_for_draft is not None and topic_bank:
                topic_keywords_data = get_topic_keywords_from_bank(topic_bank, int(topic_id_for_draft), k_each=10)
                good_topic_terms = [w for w,s in topic_keywords_data.get("good", [])]
                if good_topic_terms:
                    with st.expander(f"âœ… **'{topic_name}' ì£¼ì œ**ì˜ **ì „ì²´ ê¸°ê°„** ì„±ê³¼ ìš°ìˆ˜ ë‹¨ì–´ (ì¶”ì²œ)"):
                        st.markdown(f"**ì´ìœ :** ê³¼ê±° ì´ ì£¼ì œ(`{topic_name}`)ì˜ ì½˜í…ì¸  ì¤‘ **ë†’ì€ ì„±ê³¼**ë¥¼ ë‚¸ ë¬¸ì„œì—ì„œ ìì£¼ ë°œê²¬ëœ ë‹¨ì–´ë“¤ì…ë‹ˆë‹¤.")
                        st.info(", ".join(good_topic_terms))
                    candidates = list(dict.fromkeys(good_topic_terms + candidates)) 

            good_terms_list = clf_pack.get("good_terms", [])
            top_good = [t for t,_ in good_terms_list]
            missing  = [t for t in top_good if t not in full_draft][:8]
            if missing:
                st.info(f"ğŸ’¡ **ì „ì²´ ê¸°ê°„** ì„±ê³¼ ìš°ìˆ˜ ë‹¨ì–´ (ì´ˆì•ˆì— ì—†ìœ¼ë©´ ì¶”ê°€ ê³ ë ¤): \n\n" + ", ".join(missing))
                st.caption("â”” ì´ìœ : ì£¼ì œì™€ ê´€ê³„ì—†ì´ ì „ë°˜ì ìœ¼ë¡œ ë†’ì€ ì„±ê³¼ë¥¼ ë‚¸ ì½˜í…ì¸ ì˜ ê³µí†µ í‚¤ì›Œë“œì…ë‹ˆë‹¤.")
                candidates = list(dict.fromkeys(list(missing) + candidates))
        
        else:
            # --- "íŠ¹ì • ë‚ ì§œ" ì„ íƒ ì‹œ (ìµœê·¼ 30ì¼) ---
            if topic_id_for_draft is not None and (df_all_data is not None and not df_all_data.empty):
                # 1. ì£¼ì œë³„ ìµœê·¼ ë‹¨ì–´
                recent_topic_words = get_recent_popular_words(df_all_data, ref_date, topic_id=topic_id_for_draft, k=10)
                if recent_topic_words:
                    with st.expander(f"ğŸ“ˆ **'{topic_name}' ì£¼ì œ**ì˜ **ìµœê·¼ 30ì¼** ì¸ê¸° ë‹¨ì–´ (Good ì½˜í…ì¸ )"):
                        st.markdown(f"**ê¸°ì¤€:** `{ref_date - datetime.timedelta(days=30)}` ~ `{ref_date}` ê¸°ê°„ ë™ì•ˆ ì„±ê³¼ê°€ ì¢‹ì•˜ë˜ ë¬¸ì„œ ê¸°ì¤€")
                        st.warning(", ".join(recent_topic_words))
                    candidates = list(dict.fromkeys(recent_topic_words + candidates))
                
                # 2. ì „ì²´ ìµœê·¼ ë‹¨ì–´
                recent_all_words = get_recent_popular_words(df_all_data, ref_date, topic_id=None, k=10)
                missing_recent = [w for w in recent_all_words if w not in full_draft][:8]
                if missing_recent:
                    st.info(f"ğŸ“ˆ **ì „ì²´ ì£¼ì œ**ì˜ **ìµœê·¼ 30ì¼** ì¸ê¸° ë‹¨ì–´ (ì´ˆì•ˆì— ì—†ìœ¼ë©´ ì¶”ê°€ ê³ ë ¤): \n\n" + ", ".join(missing_recent))
                    st.caption(f"â”” ì´ìœ : `{ref_date}` ê¸°ì¤€ ìµœê·¼ 30ì¼ê°„ ì„±ê³¼ê°€ ì¢‹ì•˜ë˜ ëª¨ë“  ì½˜í…ì¸ ì˜ ê³µí†µ í‚¤ì›Œë“œì…ë‹ˆë‹¤.")
                    candidates = list(dict.fromkeys(list(missing_recent) + candidates))
                
                if not recent_topic_words and not missing_recent:
                    st.caption(f"â„¹ï¸ `{ref_date}` ê¸°ì¤€ ìµœê·¼ 30ì¼ê°„ì˜ ì¸ê¸° ë‹¨ì–´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            elif df_all_data is None or df_all_data.empty:
                 st.caption("â„¹ï¸ TAB2ì—ì„œ ë¶„ì„ì„ ì‹¤í–‰í•˜ë©´ 'ìµœê·¼ 30ì¼ ì¸ê¸° ë‹¨ì–´'ê°€ í™œì„±í™”ë©ë‹ˆë‹¤.")

        st.divider()
        
        # --- 3. ê°ì„± ì ìˆ˜ (ê°ì„± ì‚¬ì „ ë¡œë“œ ì‹œ) ---
        if senti_pack and senti_pack.get('cv') and senti_pack.get('lex'):
             senti_s, senti_i = get_sentiment_for_text(full_draft, senti_pack)
             target_s = senti_pack.get('target_s')
             target_i = senti_pack.get('target_i')

             # [ì‹ ê·œ] Plotly ê²Œì´ì§€ ì°¨íŠ¸ë¡œ S/I ì ìˆ˜ì™€ ëª©í‘œì¹˜ í‘œì‹œ
             st.plotly_chart(
                 create_sentiment_gauge(senti_s, target_s, senti_i, target_i, lexicon_max=1.0),
                 use_container_width=True
             )
             
             # [ì‹ ê·œ] ìº¡ì…˜ì— ëª©í‘œ ì ìˆ˜ í…ìŠ¤íŠ¸ ì¶”ê°€
             if target_s is not None and target_i is not None:
                st.markdown(f"**ğŸ¯ ëª©í‘œ ì ìˆ˜** (Good ì½˜í…ì¸  í‰ê· ): **S (ì ìˆ˜): {target_s:.2f}** | **I (ê°•ë„): {target_i:.2f}**")
             
             st.caption("""
             * **ê°ì„± ì ìˆ˜ (S):** ê¸ì •(>0) ë˜ëŠ” ë¶€ì •(<0)ì˜ ì •ë„. 0ì€ ì¤‘ë¦½. (ë²”ìœ„: -1 ~ 1)
             * **ê°ì„± ê°•ë„ (I):** ê°ì„±ì´ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ í‘œí˜„ë˜ì—ˆëŠ”ì§€. 0ì€ ê°ì„± ë‹¨ì–´ ì—†ìŒ. (ë²”ìœ„: 0 ~ 1)
             * **ì´ˆë¡ìƒ‰ ì„ :** ì„±ê³¼ê°€ ì¢‹ì•˜ë˜(Good) ê¸€ë“¤ì˜ í‰ê·  'ëª©í‘œ' ì ìˆ˜ì…ë‹ˆë‹¤.
             * **ê°€ì •:** ì´ ì°¨íŠ¸ëŠ” ì‚¬ìš©ëœ ê°ì„± ì‚¬ì „ì˜ ì ìˆ˜ê°€ -1ì—ì„œ +1 ì‚¬ì´ì„ì„ ê°€ì •í•©ë‹ˆë‹¤.
             """)
             
             # [ì‹ ê·œ] ë™ì  ê°ì„± ì¶”ì²œ
             if target_s is not None and target_i is not None:
                recs = []
                if abs(senti_s - target_s) > 0.05:
                    if senti_s < target_s:
                        recs.append(f"ì„±ê³¼ ì¢‹ì€ ê¸€(ëª©í‘œ {target_s:.2f})ì€ **ë” ê¸ì •ì **ì…ë‹ˆë‹¤. (ê¸ì • ë‹¨ì–´ ì¶”ê°€)")
                    else:
                        recs.append(f"ì„±ê³¼ ì¢‹ì€ ê¸€(ëª©í‘œ {target_s:.2f})ì€ **ë” ì°¨ë¶„(ì¤‘ë¦½/ë¶€ì •)**í•©ë‹ˆë‹¤. (ê¸ì • ë‹¨ì–´ ê°ì†Œ)")
                if abs(senti_i - target_i) > 0.1:
                    if senti_i < target_i:
                        recs.append(f"ì„±ê³¼ ì¢‹ì€ ê¸€(ëª©í‘œ {target_i:.2f})ì€ **ê°ì„± í‘œí˜„ì´ ë” ê°•í•©ë‹ˆë‹¤.** (ê°ì„± ë‹¨ì–´ ì¶”ê°€)")
                    else:
                        recs.append(f"ì„±ê³¼ ì¢‹ì€ ê¸€(ëª©í‘œ {target_i:.2f})ì€ **ë” ê°ê´€ì (ê°ì„± ê°•ë„ ë‚®ìŒ)**ì…ë‹ˆë‹¤. (ê°ì„± ë‹¨ì–´ ê°ì†Œ)")
                
                if recs:
                    st.info("ğŸ’¡ **[ê°ì„± ì¶”ì²œ]** " + " ".join(recs))

        else:
             st.info("ğŸ’– ê°ì„± ì ìˆ˜ (S/I)\n\nTAB2ì—ì„œ 'ê°ì„± ì‚¬ì „'ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹¤í–‰í•˜ë©´ í™œì„±í™”ë©ë‹ˆë‹¤.")
        
        st.divider()

    # --- 4. LLM ë¦¬ë­ì»¤ ---
    st.subheader("2) ë¬¸ë§¥í˜• ì¶”ì²œ (LLM ë¦¬ë­ì»¤, ê¸°ì¡´ í›„ë³´ ë‹¨ì–´ë§Œ ì‚¬ìš©)")
    topk = st.slider("ì¶”ì²œ ê°œìˆ˜ (Top-K)", 3, 15, 8)
    btn = st.button("âœ¨ ë¬¸ë§¥í˜• ìš©ì–´ ì¶”ì²œ ìƒì„±", disabled=not LLM_OK)

    if btn:
        require_llm()
        if not full_draft.strip():
            st.warning("ì œëª©ì´ë‚˜ ë³¸ë¬¸ ì´ˆì•ˆ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            with st.spinner("LLMì´ ì´ˆì•ˆ ë¬¸ë§¥ì— ë§ëŠ” ë‹¨ì–´ë¥¼ ì„ ë³„ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    recs = llm_rerank_with_explanations(
                        draft_title=draft_title, 
                        draft_body=draft_body, 
                        candidates=candidates, 
                        topk=topk, 
                        audience=audience, 
                        tone=tone
                    )
                    st.success("ì¶”ì²œ ì™„ë£Œ!")
                    st.session_state["last_recs"] = recs
                    st.session_state["last_recs_time"] = time.strftime("%Y-%m-%d %H:%M:%S")
                    st.session_state["last_draft"] = full_draft
                    st.session_state["last_candidates"] = list(candidates)
                except Exception as e:
                    st.error(str(e))

    if st.session_state.get("last_recs"):
        for i, r in enumerate(st.session_state["last_recs"], 1):
            with st.expander(f"**{i}. {r.get('term','(ìš©ì–´)')}** â€” ë¶„ë¥˜: {r.get('category','')}", expanded=(i==1)):
                st.markdown(f"**ì¶”ì²œ ì´ìœ **: {r.get('why','')}") 
                st.markdown(f"**ì–´ë””ì— ì‚½ì…í• ê¹Œìš”?**: **{r.get('where_to_add','')}**") 
                if r.get("insertion_example"): st.markdown(f"**ì˜ˆì‹œ**: {r['insertion_example']}")
                if r.get("expected_effect"):   st.markdown(f"**ì˜ˆìƒë˜ëŠ” íš¨ê³¼**: {r['expected_effect']}") 
                if r.get("cautions"):          st.markdown(f"**ì‚¬ìš© ì‹œ ì£¼ì˜í•  ì **: {r['cautions']}")
    else:
        st.info("ì•„ì§ ìƒì„±ëœ ì¶”ì²œì´ ì—†ìŠµë‹ˆë‹¤. ìœ„ ë²„íŠ¼ìœ¼ë¡œ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")

    st.markdown("---")
    st.caption("â€¢ ì¶”ì²œ ìš©ì–´ëŠ” TAB2ì—ì„œ ë¶„ì„í•œ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ 'í† í”½ë³„ í•µì‹¬ ë‹¨ì–´' í’€ì—ì„œ ì„ ë³„ë©ë‹ˆë‹¤.")

# ================= TAB2 =================
with TAB2:
    st.header("ğŸ“Š ë°ì´í„° ì—…ë¡œë“œ ë° ì„±ê³¼ ë¶„ì„")
    st.caption("ì½˜í…ì¸  ì„±ê³¼ë¥¼ ë¶„ì„í•˜ë ¤ë©´ ì•„ë˜ ë‘ ê°€ì§€ CSV íŒŒì¼ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤.")
    c1, c2 = st.columns(2)
    with c1:
        f_content = st.file_uploader("ğŸ“ (1) ì½˜í…ì¸  ìƒì„¸ CSV (article_id, title, content, date í¬í•¨)", type=["csv"], key="content")
    with c2:
        f_metrics = st.file_uploader("ğŸ“ˆ (2) ì„±ê³¼ ì¸¡ì • CSV (article_id, views_total, likes, comments, period í¬í•¨)", type=["csv"], key="metrics")

    st.markdown("---")
    st.subheader("âš™ï¸ ë¶„ì„ ì„¤ì •")
    c3, c4, c5 = st.columns(3)
    lda_topics = c3.number_input("ì£¼ì œ ë¶„ë¥˜ ê°œìˆ˜ (LDA í† í”½ ìˆ˜)", min_value=5, max_value=40, value=10, step=1)
    c4.markdown("**ì½˜í…ì¸  ë§¤ë ¥ ì ìˆ˜ ê°€ì¤‘ì¹˜** (ì´í•© 1.0)")
    wv = c4.slider("ì¡°íšŒìˆ˜ ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.4, 0.05, key="wv_slider")
    wl = c4.slider("ì¢‹ì•„ìš” ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.4, 0.05, key="wl_slider")
    wc = c4.slider("ëŒ“ê¸€ ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.2, 0.05, key="wc_slider")
    
    f_lex = c5.file_uploader("ğŸ’– ê°ì„± ì‚¬ì „ CSV (ì„ íƒ: word,score)", type=["csv"], key="lex")

    st.session_state.setdefault('analysis_done', False)

    def prepare_by_mode(df_in: pd.DataFrame, mode_cfg: dict, lda_topics_ui: int):
        if mode_cfg["sample_n"]:
            n = min(mode_cfg["sample_n"], len(df_in))
            df_work = df_in.sample(n=n, random_state=42).reset_index(drop=True)
            st.info(f"ìƒ˜í”Œë§ ì‚¬ìš©: **{n}í–‰**ìœ¼ë¡œ ì¶•ì†Œ(ì›ë³¸ {len(df_in)}).")
        else:
            df_work = df_in.copy()
        
        n_topics = mode_cfg["lda_topics"] if mode_cfg["lda_topics"] > 0 else int(lda_topics_ui) 

        lda_kwargs = dict(
            n_topics=n_topics,
            max_features=mode_cfg["max_features"],
            batch_size=mode_cfg["batch_size"],
            n_epochs=mode_cfg["n_epochs"],
        )
        clf_kwargs = dict(
            epochs=mode_cfg["clf_epochs"],
            batch_size=mode_cfg["clf_batch"],
            ngram_range=mode_cfg["ngram_range"],
        )
        return df_work, lda_kwargs, clf_kwargs

    @st.cache_resource(show_spinner=False)
    def cached_lda_run(texts_tuple, n_topics, max_features, batch_size, n_epochs):
        return run_lda_topics_streaming(
            list(texts_tuple), n_topics=n_topics, max_features=max_features,
            batch_size=batch_size, n_epochs=n_epochs
        )

    if f_content is not None and f_metrics is not None:
        try:
            if (st.session_state.get('f_content_name') != f_content.name) or \
               (st.session_state.get('f_metrics_name') != f_metrics.name):
                st.session_state['analysis_done'] = False
                st.session_state['f_content_name'] = f_content.name
                st.session_state['f_metrics_name'] = f_metrics.name
                st.info("ìƒˆë¡œìš´ íŒŒì¼ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¶„ì„ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

            df_c = coerce_article_id(read_csv_robust(f_content))
            df_m_raw = coerce_article_id(read_csv_robust(f_metrics)) 
            df_c["article_id"] = df_c["article_id"].astype(str)
            df_m_raw["article_id"] = df_m_raw["article_id"].astype(str)

            st.info("ì„±ê³¼ CSV(2)ê°€ ê¸°ê°„ë³„ ë°ì´í„°(long format)ì…ë‹ˆë‹¤. article_id ê¸°ì¤€ìœ¼ë¡œ ì„±ê³¼(views, likes, comments)ë¥¼ **í•©ì‚°(sum)**í•©ë‹ˆë‹¤.")
            metric_cols = ["views_total", "likes", "comments"]
            
            for col in metric_cols:
                if col in df_m_raw.columns:
                    df_m_raw[col] = pd.to_numeric(df_m_raw[col], errors='coerce').fillna(0)
            
            df_m = df_m_raw.groupby("article_id")[metric_cols].sum().reset_index()

            df = pd.merge(df_c, df_m, on="article_id", how="inner")
            
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
            st.session_state['df_for_analysis'] = df.copy() 
            
            st.success(f"ë°ì´í„° ë³‘í•© ì™„ë£Œ: **{len(df)}** ê±´ì˜ **ê³ ìœ  ì½˜í…ì¸ ** ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ (ì„±ê³¼ëŠ” Total Sum ê¸°ì¤€)")

            df = build_engagement(df, w_views=wv, w_likes=wl, w_comments=wc)
            df = label_quality_by_quantile(df, col="engagement", low_q=0.33, high_q=0.66)
            
            st.session_state['df_for_analysis']['quality_label'] = df['quality_label'].values

            st.subheader("1. ì½˜í…ì¸  ë“±ê¸‰ í™•ì¸")
            st.caption("ì½˜í…ì¸  ë§¤ë ¥ ì ìˆ˜(Total Engagement)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 33%ëŠ” 'ìƒ (good)', í•˜ìœ„ 33%ëŠ” 'í•˜ (bad)'ë¡œ ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤.")
            grade_counts = df["quality_label"].value_counts().rename({"good": "ìƒ (Good)", "medium": "ì¤‘ (Medium)", "bad": "í•˜ (Bad)"})
            st.dataframe(grade_counts.to_frame(name="ì½˜í…ì¸  ìˆ˜"), use_container_width=True)


            colm1, colm2 = st.columns(2)
            do_quick = colm1.button("âš¡ï¸ ë¹ ë¥¸ ë¶„ì„ (ìƒ˜í”Œ/ê²½ëŸ‰ ëª¨ë¸)", use_container_width=True)
            do_full = colm2.button("ğŸ”¬ ì •ë°€ ë¶„ì„ (ì „ì²´/ê³ ì •ë°€ ëª¨ë¸)", use_container_width=True)

            if do_quick or do_full:
                # ----------------- ë¶„ì„ ì‹¤í–‰ ë¸”ë¡ ì‹œì‘ -----------------
                mode = "quick" if do_quick else "full"
                cfg = MODE_CFG[mode] 
                df_work, lda_kw, clf_kw = prepare_by_mode(df, cfg, lda_topics)

                # ===== LDA =====
                st.subheader("2. ì£¼ì œ(í† í”½) ë¶„ë¥˜ ë° ë¶„ì„")
                with st.spinner(f"LDA({mode}) ì£¼ì œ ë¶„ì„ ì‹¤í–‰ ì¤‘â€¦ (ë¶ˆìš©ì–´ ì ìš©)"):
                    df_sig = tuple(df_work["content"].fillna("").tolist())
                    df_topic, vect, lda, W = cached_lda_run(tuple(df_sig), **lda_kw)
                df_work["topic"] = df_topic["topic"]
                st.write("ì£¼ì œ ë¶„ë¥˜ ê²°ê³¼ (ìƒ˜í”Œ):", df_work[["article_id","topic","title"]].head(10))

                topics_top_words = get_topic_top_words(lda, vect, topn=8)
                st.info("í† í”½ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM/íœ´ë¦¬ìŠ¤í‹±ì„ ì‚¬ìš©í•´ ì£¼ì œ ì´ë¦„ê³¼ ì„¤ëª…ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.")
                with st.spinner("LLM/íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í† í”½ ë¼ë²¨ë§ ì¤‘..."):
                    topic_labels = llm_name_topics(topics_top_words)

                st.session_state["topic_labels"] = topic_labels
                st.session_state["lda_vect"] = vect 
                st.session_state["lda_model"] = lda
                
                if 'df_for_analysis' in st.session_state and st.session_state['df_for_analysis'] is not None:
                    full_texts = (st.session_state['df_for_analysis']["title"].fillna("") + " " + st.session_state['df_for_analysis']["content"].fillna("")).tolist()
                    full_X = vect.transform(full_texts)
                    full_topics = lda.transform(full_X).argmax(axis=1)
                    st.session_state['df_for_analysis']['topic'] = full_topics
                    st.info("ì „ì²´ ë°ì´í„°(TAB1/TAB4ìš©)ì— í† í”½ ë¶„ë¥˜ ì ìš© ì™„ë£Œ.")

                # ===== ë¶„ë¥˜ê¸° í•™ìŠµ =====
                st.subheader("3. ì½˜í…ì¸  ë“±ê¸‰ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
                df_train = df_work[df_work["quality_label"] != "medium"]
                st.info(f"í•™ìŠµ ë°ì´í„°ì…‹: 'ìƒ(Good)'ê³¼ 'í•˜(Bad)' {len(df_train)}ê±´ ì‚¬ìš©.")
                
                with st.spinner(f"SGD ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘â€¦ (ë¶ˆìš©ì–´ ì ìš©)"):
                    clf_pack = train_logreg_with_progress(
                        texts = df_train["title"].fillna("") + " " + df_train["content"].fillna(""),
                        labels = df_train["quality_label"],
                        stoplist=None, 
                        **clf_kw
                    )
                st.session_state["clf_pack"] = clf_pack
                st.success("ë“±ê¸‰ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
                
                # ===== [ìˆ˜ì •] í† í”½ ë‹¨ì–´ ì€í–‰ êµ¬ì¶• (Chi2) =====
                st.subheader("4. í† í”½ë³„ í•µì‹¬ ë‹¨ì–´ ì€í–‰ êµ¬ì¶• (TAB1 ì¶”ì²œ ê¸°ë°˜)")
                with st.spinner("í† í”½ë³„ ì„±ê³¼ ìš°ìˆ˜/ì €ì¡° ë‹¨ì–´ ë¶„ì„ ì¤‘â€¦ (Chi2 í†µê³„ ì ìš©)"):
                    df_full_with_topic = st.session_state.get('df_for_analysis', df_work) 
                    # [ìˆ˜ì •] Chi2 í•¨ìˆ˜ í˜¸ì¶œ, lda_vect ì „ë‹¬
                    topic_term_bank = build_topic_term_bank(df_full_with_topic, vect, lda, topn=50) 
                    st.session_state["topic_term_bank"] = topic_term_bank
                    st.success("í† í”½ ê¸°ë°˜ ìš©ì–´ ì€í–‰(Chi2) êµ¬ì¶• ì™„ë£Œ! (TAB1ì—ì„œ í™œìš© ê°€ëŠ¥)")

                # [ìˆ˜ì •] ê°ì„± ë¶„ì„ê¸° ìƒì„± ë¡œì§ (TAB1ìš©)
                if f_lex is not None:
                    st.subheader("5. ê°ì„± ë¶„ì„ê¸° ìƒì„± (TAB1ìš©)")
                    with st.spinner("ê°ì„± ì‚¬ì „ì„ ì²˜ë¦¬í•˜ì—¬ TAB1ì—ì„œ ì‚¬ìš©í•  ë¶„ì„ê¸°ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                        try:
                            f_lex.seek(0)
                            lex_df = read_csv_robust(f_lex)
                            if not set(["word","score"]).issubset(lex_df.columns):
                                st.warning("ê°ì„± ì‚¬ì „ì— 'word', 'score' ì»¬ëŸ¼ì´ ì—†ì–´ S/I ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                                st.session_state['sentiment_pack'] = None
                            else:
                                lex_dict = dict(zip(lex_df["word"].astype(str), lex_df["score"].astype(float)))
                                senti_cv = CountVectorizer(min_df=1, stop_words=STOPWORDS_KO)
                                texts = (df["title"].fillna("") + " " + df["content"].fillna("")).tolist() 
                                senti_cv.fit(texts) 
                                
                                # [ìˆ˜ì •] 'Good' ê¸€ì˜ í‰ê·  S/I ì ìˆ˜ ê³„ì‚° ë° NaN ë°©ì–´
                                df_work_senti = compute_sentiment_SI(df_work, senti_cv, lex_dict)
                                avg_s_good = df_work_senti[df_work_senti['quality_label'] == 'good']['S'].mean()
                                avg_i_good = df_work_senti[df_work_senti['quality_label'] == 'good']['I'].mean()

                                # [ìˆ˜ì •] NaNì¸ ê²½ìš° 0.0ìœ¼ë¡œ í´ë°±
                                target_s_val = float(avg_s_good) if pd.notna(avg_s_good) else 0.0
                                target_i_val = float(avg_i_good) if pd.notna(avg_i_good) else 0.0
                                
                                st.session_state['sentiment_pack'] = {
                                    'lex': lex_dict, 
                                    'cv': senti_cv,
                                    'target_s': target_s_val, 
                                    'target_i': target_i_val
                                }
                                st.session_state['lex_file_name'] = f_lex.name
                                st.success(f"ê°ì„± ë¶„ì„ê¸°(S/I)ê°€ TAB1ì„ ìœ„í•´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ëª©í‘œ S: {target_s_val:.2f}, ëª©í‘œ I: {target_i_val:.2f})")
                        except Exception as e:
                            st.error(f"ê°ì„± ì‚¬ì „ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                            st.session_state['sentiment_pack'] = None
                else:
                    st.session_state['sentiment_pack'] = None 

                # [ìˆ˜ì •] ë¶„ì„ ì™„ë£Œ í”Œë˜ê·¸ ë° ì‹œê°í™”ìš© ë°ì´í„° ì €ì¥
                st.session_state['analysis_done'] = True
                st.session_state['df_work_for_viz'] = df_work.copy()
                st.session_state['df_m_raw_for_viz'] = df_m_raw.copy()
                st.session_state['topic_labels_for_viz'] = topic_labels
                st.rerun() 
                
                # ----------------- ë¶„ì„ ì‹¤í–‰ ë¸”ë¡ ë -----------------

            # [ìˆ˜ì •] ì‹œê°í™” ë¸”ë¡ ì „ì²´ë¥¼ 'analysis_done' í”Œë˜ê·¸ ê¸°ë°˜ìœ¼ë¡œ ë°–ìœ¼ë¡œ ì´ë™
            if st.session_state.get('analysis_done', False):
                
                # [ìˆ˜ì •] ì„¸ì…˜ì—ì„œ ì‹œê°í™”ìš© ë°ì´í„° ë¡œë“œ
                df_work_viz = st.session_state.get('df_work_for_viz')
                df_m_raw_viz = st.session_state.get('df_m_raw_for_viz')
                topic_labels_viz = st.session_state.get('topic_labels_for_viz', {})
                clf_pack_viz = st.session_state.get('clf_pack') 
                senti_pack_viz = st.session_state.get('sentiment_pack')

                if df_work_viz is None or df_m_raw_viz is None or topic_labels_viz is None or clf_pack_viz is None:
                    st.error("ì‹œê°í™” ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. ë¶„ì„ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
                    st.stop()

                st.markdown("---")
                st.header("ğŸ”¬ ì¶”ê°€ ë¶„ì„ ì‹œê°í™”")

                # [ìˆ˜ì •] í† í”½ í•„í„°
                topic_names_list = ["ì „ì²´ (All)"] + [v.get('name', k) for k,v in topic_labels_viz.items()]
                filter_topic_name = st.selectbox("ğŸ”¬ ì‹œê°í™” í† í”½ í•„í„°", topic_names_list)

                # [ìˆ˜ì •] í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„± (df_viz)
                topic_names_map = {int(k.split(' ')[1]): v.get('name', k) for k, v in topic_labels_viz.items()}
                
                if 'topic_name' not in df_work_viz.columns:
                     df_work_viz['topic_name'] = df_work_viz['topic'].map(topic_names_map).fillna('ê¸°íƒ€')

                if filter_topic_name == "ì „ì²´ (All)":
                    df_viz = df_work_viz
                else:
                    df_viz = df_work_viz[df_work_viz['topic_name'] == filter_topic_name]

                # [ì‹œê°í™” 1] í† í”½ë³„ ì„±ê³¼ (ì´ ì°¨íŠ¸ëŠ” í•„í„° ì ìš© ì•ˆí•¨)
                st.subheader("A. ì£¼ì œë³„ ì„±ê³¼ ë¶„í¬")
                try:
                    fig_topic_box = px.box(
                        df_work_viz, 
                        x='topic_name', 
                        y='engagement', 
                        color='topic_name',
                        title='ì£¼ì œ(í† í”½)ë³„ ì½˜í…ì¸  ë§¤ë ¥ ì ìˆ˜(Total Engagement) ë¶„í¬',
                        labels={'topic_name': 'ì£¼ì œëª…', 'engagement': 'ì½˜í…ì¸  ë§¤ë ¥ ì ìˆ˜(ì´í•©)'}
                    )
                    st.plotly_chart(fig_topic_box, use_container_width=True)
                except Exception as e:
                    st.error(f"í† í”½ ì„±ê³¼ ì‹œê°í™” ì‹¤íŒ¨: {e}")

                # [ì‹œê°í™” 2] ì„±ê³¼ í•µì‹¬ í‚¤ì›Œë“œ (ì „ì²´ ì£¼ì œ ê¸°ì¤€)
                st.subheader("B. ì„±ê³¼ ì˜ˆì¸¡ í•µì‹¬ í‚¤ì›Œë“œ (ì „ì²´ ì£¼ì œ ê¸°ì¤€)")
                st.caption("â”” ì´ ì°¨íŠ¸ëŠ” 'ì „ì²´ ì£¼ì œ'ì— ëŒ€í•´ í•™ìŠµëœ **ë‹¨ì¼ ëª¨ë¸**ì˜ ê²°ê³¼ì´ë¯€ë¡œ í† í”½ í•„í„°ê°€ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                if clf_pack_viz: 
                    try:
                        good_df = pd.DataFrame(clf_pack_viz['good_terms'], columns=['term', 'score'])
                        bad_df = pd.DataFrame(clf_pack_viz['bad_terms'], columns=['term', 'score'])
                        drivers_df = pd.concat([good_df, bad_df])
                        drivers_df['type'] = np.where(drivers_df['score'] > 0, 'ì„±ê³¼ ìš°ìˆ˜ (Good)', 'ì„±ê³¼ ì €ì¡° (Bad)')

                        fig_drivers = px.bar(
                            drivers_df.sort_values('score'), 
                            x='score', 
                            y='term', 
                            color='type',
                            orientation='h', 
                            title='ì½˜í…ì¸  ë“±ê¸‰ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œ',
                            labels={'term': 'í‚¤ì›Œë“œ', 'score': 'ì˜í–¥ë ¥ ì ìˆ˜ (ê³„ìˆ˜)'},
                            color_discrete_map={'ì„±ê³¼ ìš°ìˆ˜ (Good)': 'blue', 'ì„±ê³¼ ì €ì¡° (Bad)': 'red'}
                        )
                        fig_drivers.update_layout(yaxis_title="í‚¤ì›Œë“œ")
                        st.plotly_chart(fig_drivers, use_container_width=True)
                    except Exception as e:
                        st.error(f"í‚¤ì›Œë“œ ì‹œê°í™” ì‹¤íŒ¨: {e}")

                # [ì‹œê°í™” 3] ê°ì„±ê³¼ ì„±ê³¼ (í•„í„° ì ìš©)
                st.subheader(f"C. ê°ì„±(S/I)ê³¼ ì„±ê³¼ ({filter_topic_name})")
                if f_lex is not None:
                    if senti_pack_viz and senti_pack_viz.get('cv') and senti_pack_viz.get('lex'):
                        if 'S' not in df_viz.columns:
                            with st.spinner(f"({filter_topic_name}) ê°ì„±(S/I) ì ìˆ˜ ê³„ì‚° ì¤‘..."):
                                df_viz = compute_sentiment_SI(df_viz, senti_pack_viz['cv'], senti_pack_viz['lex'])
                        
                        if 'S' in df_viz.columns and df_viz['S'].abs().sum() > 0:
                            fig_senti_scatter = px.scatter(
                                df_viz, 
                                x='S', 
                                y='engagement', 
                                color='quality_label',
                                title=f'ì½˜í…ì¸  ê°ì„±(S)ê³¼ ì„±ê³¼(Total Engagement) ê´€ê³„ ({filter_topic_name})',
                                labels={'S': 'í‰ê·  ê°ì„± ì ìˆ˜ (S)', 'engagement': 'ì½˜í…ì¸  ë§¤ë ¥ ì ìˆ˜(ì´í•©)'},
                                hover_data=['title']
                            )
                            st.plotly_chart(fig_senti_scatter, use_container_width=True)
                        else:
                            st.warning(f"'{filter_topic_name}' í† í”½ì—ì„œ ìœ íš¨í•œ ê°ì„± ì ìˆ˜(S)ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.caption("ğŸ’– 'ê°ì„± ì‚¬ì „ CSV'ë¥¼ ì—…ë¡œë“œí•˜ë©´ ê°ì„±-ì„±ê³¼ ê´€ê³„ ë¶„ì„ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")
                else:
                    st.caption("ğŸ’– 'ê°ì„± ì‚¬ì „ CSV'ë¥¼ ì—…ë¡œë“œí•˜ë©´ ê°ì„±-ì„±ê³¼ ê´€ê³„ ë¶„ì„ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")
                
                # [ì‹œê°í™” 4] ì‹œê°„ëŒ€ë³„ ì„±ê³¼ ì¶”ì„¸ (ì „ì²´ ì£¼ì œ ê¸°ì¤€)
                st.subheader("D. ê¸°ê°„ë³„ ì„±ê³¼ ì¶”ì„¸ (ì „ì²´ ì£¼ì œ ê¸°ì¤€)")
                st.caption("â”” ì´ ì°¨íŠ¸ëŠ” 'ì „ì²´ ì£¼ì œ'ì˜ **í‰ê· ** ì¶”ì„¸ì´ë©°, í† í”½ í•„í„°ê°€ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                if 'period' in df_m_raw_viz.columns: 
                    try:
                        df_trend = df_m_raw_viz.groupby('period')[['views_total', 'likes', 'comments']].mean().reset_index()
                        df_trend['parsed_period'] = pd.to_datetime(df_trend['period'], errors='coerce')
                        df_trend = df_trend.dropna(subset=['parsed_period']).sort_values('parsed_period')
                        
                        if not df_trend.empty:
                            df_trend_melted = df_trend.melt(
                                id_vars='parsed_period', 
                                value_vars=['views_total', 'likes', 'comments'],
                                var_name='Metric', 
                                value_name='Average Value'
                            )
                            
                            fig_time_trend = px.line(
                                df_trend_melted, 
                                x='parsed_period', 
                                y='Average Value', 
                                color='Metric', 
                                title='ê¸°ê°„(Period)ë³„ í‰ê·  ì„±ê³¼(ì¡°íšŒ/ì¢‹ì•„ìš”/ëŒ“ê¸€) ì¶”ì„¸',
                                labels={'parsed_period': 'ê¸°ê°„', 'Average Value': 'í‰ê·  ì„±ê³¼ ê°’', 'Metric': 'ì„±ê³¼ ì§€í‘œ'}
                            )
                            st.plotly_chart(fig_time_trend, use_container_width=True)
                        else:
                            st.warning("ìœ íš¨í•œ 'period' ì»¬ëŸ¼ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì‹œê°„ëŒ€ë³„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"ê¸°ê°„(period) ì»¬ëŸ¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                else:
                    st.caption("ğŸ“… ì„±ê³¼ ì¸¡ì • CSV(2)ì— 'period' ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì‹œê°„ëŒ€ë³„ ì¶”ì„¸ ë¶„ì„ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")
            
        except Exception as e:
            st.error(f"ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")


# ================= TAB3 =================
with TAB3:
    st.header("ğŸ› ï¸ LLM íŒŒì¸íŠœë‹ (ê³¨ê²©)")
    st.info("ì´ íƒ­ì€ LLM íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤ì— í•„ìš”í•œ ìµœì†Œí•œì˜ êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‹¤ì œ íŒŒì¸íŠœë‹ API ì—°ë™ ë° ë°ì´í„° ì „ì²˜ë¦¬ ì½”ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    st.code("""
# [TODO] 1. í•™ìŠµ ë°ì´í„° ì¤€ë¹„
# df_train = ... (good/bad ì½˜í…ì¸  ë°ì´í„°í”„ë ˆì„)
# df_train['prompt'] = 'ë‹¤ìŒ ì½˜í…ì¸ ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë” ë†’ì€ ì„±ê³¼ë¥¼ ìœ„í•œ ìµœì ì˜ ì œëª©ì„ ì¶”ì²œí•´ì¤˜: ' + df_train['content']
# df_train['completion'] = df_train['title'] # (ì˜ˆì‹œ)

# [TODO] 2. ë°ì´í„°ì…‹ í˜•ì‹ ë³€í™˜ (JSONL)
# training_data = [{'prompt': p, 'completion': c} for p, c in zip(df_train['prompt'], df_train['completion'])]
# jsonl_output = "\n".join([json.dumps(item, ensure_ascii=False) for item in training_data])

# [TODO] 3. OpenAI íŒŒì¼ ì—…ë¡œë“œ ë° íŠœë‹ API í˜¸ì¶œ
# if st.button("íŒŒì¸íŠœë‹ ì‹œì‘"):
#     try:
#         file = client.files.create(file=io.BytesIO(jsonl_output.encode('utf-8')), purpose='fine-tune')
#         job = client.fine_tuning.jobs.create(
#             training_file=file.id, model='gpt-3.5-turbo-0125' # ë˜ëŠ” gpt-4o-mini
#         )
#         st.success(f"íŒŒì¸íŠœë‹ ì‘ì—… ì‹œì‘ë¨: {job.id}")
#     except Exception as e:
#         st.error(f"íŒŒì¸íŠœë‹ ì‹¤íŒ¨: {e}")
    """, language="python")

# ================= [ì‹ ê·œ] TAB4 =================
with TAB4:
    st.header("ğŸ”¬ ëª¨ë¸ ê´€ë¦¬ì (Admin)")
    st.info("ì´ íƒ­ì€ `TAB2`ì—ì„œ ë¶„ì„ì´ ì™„ë£Œëœ í›„ í™œì„±í™”ë©ë‹ˆë‹¤. í˜„ì¬ ì ìš©ëœ ëª¨ë¸ì˜ ìƒíƒœì™€ ì„±ëŠ¥ì„ ì ê²€í•©ë‹ˆë‹¤.")

    # ì„¸ì…˜ì—ì„œ ë°ì´í„° ë¡œë“œ
    df_full = st.session_state.get('df_for_analysis')
    topic_bank = st.session_state.get('topic_term_bank')
    clf_pack = st.session_state.get('clf_pack')
    lda_model = st.session_state.get('lda_model')
    lda_vect = st.session_state.get('lda_vect')
    topic_labels = st.session_state.get('topic_labels', {})

    # [ìˆ˜ì •] DataFrame __nonzero__ ì˜¤ë¥˜ ìˆ˜ì •: 'is None or .empty' ì‚¬ìš©
    if df_full is None or df_full.empty or clf_pack is None or lda_model is None or topic_bank is None:
        st.error("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. TAB2ì—ì„œ ë¨¼ì € 'ë¹ ë¥¸ ë¶„ì„' ë˜ëŠ” 'ì •ë°€ ë¶„ì„'ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")
    else:
        # --- 1. ë¶ˆìš©ì–´ ---
        st.subheader("1. ë¶ˆìš©ì–´(Stopwords) ê´€ë¦¬")
        with st.expander("í˜„ì¬ ì ìš© ì¤‘ì¸ ê¸°ë³¸ ë¶ˆìš©ì–´ ëª©ë¡ ë³´ê¸°"):
            st.text(f"ì´ {len(STOPWORDS_KO)}ê°œ ë‹¨ì–´:")
            st.json(STOPWORDS_KO)
        
        # [ìˆ˜ì •] 'ë¶ˆìš©ì–´ ì˜ì‹¬' ë¡œì§ ë³€ê²½ (ê³ ë¹ˆë„ ì¼ë°˜ì–´)
        with st.expander("ë¶ˆìš©ì–´ ì˜ì‹¬ ë‹¨ì–´ ë³´ê¸° (ê³ ë¹ˆë„ ì¼ë°˜ ë‹¨ì–´)"):
            st.markdown("í† í”½/ì„±ê³¼ì™€ ê´€ê³„ì—†ì´ **ëª¨ë“  ë¬¸ì„œì— ë„ˆë¬´ ìì£¼ ë“±ì¥**í•˜ëŠ” ë‹¨ì–´(ì˜ˆ: 10% ì´ìƒ)ì…ë‹ˆë‹¤. 'ë¯¸êµ­' ê°™ì€ ê³ ìœ ëª…ì‚¬ë³´ë‹¤ **'ê²ƒì´ë‹¤', 'ìˆë‹¤'** ê°™ì€ ì¼ë°˜ ë‹¨ì–´ê°€ ì—¬ê¸° ëœ¬ë‹¤ë©´ ë¶ˆìš©ì–´ ì¶”ê°€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
            with st.spinner("ëª¨ë“  ë¬¸ì„œì—ì„œ ê³ ë¹ˆë„ ì¼ë°˜ ë‹¨ì–´ë¥¼ ì¶”ì¶œ ì¤‘ì…ë‹ˆë‹¤..."):
                suspected = get_suspected_stopwords(df_full, k=50)
                if suspected:
                    st.warning("ì•„ë˜ ë‹¨ì–´ë“¤ì€ ì´ë¯¸ ê¸°ë³¸ ë¶ˆìš©ì–´(STOPWORDS_KO)ì— í¬í•¨ëœ ê²ƒì„ ì œì™¸í•œ ê³ ë¹ˆë„ ë‹¨ì–´ì…ë‹ˆë‹¤.")
                    st.text(", ".join(suspected))
                else:
                    st.info("ë¶ˆìš©ì–´ ì˜ì‹¬ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # --- 2. í† í”½ ë‹¨ì–´ ì€í–‰ ---
        st.subheader("2. í† í”½ë³„ í•µì‹¬ ë‹¨ì–´ ì€í–‰ (TAB1 ì¶”ì²œ ê¸°ë°˜)")
        # [ìˆ˜ì •] ë‹¨ì–´ ì€í–‰ ìƒì„± ë¡œì§ ë³€ê²½ ì•ˆë‚´ (Chi2)
        st.markdown("""
        [ìˆ˜ì •] ì´ ë‹¨ì–´ ì€í–‰ì€ `build_topic_term_bank` (Chi2) í•¨ìˆ˜ë¡œ ìƒì„±ë©ë‹ˆë‹¤.
        - **ì„±ê³¼ ìš°ìˆ˜ ë‹¨ì–´ (Good):** í•´ë‹¹ í† í”½ì˜ 'Good' ë¼ë²¨ê³¼ í†µê³„ì (**ì¹´ì´ì œê³±**)ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ê²Œ ì—°ê´€ëœ ë‹¨ì–´ì…ë‹ˆë‹¤. (ì¶”ì²œ)
        - **ì„±ê³¼ ì €ì¡° ë‹¨ì–´ (Bad):** í•´ë‹¹ í† í”½ì˜ 'Bad' ë¼ë²¨ê³¼ í†µê³„ì (**ì¹´ì´ì œê³±**)ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ê²Œ ì—°ê´€ëœ ë‹¨ì–´ì…ë‹ˆë‹¤. (ë¹„ê¶Œì¥)
        - **ë‹¨ìˆœ ë¹ˆë„ ë‹¨ì–´ (All):** ì„±ê³¼ì™€ ë¬´ê´€í•˜ê²Œ í•´ë‹¹ í† í”½ì—ì„œ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ì…ë‹ˆë‹¤.
        """)
        st.caption("â”” ì´ ë‹¨ì–´ ì€í–‰ì€ TAB2ì—ì„œ 'ì „ì²´ ê¸°ê°„' ë°ì´í„°ë¡œ í•™ìŠµëœ **ê³ ì •ëœ ëª¨ë¸**ì˜ ê²°ê³¼ì…ë‹ˆë‹¤. (ìµœê·¼ í•œ ë‹¬ ë™ì  ë‹¨ì–´ëŠ” TAB1 ì°¸ì¡°)")
        
        if topic_labels:
            topic_names_map = {v.get('name', k): int(k.split(' ')[1]) for k,v in topic_labels.items()}
            selected_name = st.selectbox("í™•ì¸í•  í† í”½ ì„ íƒ", list(topic_names_map.keys()))
            
            if selected_name:
                selected_id = topic_names_map[selected_name]
                
                if selected_id not in topic_bank:
                     st.error(f"í† í”½ {selected_id}ê°€ ë‹¨ì–´ ì€í–‰ì— ì—†ìŠµë‹ˆë‹¤. (TAB2 ì¬ì‹¤í–‰ í•„ìš”)")
                else:
                    bank_data = topic_bank[selected_id]
                    # [ìˆ˜ì •] status í‚¤ë¥¼ í™•ì¸í•˜ì—¬ êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ í‘œì‹œ
                    if bank_data.get("status") == "ok":
                        c_g, c_b, c_a = st.columns(3)
                        c_g.dataframe({"ì„±ê³¼ ìš°ìˆ˜ ë‹¨ì–´ (Good)": [w for w,s in bank_data['good'][:20]]})
                        c_b.dataframe({"ì„±ê³¼ ì €ì¡° ë‹¨ì–´ (Bad)": [w for w,s in bank_data['bad'][:20]]})
                        c_a.dataframe({"ë‹¨ìˆœ ë¹ˆë„ ë‹¨ì–´ (All)": [w for w,s in bank_data['all'][:20]]})
                    else:
                        st.warning(f"'{selected_name}' í† í”½ì˜ ë‹¨ì–´ ì€í–‰ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n**ì‚¬ìœ :** {bank_data.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")


        # --- 3. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ---
        st.subheader("3. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
        st.info("ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì „ì²´ ë°ì´í„°ë¥¼ 80(í•™ìŠµ)/20(í…ŒìŠ¤íŠ¸)ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ì¬í‰ê°€í•©ë‹ˆë‹¤.")
        
        if st.button("ğŸš€ ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰ (80/20 ë¶„í• )"):
            
            # 1. ë¶„ë¥˜(Classifier) ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
            st.markdown("#### A. ì„±ê³¼ ì˜ˆì¸¡ ëª¨ë¸ (SGDClassifier) ì„±ëŠ¥")
            st.caption("ëª©í‘œ: 'Good' / 'Bad' ë¼ë²¨ì„ ì–¼ë§ˆë‚˜ ì˜ ë§ì¶”ëŠ”ê°€? (ë¶„ë¥˜ ëª¨ë¸)")
            
            with st.spinner("ì„±ê³¼ ì˜ˆì¸¡ ëª¨ë¸ì„ 80/20 ë°ì´í„°ë¡œ ì¬í•™ìŠµ ë° í‰ê°€ ì¤‘..."):
                try:
                    df_trainable = df_full[df_full['quality_label'] != 'medium'].copy()
                    texts = (df_trainable["title"].fillna("") + " " + df_trainable["content"].fillna("")).tolist()
                    labels = df_trainable["quality_label"].tolist()

                    X_train_txt, X_test_txt, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)
                    
                    tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=30000, min_df=2, stop_words=STOPWORDS_KO)
                    X_train_vec = tfidf.fit_transform(X_train_txt)
                    X_test_vec = tfidf.transform(X_test_txt)
                    
                    clf_test = SGDClassifier(loss="log_loss", learning_rate="optimal", alpha=1e-5, random_state=42)
                    clf_test.fit(X_train_vec, y_train)
                    
                    y_pred = clf_test.predict(X_test_vec)
                    
                    # [ìˆ˜ì •] classification_reportë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°›ì•„ DataFrameìœ¼ë¡œ ë³€í™˜
                    report_dict = classification_report(y_test, y_pred, output_dict=True)
                    report_df = pd.DataFrame(report_dict).transpose()
                    st.text("Classification Report (Test Set):")
                    st.dataframe(report_df.round(3)) # [ìˆ˜ì •] í‘œ(DataFrame)ë¡œ í‘œì‹œ
                    
                    cm = confusion_matrix(y_test, y_pred, labels=['good', 'bad'])
                    st.text("Confusion Matrix (Test Set):")
                    st.dataframe(pd.DataFrame(cm, index=['True: Good', 'True: Bad'], columns=['Pred: Good', 'Pred: Bad']))

                except Exception as e:
                    st.error(f"ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            
            st.markdown("---")
            
            # 2. í† í”½(LDA) ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
            st.markdown("#### B. ì£¼ì œ ë¶„ë¥˜ ëª¨ë¸ (LDA) ì„±ëŠ¥")
            st.caption("ëª©í‘œ: ë¬¸ì„œë¥¼ ì–¼ë§ˆë‚˜ ì¼ê´€ì„± ìˆëŠ” ì£¼ì œë¡œ ì˜ ë¬¶ì—ˆëŠ”ê°€? (ë¹„ì§€ë„ í•™ìŠµ)")
            st.warning("""
            **ì¤‘ìš”:** LDAëŠ” ì •ë‹µì´ ì—†ëŠ” ë¹„ì§€ë„ í•™ìŠµì´ë¯€ë¡œ **Accuracy(ì •í™•ë„)ë‚˜ F1-Scoreë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.** ëŒ€ì‹ , ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ 'í™•ì‹ ì„ ê°€ì§€ê³ ' ë¬¸ì„œë¥¼ ë¶„ë¥˜í–ˆëŠ”ì§€, ì£¼ì œê°€ ì–¼ë§ˆë‚˜ ëª…í™•íˆ êµ¬ë¶„ë˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” **Perplexity(í˜¼ì¡ë„)**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            - **Perplexity (í˜¼ì¡ë„):** **ë‚®ì„ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤.** ëª¨ë¸ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
            """)
            
            with st.spinner("LDA ëª¨ë¸ì˜ Perplexity(í˜¼ì¡ë„)ë¥¼ ê³„ì‚° ì¤‘..."):
                try:
                    texts = (df_full["title"].fillna("") + " " + df_full["content"].fillna("")).tolist()
                    X_full = lda_vect.transform(texts)
                    
                    perplexity = lda_model.perplexity(X_full)
                    st.metric("Perplexity (í˜¼ì¡ë„) - ì „ì²´ ë°ì´í„° ê¸°ì¤€", f"{perplexity:,.2f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
                except Exception as e:
                    st.error(f"LDA ì„±ëŠ¥ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")

            st.markdown("---")
            st.markdown("#### C. íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ (RÂ² / MSE)")
            st.error("ë³¸ ì•±ì€ 'Good/Bad'ë¥¼ ë§ì¶”ëŠ” **ë¶„ë¥˜(Classification) ëª¨ë¸**ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, RÂ²(Adjusted R-squared)ë‚˜ MSE ê°™ì€ **íšŒê·€(Regression) ì§€í‘œ**ëŠ” í•´ë‹¹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹  ìœ„ (A)ì˜ **ì •í™•ë„(Accuracy)**ì™€ **F1-Score**ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.")