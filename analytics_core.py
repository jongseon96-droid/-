# analytics_core.py
# ì´ íŒŒì¼ì—ëŠ” Streamlit UIì™€ ê´€ë ¨ëœ ì½”ë“œë¥¼ ì œì™¸í•œ ëª¨ë“  ìƒìˆ˜, í´ë˜ìŠ¤, í•¨ìˆ˜ë§Œ ì •ì˜ë©ë‹ˆë‹¤.

import os, io, json, time, re as regx
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import datetime
from typing import List, Dict, Tuple

# ==== OpenAI SDK ====
from openai import OpenAI
from openai import APIError, RateLimitError

# ==== ML / NLP ====
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.preprocessing import RobustScaler
from sklearn.utils import shuffle as sk_shuffle
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import train_test_split, StratifiedKFold # StratifiedKFold ì¶”ê°€

# ==== íšŒê·€ ====
import statsmodels.api as sm
from sklearn.pipeline import Pipeline

# ================== 1. ìƒìˆ˜ / CONFIG ==================

# [ìˆ˜ì •] í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì •ì˜ (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
STOPWORDS_KO = [
    # ì¡°ì‚¬/ì–´ë¯¸ (ë§¤ìš° ë¹ˆë²ˆ)
    "ì…ë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ê°™ìŠµë‹ˆë‹¤", "ìˆìŠµë‹ˆë‹¤", "ìˆëŠ”", "ê²ƒì…ë‹ˆë‹¤", "í–ˆë‹¤", "ë“±", "ì´", "ê·¸", "ì €",
    "ìˆ˜", "ê²ƒ", "ë°", "A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z",
    "ì—ì„œ", "ìœ¼ë¡œ", "í•˜ëŠ”", "ì„", "ë¥¼", "ì€", "ëŠ”", "ì´", "ê°€", "ì˜", "ì—", "ì™€", "ê³¼", "ë„", "ê³ ", "ë¼ëŠ”",

    # ì¼ë°˜ ëª…ì‚¬ (ì‹ í˜¸ ë°©í•´)
    "ë¸”ë¡œê·¸", "í¬ìŠ¤íŒ…", "ì˜¤ëŠ˜", "ì´ë²ˆ", "ë‹¤ì–‘í•œ", "ê´€ë ¨", "ë‚´ìš©", "ì •ë³´", "ì •ë§", "ë°”ë¡œ", "ì§€ê¸ˆ", "ìƒê°",
    "ê²½ìš°", "ëŒ€í•´", "ëŒ€ë¶€ë¶„", "ë•Œë¬¸", "ê°€ì§€", "í†µí•´", "ìœ„í•´", "ëŒ€í•œ", "í†µí•œ", "ë”°ë¼","gt", "https", "lt", "ê°€ì¥", "ê°™ì€", "ê²ƒìœ¼ë¡œ", "ê²ƒì€", "ê²ƒì´", "ê²ƒì´ë‹¤",
    "ê´‘ê³ ", "ê·¸ë¦¬ê³ ", "ê¸°ì‚¬", "ê¸°ì‚¬ë¥¼", "ë‰´ìŠ¤", "ë‹¤ë¥¸", "ë§ì€", "ì•„ë‹ˆë¼",
    "ì–´ë–¤", "ì–¸ë¡ ", "ì‹ ë¬¸ê³¼ë°©ì†¡", "ì´ëŸ¬í•œ", "ì´ëŸ°", "ì´ë¥¼", "ìˆë‹¤", "ìˆì—ˆë‹¤",
    "ì§€ë‚œ", "ì§€ì—­", "ì½˜í…ì¸ ", "ì½˜í…ì¸ ë¥¼", "í•˜ì§€ë§Œ", "í•œë‹¤","ë§Œë‚˜ë³´ì„¸ìš”", "2025", "ì—†ë‹¤", "ìœ„í•œ", "the", "com", "www", "of", "news", "and", "to", "2022" ,"uk" ,"2020", "in", "1ë©´", "ë†’ì€", "ë˜í•œ", "ë‚˜íƒ€ë‚¬ë‹¤", "ë§ì´",
    "naver", "í•œëˆˆì—", "2020ë…„", "ëŠ˜ì–´ë‚œ", "ëŒ“ê¸€", "íŠ¹íˆ", "ê·¸ë¦¼", "ëŒ€ë¹„", "ë•Œë¬¸ì—", "ì—†ëŠ”", "ê²ƒì„", "ë•Œë¬¸ì´ë‹¤", "ê·¸ëŸ¬ë‚˜", "ìˆë‹¤ëŠ”", "ë¬´ìŠ¨ì¼ì´", "ë¼ê³ ", "í•¨ê»˜", "í•˜ê³ ", "ë“±ì„",
    "ì–´ë–»ê²Œ", "í™œìš©", "ë§í–ˆë‹¤", "ap", "niemanlab", "esg", "ì£¼ëª©ë°›ëŠ”", "ê°•ì¡°í•œ", "ê·¸ëŠ”", "ìˆìœ¼ë©°",
    "blog", "nft", "kpfjra", "ì—ì„œë„", "quibi", "fast", "ì´í›„", "êµ¬ë¶„", "ë¹„í•´", "ë†’ì•˜ë‹¤", "2021","1ì›”", "2ì›”", "3ì›”", "4ì›”", "5ì›”", "6ì›”",
    "7ì›”", "8ì›”", "9ì›”", "10ì›”", "11ì›”", "12ì›”",
    "1990", "1991", "1992", "1993", "1994", "1995", "1996", "1997", "1998", "1999",
    "2000", "2001", "2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009",
    "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019",
    "2020","ë³´ë‹ˆ", "ìˆê³ ", "ë¼ëŠ”" ,"ì•Šì•˜ë‹¤", "ì—¬ëŸ¬", "ëë‹¤", "ìš°ë¦¬ê°€", "ì—†ì—ˆë‹¤", "ì¢‹ì€", "ë‚˜ëŠ”","ê³µì˜ë°©ì†¡ì‚¬ì˜", "ê¸°ì‚¬ëŠ”", "ì‹ ë¬¸ê³¼", "ë°©ì†¡",
    "1990ë…„", "1991ë…„", "1992ë…„", "1993ë…„", "1994ë…„", "1995ë…„", "1996ë…„", "1997ë…„", "1998ë…„", "1999ë…„",
    "2000ë…„", "2001ë…„", "2002ë…„", "2003ë…„", "2004ë…„", "2005ë…„", "2006ë…„", "2007ë…„", "2008ë…„", "2009ë…„",
    "2010ë…„", "2011ë…„", "2012ë…„", "2013ë…„", "2014ë…„", "2015ë…„", "2016ë…„", "2017ë…„", "2018ë…„", "2019ë…„",
    "2020ë…„", "2021ë…„", "2022ë…„", "2023ë…„", "2024ë…„", "2025ë…„",
    # ìŠ¤í¬ë¦°ìƒ·ì—ì„œ ë³´ì¸ ë¬¸ì œ ë‹¨ì–´ë“¤
    "2024", "2023", "ai", "2024ë…„", "2023ë…„", "ã…‹ã…‹", "ã…ã…", "í–ˆìŠµë‹ˆë‹¤", "ìˆì—ˆìŠµë‹ˆë‹¤", "ì”¨ì˜", "ì”¨ëŠ”", "ìœ„ì—", "ê¸°ìëŠ”", "ê¸°ì‚¬ê°€", "ê³¼ì •ì„", "ì•Šì•˜ìŠµë‹ˆë‹¤", "ë°”ëë‹ˆë‹¤", "ë¯¿ì„", "ê²ë‹ˆë‹¤", "ì•Šì•˜ìŠµë‹ˆë‹¤", "ì•Šê³ ", "ë‹¤ì‹œ", "ì§ì ‘", "í•´ë‹¹", "í•´ë‹¹",
    "ê²ë‹ˆë‹¤", "ìš°ë¦¬", "ë¯¿ì„", "ê°ˆë¬´ë¦¬", "ì—†ì—ˆìŠµë‹ˆë‹¤", "í•„ìš”í•œ", "ë‚´ìš©ì„", "ê·¸ëŸ°", "ì €ëŠ”", "ê·¸ë˜ì„œ", "ë‚´ê°€", "ë‹¤ì‹œ", "ê·¸ë ‡ê²Œ", "ì´ë ‡ê²Œ", "ì¼ì„", "ë§ì„", "ìˆì„", "ë³´ë©´", "ë˜ëŠ”",
    "ì›ì˜", "ì „ì²´", "ì¸ê¸°ë¥¼", "kr", "ì•„ë‹Œ", "ë”°ë¼ì„œ", "ì‰½ê²Œ", "ì´ëŠ”", "ëœë‹¤", "ì´ì—", "ì‰½ê²Œ", "ë˜ëŠ”", "ì¬ë°‹ê²Œ", "ì‰½ê³ ", "ë²ˆì§¸", "ë°›ì„", "ì•„ë˜", "ì•ŠëŠ”", "ëìŠµë‹ˆë‹¤", "ì¸í•´", "ë§¤ìš°", "ê´€ë ¨í•´", "í•œë‹¤ëŠ”", "ì—­ì‹œ", "ë“œë¦½ë‹ˆë‹¤", "ë°í˜”ë‹¤", "ì˜ˆë¥¼", "ë“¤ë©´", "í˜¹ì€", "ë“¤ì–´"
]

# [ìˆ˜ì •] Baseline ëª¨ë¸ì— í•„ìš”í•œ Feature ë¦¬ìŠ¤íŠ¸ ì •ì˜
BASELINE_FEATURES = ["img_count", "title_length", "content_length"]

# LLM ê´€ë ¨ ì„¤ì • (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
USE_LLM = len(OPENAI_API_KEY) > 0
client = OpenAI(api_key=OPENAI_API_KEY) if USE_LLM else None
MODEL_CHAT = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o-mini-2024-07-18")

# [ì¶”ê°€] íŒŒì¸íŠœë‹ ì‹œ ì‚¬ìš©í•  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (OpenAI messages í˜•ì‹ì— í•„ìš”)
SYSTEM_PROMPT_FT = "ë‹¹ì‹ ì€ ì œì‹œëœ ì£¼ì œì™€ ë³¸ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ë…ìì˜ ì°¸ì—¬ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” ì„±ê³¼í˜• ì œëª©ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì˜¤ì§ ì œëª© í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤."


# LLM ìƒíƒœ ì²´í¬
LLM_OK = False
if USE_LLM and client:
    try:
        client.models.list()
        LLM_OK = True
    except Exception:
        LLM_OK = False

# Candidate templates (í´ë°±) (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
NUM_RE  = regx.compile(r"\b(\d+|top\s*\d+|[0-9]+ë¶„)\b", regx.I)
TIME_BANK = ["ì˜¤ëŠ˜", "ì´ë²ˆ ì£¼", "ì£¼ë§", "ì§€ê¸ˆ", "ë°©ê¸ˆ", "ì´ë²ˆ ë‹¬", "10ì›”", "11ì›”", "12ì›”"]
HOWTO_BANK = ["ë°©-step", "ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤"]
ACTION_BANK = ["ì •ë¦¬", "ë¹„êµ", "ë¶„ì„", "ì„¤ëª…", "ì¶”ì²œ", "ì ê²€", "ì‹¤ë²•", "ê°€ì´ë“œ", "ì²´í¬ë¦¬ìŠ¤íŠ¸", "íŠœí† ë¦¬ì–¼", "Step-byí—˜"]
CTA_BANK = ["ì§ˆë¬¸", "ëŒ“ê¸€", "êµ¬ë…", "ê³µìœ ", "ì•Œë¦¼", "ì°¸ì—¬"]
LIST_BANK = ["Top 5", "Top 7", "3ê°€ì§€", "5ë¶„ ìš”ì•½", "í•œëˆˆì—"]
BRAND_HINT = ["í•œì–‘ëŒ€", "ì˜¤í”ˆAI", "ì¹´ì¹´ì˜¤", "êµ¬ê¸€", "MS", "ë„¤ì´ë²„"]
DEFAULT_CANDIDATES = TIME_BANK + HOWTO_BANK + ACTION_BANK + CTA_BANK + LIST_BANK + BRAND_HINT

# MODE_CFG (ë¶„ì„ ëª¨ë“œ ì„¤ì •) (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
MODE_CFG = {
    "quick": {
        "sample_n": 5000,
        "lda_topics": 0,
        "batch_size": 500,
        "n_epochs": 2,
        "clf_epochs": 1,
        "clf_batch": 500,
        "ngram_range": (1, 2),
    },
    "full": {
        "sample_n": None,
        "lda_topics": 0,
        "batch_size": 1000,
        "n_epochs": 3,
        "clf_epochs": 3,
        "clf_batch": 1000,
        "ngram_range": (1, 3),
    },
}


# ================== 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ==================

def read_csv_robust(src, **kwargs) -> pd.DataFrame:
    """[ë‹¨ì¼ íŒŒì¼ ë¡œë“œ] UploadedFile/bytes/path/file-like ëª¨ë‘ ì§€ì›. ì¸ì½”ë”©ê³¼ êµ¬ë¶„ì ìë™ ì¬ì‹œë„."""
    encodings = ["utf-8", "utf-8-sig", "cp949", "euc-kr", "latin1"]
    seps = [None, ",", "\t", ";"]

    # bytesë¡œ ì•ˆì „ ë³µì‚¬
    if hasattr(src, "getvalue"):        # Streamlit UploadedFile
        raw = src.getvalue()
    elif isinstance(src, (bytes, bytearray)):
        raw = bytes(src)
    elif isinstance(src, str):          # ê²½ë¡œ
        with open(src, "rb") as f:
            raw = f.read()
    else:                               # file-like
        raw = src.read()
        try:
            src.seek(0)
        except Exception:
            pass

    last_err = None
    for enc in encodings:
        for sep in seps:
            try:
                return pd.read_csv(io.BytesIO(raw), encoding=enc, sep=sep, engine="python", **kwargs)
            except Exception as e:
                last_err = e
                continue
    raise RuntimeError(f"CSV ë””ì½”ë”© ì‹¤íŒ¨: ë§ˆì§€ë§‰ ì˜¤ë¥˜={last_err}")

def _parse_json_safely(txt: str):
    """ì½”ë“œíœìŠ¤/ì•ë’¤ ì“°ë ˆê¸°/í•œê¸€ BOM ì œê±°, ì²« JSON ê°ì²´/ë°°ì—´ë§Œ íŒŒì‹±"""
    if not isinstance(txt, str):
        raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
    t = txt.strip().lstrip("\ufeff")
    if t.startswith("```"):
        parts = t.split("```")
        if len(parts) >= 3:
            cand = parts[1] if parts[1].strip().startswith(("{","[")) else parts[2]
            t = cand
        else:
            t = t.replace("```","").strip()
    si, sj = t.find("["), t.rfind("]")
    oi, oj = t.find("{"), t.rfind("}")
    if 0 <= si < sj:
        return json.loads(t[si:sj+1])
    if 0 <= oi < oj:
        return json.loads(t[oi:oj+1])
    raise ValueError("LLM ì‘ë‹µì—ì„œ JSONì„ ì°¾ì§€ ëª»í•¨")

def categorize_term(t: str) -> str:
    """ë‹¨ì–´ë¥¼ ìœ í˜•ë³„ë¡œ ë¶„ë¥˜"""
    t_low = t.lower()
    if NUM_RE.search(t_low) or any(x in t for x in LIST_BANK): return "ìˆ«ì/ë¦¬ìŠ¤íŠ¸"
    if any(k in t for k in TIME_BANK): return "ì‹œê°„í‘œí˜„"
    if any(k in t for k in HOWTO_BANK): return "How-to/ê°€ì´ë“œ"
    if any(k in t for k in CTA_BANK): return "ì§ˆë¬¸/CTA"
    if any(k in t for k in ACTION_BANK): return "í–‰ë™ë™ì‚¬/í–‰ìœ„"
    if regx.match(r"[A-Z][a-zA-Z0-9]+", t) or "ëŒ€" in t or "ëŒ€í•™" in t or any(b in t for b in BRAND_HINT):
        return "ê³ ìœ ëª…ì‚¬/ë¸Œëœë“œ"
    return "ê¸°íƒ€"

_CTRL = regx.compile(r"[\x00-\x1f\x7f]")

def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ì»¬ëŸ¼ëª… BOM/ê³µë°± ì œê±° + ì†Œë¬¸ì + ìŠ¤í˜ì´ìŠ¤â†’ì–¸ë”ìŠ¤ì½”ì–´"""
    df = df.copy()
    df.columns = (
        pd.Index(df.columns)
        .map(lambda c: str(c).lstrip("\ufeff").strip().lower().replace(" ", "_"))
    )
    return df

def coerce_article_id(df: pd.DataFrame) -> pd.DataFrame:
    """article_id ì»¬ëŸ¼ ì •ê·œí™”"""
    df = _normalize_columns(df.copy())
    aliases = ["article_id", "id", "doc_id", "post_id", "review_id", "news_id", "content_id"]
    found = None
    for a in aliases:
        if a in df.columns:
            found = a
            break
    if found is None:
        raise KeyError(f"CSVì— article_id ê³„ì—´ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (ì»¬ëŸ¼={list(df.columns)[:12]})")
    if found != "article_id":
        df = df.rename(columns={found: "article_id"})

    df["article_id"] = (
        df["article_id"]
        .astype(str)
        .str.replace("\ufeff", "", regex=False)
        .apply(lambda x: _CTRL.sub("", x))
        .str.strip()
    )
    return df

def prepare_by_mode(df_in: pd.DataFrame, mode_cfg: dict, lda_topics_ui: int):
    """ë¶„ì„ ëª¨ë“œì— ë”°ë¥¸ ë°ì´í„° ìƒ˜í”Œë§ ë° ì„¤ì • ë°˜í™˜ (LDA ë¶€ë¶„ë§Œ ì²˜ë¦¬)"""
    if mode_cfg["sample_n"]:
        n = min(mode_cfg["sample_n"], len(df_in))
        df_work = df_in.sample(n=n, random_state=42).reset_index(drop=True)
    else:
        df_work = df_in.copy()

    n_topics = mode_cfg["lda_topics"] if mode_cfg["lda_topics"] > 0 else int(lda_topics_ui)

    lda_kwargs = dict(
        n_topics=n_topics,
        max_features=mode_cfg.get("max_features"),
        batch_size=mode_cfg["batch_size"],
        n_epochs=mode_cfg["n_epochs"],
    )
    clf_kwargs = dict(
        epochs=mode_cfg["clf_epochs"],
        batch_size=mode_cfg["clf_batch"],
        ngram_range=mode_cfg["ngram_range"],
    )
    return df_work, lda_kwargs, clf_kwargs


# ================== 3. ë¨¸ì‹ ëŸ¬ë‹/í†µê³„ í•¨ìˆ˜ ==================

def build_engagement(df: pd.DataFrame, w_views=0.4, w_likes=0.4, w_comments=0.2) -> pd.DataFrame:
    """
    [ìˆ˜ì •] RobustScaler(ì¤‘ì•™ê°’/IQR) ê¸°ë°˜ ì •ê·œí™”ëœ ì½˜í…ì¸  ë§¤ë ¥ ì ìˆ˜ ê³„ì‚° ë° Baseline í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§.
    Baseline í”¼ì²˜(title_length, content_length, img_count)ì˜ ìŠ¤ì¼€ì¼ë§ì€ ì œê±°í–ˆìŠµë‹ˆë‹¤. (ëˆ„ìˆ˜ ë°©ì§€)
    """
    df = df.copy()
    metric_cols = ["views_total", "likes", "comments"]
    
    # 1. ì„±ê³¼ ì§€í‘œ ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ê·œí™”
    for c in metric_cols:
        if c not in df.columns:
            df[c] = 0
        df[c] = df[c].fillna(0)

    scaler_eng = RobustScaler(quantile_range=(25.0, 75.0))
    for c in metric_cols:
        df[c + "_rob"] = scaler_eng.fit_transform(df[[c]]).ravel()

    df["engagement"] = (
        w_views   * df["views_total_rob"] +
        w_likes   * df["likes_rob"] +
        w_comments* df["comments_rob"]
    )

    # 2. Baseline í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (ìŠ¤ì¼€ì¼ë§ ì œì™¸)
    df["title_length"] = df["title"].fillna("").astype(str).str.len()
    df["content_length"] = df["content"].fillna("").astype(str).str.len()
    
    if "img_count" not in df.columns:
          df["img_count"] = 0 # CSVì— ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì²˜ë¦¬

    # BASELINE_FEATURESì˜ Robust Scalingì€ ì´ì œ train_quality_classifier ë‚´ë¶€ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
    
    return df

def label_quality_by_quantile(df: pd.DataFrame, col="engagement", low_q=0.33, high_q=0.66) -> pd.DataFrame:
    """ë¶„ìœ„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì½˜í…ì¸  í’ˆì§ˆ ë¼ë²¨ë§ (good, bad, medium)"""
    df = df.copy()
    q_low, q_high = df[col].quantile([low_q, high_q])
    def _label(x):
        if x >= q_high: return "good"
        if x <= q_low: return "bad"
        return "medium"
    df["quality_label"] = df[col].apply(_label)
    return df

def train_quality_classifier(df_train: pd.DataFrame,
                             mode: str,
                             clf_kwargs: dict,
                             lda_vect: CountVectorizer = None,
                             model_type: str = "SGDClassifier") -> Dict: 
    """
    [ìˆ˜ì •] Baseline ë˜ëŠ” Advanced ëª¨ë“œë¡œ SGD/RandomForest ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµ. í›ˆë ¨ ë°ì´í„°ì—ë§Œ ìŠ¤ì¼€ì¼ë§ì„ ì ìš©í•©ë‹ˆë‹¤.
    (SGD ClassifierëŠ” partial_fit ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ class_weight='balanced'ë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤.)
    """
    if df_train.empty:
        raise ValueError("í•™ìŠµ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    df_train = df_train[df_train["quality_label"] != "medium"].copy()
    if len(np.unique(df_train["quality_label"])) < 2:
        raise ValueError("ë‹¨ì¼ ë¼ë²¨ë§Œ ì¡´ì¬í•˜ì—¬ ë¶„ë¥˜ê¸° í•™ìŠµì„ ê±´ë„ˆë›°ì›ë‹ˆë‹¤.")

    y = np.array([1 if l=="good" else 0 for l in df_train["quality_label"]])
    feature_names = []
    
    scaler = RobustScaler() 

    if mode == "baseline":
        feature_cols = BASELINE_FEATURES
        X_num_scaled = scaler.fit_transform(df_train[feature_cols].values)
        X = X_num_scaled
        tfidf = None
        feature_names = feature_cols
        
    elif mode == "advanced":
        if lda_vect is None or 'topic' not in df_train.columns:
            raise ValueError("Advanced ëª¨ë“œëŠ” LDA ëª¨ë¸ í•™ìŠµ ë° í† í”½ í• ë‹¹ì´ ì„ í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

        # 1. Tfidf (í…ìŠ¤íŠ¸)
        texts = (df_train["content"].fillna("")).tolist()
        #texts = (df_train["title"].fillna("") + " " + df_train["content"].fillna("")).tolist()
        tfidf = TfidfVectorizer(
            ngram_range=clf_kwargs.get("ngram_range", (1, 2)),
            min_df=5, max_df=0.80, stop_words=STOPWORDS_KO
        )
        X_text = tfidf.fit_transform(texts)

        # 2. ìˆ˜ì¹˜ í”¼ì²˜ (í›ˆë ¨ ë°ì´í„°ì—ë§Œ ìŠ¤ì¼€ì¼ë§)
        X_num = df_train[BASELINE_FEATURES].values
        X_num_scaled = scaler.fit_transform(X_num) 
        
        # 3. í† í”½ í”¼ì²˜
        X_topic_df = pd.get_dummies(df_train['topic'], prefix='topic')
        X_topic = X_topic_df.values
        
        # ëª¨ë“  í”¼ì²˜ë¥¼ í†µí•©
        X = np.hstack([X_text.toarray(), X_num_scaled, X_topic])
        
        feature_names = list(tfidf.get_feature_names_out()) + BASELINE_FEATURES + list(X_topic_df.columns)

    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë¶„ë¥˜ ëª¨ë“œ: {mode}")

    # [ìˆ˜ì •] ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ ë¡œì§ (Random Forest ë„ì…)
    if model_type == "RandomForest":
        # Random Forest: fit ì‚¬ìš©
        clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
        clf.fit(X, y)
    elif model_type == "SGDClassifier": # Default for Baseline Mode
        # SGDClassifier: partial_fit ì‚¬ìš©
        clf = SGDClassifier(
            loss="log_loss", learning_rate="optimal", alpha=1e-5, random_state=42, 
            warm_start=True
        )
        # ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ
        batch_size = clf_kwargs.get("batch_size", 2000)
        epochs = clf_kwargs.get("epochs", 3)
        classes = np.array([0,1])
        n = X.shape[0]
        idx_all = np.arange(n)
        n_batches = int(np.ceil(n / batch_size))
        
        for ep in range(epochs):
            idx_all = sk_shuffle(idx_all, random_state=42 + ep)
            for b in range(n_batches):
                bs = idx_all[b*batch_size : (b+1)*batch_size]
                Xb = X[bs]; yb = y[bs]
                clf.partial_fit(Xb, yb, classes=classes)
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")

    # í‚¤ì›Œë“œ/í”¼ì²˜ ì¤‘ìš”ë„ ì¶”ì¶œ (Tfidf í”¼ì²˜ë§Œ)
    good_terms, bad_terms = [], []
    if mode == "advanced" and tfidf is not None and model_type == "SGDClassifier":
        tfidf_feature_count = len(tfidf.get_feature_names_out())
        coefs = clf.coef_[0][:tfidf_feature_count] 
        vocab = np.array(tfidf.get_feature_names_out())
        order = np.argsort(coefs)
        k_show = 20
        good_terms = [(vocab[i], float(coefs[i])) for i in order[::-1] if coefs[i] > 0][:k_show]
        bad_terms  = [(vocab[i], float(coefs[i]))  for i in order if coefs[i] < 0][:k_show]
    
    return {
        "clf": clf,
        "tfidf": tfidf, 
        "scaler": scaler, 
        "features": feature_names,
        "mode": mode,
        "good_terms": good_terms,
        "bad_terms": bad_terms,
        "model_type": model_type
    }

def evaluate_comparison_models(df_full: pd.DataFrame,
                               lda_vect: CountVectorizer,
                               models: List[str] = ["SGDClassifier", "LogisticRegression", "RandomForestClassifier"]):
    """
    [ìˆ˜ì •] Advanced ëª¨ë“œ í”¼ì²˜ì…‹ì„ ì‚¬ìš©í•˜ì—¬ StratifiedKFold êµì°¨ ê²€ì¦ìœ¼ë¡œ 3ê°€ì§€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.
    (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ ê° Fold ë‚´ë¶€ì—ì„œ ìˆ˜ì¹˜ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ìˆ˜í–‰)
    """
    df_trainable = df_full[df_full['quality_label'] != 'medium'].copy().reset_index(drop=True)
    if df_trainable.empty or 'topic' not in df_trainable.columns:
        return {"error": "í‰ê°€ ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆê±°ë‚˜ í† í”½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."}

    # 1. ì›ë³¸ ë°ì´í„° ì¤€ë¹„ (ìŠ¤ì¼€ì¼ë§ ì „)
    texts = (df_trainable["title"].fillna("") + " " + df_trainable["content"].fillna("")).tolist()
    y = df_trainable["quality_label"].values
    X_num_raw = df_trainable[BASELINE_FEATURES].values
    X_topic = pd.get_dummies(df_trainable['topic'], prefix='topic').values
    
    # Tfidf (í…ìŠ¤íŠ¸) - ì „ì²´ í›ˆë ¨ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ ì¶”ì¶œ (K-Fold ë°–ì—ì„œ fit)
    tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.8, stop_words=STOPWORDS_KO)
    X_text_full = tfidf.fit_transform(texts).toarray()
    
    # ì „ì²´ í”¼ì²˜ (Tfidf, ìˆ˜ì¹˜, í† í”½)
    X_full = np.hstack([X_text_full, X_num_raw, X_topic])

    # ìˆ˜ì¹˜ í”¼ì²˜ê°€ ì‹œì‘ë˜ëŠ” ì¸ë±ìŠ¤ ê³„ì‚°
    num_start_idx = X_text_full.shape[1] 
    num_end_idx = num_start_idx + X_num_raw.shape[1]

    # 2. Stratified K-Fold ì„¤ì • (5-Fold ì‚¬ìš©)
    N_SPLITS = 5
    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)
    
    model_metrics = {m: {"Accuracy": [], "F1_Good": [], "CM_Total": np.zeros((2, 2))} for m in models}
    
    # 3. K-Fold ë°˜ë³µ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ë¡œì§ ì‹¤í–‰)
    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X_full, y)):
        
        X_train_fold, X_test_fold = X_full[train_idx].copy(), X_full[test_idx].copy()
        y_train_fold, y_test_fold = y[train_idx], y[test_idx]
        
        # â˜…â˜…â˜… Fold ë‚´ë¶€ì—ì„œ ìˆ˜ì¹˜ í”¼ì²˜ë§Œ ìŠ¤ì¼€ì¼ë§ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€) â˜…â˜…â˜…
        scaler = RobustScaler()
        scaler.fit(X_train_fold[:, num_start_idx:num_end_idx])
        
        X_train_fold[:, num_start_idx:num_end_idx] = scaler.transform(X_train_fold[:, num_start_idx:num_end_idx])
        X_test_fold[:, num_start_idx:num_end_idx] = scaler.transform(X_test_fold[:, num_start_idx:num_end_idx])
        
        # 4. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        for model_name in models:
            try:
                if model_name == "SGDClassifier":
                    clf = SGDClassifier(loss="log_loss", alpha=1e-5, random_state=42 + fold_idx, class_weight='balanced')
                elif model_name == "LogisticRegression":
                    clf = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42 + fold_idx, class_weight='balanced')
                elif model_name == "RandomForestClassifier":
                    clf = RandomForestClassifier(n_estimators=100, random_state=42 + fold_idx, class_weight='balanced')
                else:
                    continue
                    
                clf.fit(X_train_fold, y_train_fold)
                y_pred = clf.predict(X_test_fold)
                
                acc = accuracy_score(y_test_fold, y_pred)
                report = classification_report(y_test_fold, y_pred, output_dict=True, zero_division=0)
                f1_good = report['good']['f1-score']
                cm_fold = confusion_matrix(y_test_fold, y_pred, labels=['good', 'bad'])
                
                model_metrics[model_name]["Accuracy"].append(acc)
                model_metrics[model_name]["F1_Good"].append(f1_good)
                model_metrics[model_name]["CM_Total"] += cm_fold
                
            except Exception as e:
                pass 
                
    # 5. ìµœì¢… ê²°ê³¼ ì •ë¦¬ (í‰ê·  ë° ì „ì²´ Confusion Matrix)
    final_results = {}
    for model_name, metrics in model_metrics.items():
        if metrics["Accuracy"]:
            final_results[model_name] = {
                "Accuracy_Mean": np.mean(metrics["Accuracy"]),
                "F1_Good_Mean": np.mean(metrics["F1_Good"]),
                "Report_DF": pd.DataFrame({
                    "Fold_Accuracy_Mean": np.mean(metrics["Accuracy"]).round(3),
                    "Fold_F1_Good_Mean": np.mean(metrics["F1_Good"]).round(3),
                    "N_Folds": N_SPLITS
                }, index=[model_name]).T,
                "CM_Total": metrics["CM_Total"],
                "Detail": f"{N_SPLITS} Fold êµì°¨ ê²€ì¦ ê²°ê³¼"
            }
        else:
            final_results[model_name] = {"error": "êµì°¨ ê²€ì¦ ì¤‘ í•™ìŠµëœ Foldê°€ ì—†ìŠµë‹ˆë‹¤."}
            
    return final_results

# build_topic_term_bank_logreg í•¨ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
def build_topic_term_bank_logreg(df_all: pd.DataFrame,
                                 topn: int = 50,
                                 min_samples_warn: int = 50,
                                 min_samples_block: int = 10) -> dict:
    """Logistic Regression ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í”½ë³„ ë‹¨ì–´ ì€í–‰ êµ¬ì¶•"""
    bank = {}

    if 'topic' not in df_all.columns:
        return bank

    valid_topics = df_all["topic"].dropna().unique()
    unique_topics = sorted([t for t in valid_topics if pd.notna(t)])

    for t in unique_topics:
        try:
            topic_int = int(t)
        except ValueError:
            continue

        df_topic = df_all[df_all["topic"] == t]
        df_train = df_topic[df_topic["quality_label"] != "medium"]

        if len(df_train) < min_samples_block:
            bank[topic_int] = {
                "status": "error",
                "message": f"ìƒ˜í”Œ ì™„ì „ ë¶€ì¡± (N={len(df_train)}, ìµœì†Œ {min_samples_block} í•„ìš”)"
            }
            continue

        warning_msg = None
        if len(df_train) < min_samples_warn:
            warning_msg = f"ìƒ˜í”Œ ìˆ˜(N={len(df_train)})ê°€ ê¶Œì¥({min_samples_warn})ë³´ë‹¤ ì ì–´ í†µê³„ì  ì‹ ë¢°ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        texts = (df_train["title"].fillna("") + " " + df_train["content"].fillna("")).tolist()
        y = (df_train["quality_label"] == "good").astype(int).values

        try:
            if len(np.unique(y)) < 2:
                bank[topic_int] = {"status": "error", "message": f"ë‹¨ì¼ ë¼ë²¨ë§Œ ì¡´ì¬ (N={len(df_train)})"}
                continue

            tfidf = TfidfVectorizer(ngram_range=(1,1), max_features=5000, min_df=3, stop_words=STOPWORDS_KO)
            X = tfidf.fit_transform(texts)

            clf = LogisticRegression(max_iter=1000, solver="liblinear", random_state=42, class_weight='balanced')
            clf.fit(X, y)

            if not hasattr(clf, "coef_"):
                bank[topic_int] = {"status": "error", "message": "ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨ (ê³„ìˆ˜ ì—†ìŒ)"}
                continue

            coefs = clf.coef_[0]
            vocab = np.array(tfidf.get_feature_names_out())
            order = np.argsort(coefs)

            good_terms = [(vocab[i], float(coefs[i])) for i in order[::-1] if coefs[i] > 0][:topn]
            bad_terms = [(vocab[i], float(coefs[i])) for i in order if coefs[i] < 0][:topn]

            cv_all = CountVectorizer(max_features=topn, min_df=3, stop_words=STOPWORDS_KO)
            X_all = cv_all.fit_transform(texts)
            counts = np.asarray(X_all.sum(axis=0)).ravel()
            vocab_all = np.array(cv_all.get_feature_names_out())
            order_all = np.argsort(counts)[::-1]
            all_terms = [(vocab_all[i], float(counts[i])) for i in order_all]

            bank[topic_int] = {
                "good": good_terms,
                "bad": bad_terms,
                "all": all_terms,
                "status": "ok",
                "message": f"ì„±ê³µ (N={len(df_train)})",
                "warning": warning_msg
            }
        except Exception as e:
            bank[topic_int] = {
                "status": "error",
                "message": f"ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}"
            }
    return bank

# infer_topic_for_text (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def infer_topic_for_text(txt: str,
                         vect: CountVectorizer,
                         lda_model: LatentDirichletAllocation) -> Tuple[int, np.ndarray]:
    """í…ìŠ¤íŠ¸ì— ëŒ€í•œ í† í”½ ì¶”ë¡ """
    Xd = vect.transform([txt if isinstance(txt, str) else ""])
    dist = lda_model.transform(Xd)[0]
    return int(dist.argmax()), dist

# get_topic_keywords_from_bank (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def get_topic_keywords_from_bank(bank: dict, topic_id: int, k_each: int = 30) -> Dict[str, List[Tuple[str, float]]]:
    """ì£¼ì œ IDì— í•´ë‹¹í•˜ëŠ” 'good'/'all' í‚¤ì›Œë“œë¥¼ (ë‹¨ì–´, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    if topic_id not in bank or bank[topic_id].get("status") != "ok":
        return {"good": [], "all": []}

    goods = [(w, s) for w,s in bank[topic_id].get("good", [])[:k_each]]
    alls = [(w, s) for w,s in bank[topic_id].get("all", [])[:max(1, k_each//2)]]

    seen = set()
    unique_goods = []
    for w,s in goods:
        if w not in seen:
            unique_goods.append((w,s)); seen.add(w)

    unique_alls = []
    for w,s in alls:
        if w not in seen:
            unique_alls.append((w,s)); seen.add(w)

    return {"good": unique_goods, "all": unique_alls}


# llm_rerank_or_generate (ìˆ˜ì •ëœ ìµœì¢… ë²„ì „)
def llm_rerank_or_generate(
    draft_title: str,
    draft_body: str,
    candidates: List[str],
    topic_name: str,
    topk: int = 8,  # ë¦¬ë­ì»¤ ëª¨ë“œì—ì„œ ì‚¬ìš©, íŒŒì¸íŠœë‹ ëª¨ë“œì—ì„œ ë¬´ì‹œ
    audience: str = "í˜¼í•©",
    tone: str = "ë¶„ì„ì ",
    temperature: float = 0.5,
    use_finetuned: bool = False,
    ft_model_id: str = MODEL_CHAT
) -> List[Dict]:
    """LLMì„ ì´ìš©í•´ ì œëª© í›„ë³´ë¥¼ ìƒì„±í•˜ê±°ë‚˜ (íŒŒì¸íŠœë‹), í†µê³„ ê¸°ë°˜ ë‹¨ì–´ë¥¼ ë¦¬ë­í¬ (ê¸°ë³¸)"""
    if not USE_LLM or client is None or not LLM_OK:
        raise RuntimeError("APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: OPENAI_API_KEY/ë„¤íŠ¸ì›Œí¬/ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")

    if use_finetuned and ft_model_id.startswith("ft:"):
        # ===== 1. íŒŒì¸íŠœë‹ ëª¨ë¸ (ì œëª© ìƒì„±) ë¡œì§ (ìš”ì²­ ì‚¬í•­ ë°˜ì˜) =====
        topic_name_current = topic_name if topic_name != "ë¯¸ë¶„ë¥˜" else "ì¼ë°˜"
        
        # [ìˆ˜ì • 1, 2, 4] í”„ë¡¬í”„íŠ¸ ê°•í™”: ìµœëŒ€ ê°œìˆ˜ ìš”ì²­ ë° Why/Effect êµ¬ì¡°í™”ëœ JSON ë°˜í™˜ ìš”ì²­
        system_prompt = f"""ë‹¹ì‹ ì€ ì œì‹œëœ ì£¼ì œì™€ ë³¸ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ë…ìì˜ ì°¸ì—¬ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” ì„±ê³¼í˜• ì œëª©ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤.
ìƒì„±í•  ì œëª©ì€ ìµœëŒ€ 20ê°œ ë‚´ì™¸ë¡œ í•©ë‹ˆë‹¤.
ê° ì œëª©ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ì„ ê°€ì§„ JSON ë°°ì—´ì„ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ëª©ë¡ ë²ˆí˜¸ë‚˜ ì½”ë“œ íœìŠ¤ëŠ” ì ˆëŒ€ ë„£ì§€ ë§ˆì„¸ìš”.
[
    {{
        "term": "ìƒì„±ëœ ì œëª© í…ìŠ¤íŠ¸",
        "why": "ì´ ì œëª©ì´ ë…ìë¥¼ ëŒì–´ë‹¹ê¸°ëŠ” êµ¬ì²´ì ì¸ ì‹¬ë¦¬ì /ê¸°ìˆ ì  ì´ìœ  (20ì ë‚´ì™¸, ì°½ì˜ì )",
        "expected_effect": "ì´ ì œëª©ì„ ì‚¬ìš©í–ˆì„ ë•Œ ì˜ˆìƒë˜ëŠ” ì°½ì˜ì ì´ê³  êµ¬ì²´ì ì¸ ì„±ê³¼ íš¨ê³¼ (20ì ë‚´ì™¸, ì°½ì˜ì )"
    }},
    ... (ìµœëŒ€ 20ê°œ ë‚´ì™¸)
]
"""
        user_prompt = f"""ì£¼ì œ: {topic_name_current}
ë³¸ë¬¸ ì´ˆì•ˆ: {draft_body}
---
ìœ„ ë³¸ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì œì•½ ì¡°ê±´ì— ë§ëŠ” ì œëª©ì„ ê°€ëŠ¥í•œ í•œ ë§ì´(ìµœëŒ€ 20ê°œ) ìƒì„±í•´ì£¼ì„¸ìš”.
- ë…ììˆ˜ì¤€: {audience}
- í†¤/ìŠ¤íƒ€ì¼: {tone}
- í˜•ì‹: ë°˜ë“œì‹œ ìš”ì²­ëœ JSON ë°°ì—´ í˜•ì‹ë§Œ ë”°ë¥´ì„¸ìš”.
"""

        resp = client.chat.completions.create(
            model=ft_model_id,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            n=1,
            temperature=temperature,
            response_format={"type": "json_object"}
        )

        raw = (resp.choices[0].message.content or "").strip()
        
        try:
            # ì•ˆì „í•œ JSON íŒŒì‹± í•¨ìˆ˜ ì‚¬ìš©
            data_obj = _parse_json_safely(raw)
            titles_list = data_obj if isinstance(data_obj, list) else (data_obj.get("items") if isinstance(data_obj, dict) else [])
        except Exception as e:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ, í…ìŠ¤íŠ¸ ì‘ë‹µì„ ì¤„ ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ í´ë°± ì²˜ë¦¬
            titles_list = [{"term": line.strip()} for line in raw.split('\n') if line.strip()]
        if titles_list is None:
            titles_list = []
        recs = []
        seen_terms = set()
        for i, item in enumerate(titles_list):
            title_text = item.get('term', '').strip()
            if not title_text or title_text in seen_terms:
                continue
            why_text = item.get('why', "íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ë¶„ì„í•œ ê²°ê³¼, ì´ ì œëª©ì€ ë†’ì€ ì„±ê³¼ë¥¼ ë‚¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.") 
            effect_text = item.get('expected_effect', "ë…ìì˜ í˜¸ê¸°ì‹¬ì„ ìê·¹í•˜ì—¬ í´ë¦­ë¥ ì„ íšê¸°ì ìœ¼ë¡œ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            if title_text:
                recs.append({
                    "term": title_text,
                    "category": f"AI ìƒì„± ì œëª© {i+1}",
                    "why": why_text,
                    "where_to_add": "ì œëª©", 
                    "insertion_example": "", # [ìˆ˜ì • 3] ì ìš© ì˜ˆì‹œ ì œê±°
                    "expected_effect": effect_text, 
                    "cautions": "ì›ë³¸ ëª¨ë¸ì˜ ì°½ì˜ì„±ì´ ë°˜ì˜ë˜ì–´ ë¬¸ë§¥ì„ ì¬ê²€í† í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                })
                seen_terms.add(title_text) # ì¤‘ë³µ ë°©ì§€ ë¡œì§ ì¶”ê°€
        return recs # <-- IF ë¸”ë¡ ì¢…ë£Œ

    else: # <-- ELSE ë¸”ë¡ ì‹œì‘ (ë¦¬ë­ì»¤ ëª¨ë“œ)
        # ===== 2. ê¸°ë³¸ LLM (ë¦¬ë­ì»¤) ë¡œì§ (ê¸°ì¡´ ë¡œì§ ìœ ì§€, ì¶œë ¥ í˜•ì‹ë§Œ ìˆ˜ì •) =====
        cand = [c.strip() for c in candidates if str(c).strip()]
        cand_unique = list(dict.fromkeys(cand))[:500]
        if not cand_unique:
            raise RuntimeError("í›„ë³´ ë‹¨ì–´ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (í†µê³„ ê¸°ë°˜ ì¶”ì²œ ë‹¨ì–´ ì—†ìŒ)")

        # [ìˆ˜ì •] ë¦¬ë­ì»¤ ëª¨ë“œìš© í”„ë¡¬í”„íŠ¸ë„ why/effectë¥¼ ì°½ì˜ì ìœ¼ë¡œ ìš”ì²­í•˜ë„ë¡ ìˆ˜ì •
        sys_prompt = (
            "ë„ˆëŠ” í•œêµ­ì–´ ì½˜í…ì¸  í¸ì§‘ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. ë°˜ë“œì‹œ JSON ê°ì²´ë§Œ ì¶œë ¥í•œë‹¤. "
            "ì´ˆì•ˆì€ {'title': '...', 'body': '...'} JSON ê°ì²´ë¡œ ì œê³µëœë‹¤. 'title'ê³¼ 'body'ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ë¶„ì„í•´ì•¼ í•œë‹¤. "
            "ê°ì²´ëŠ” {'items': [...]} í˜•ì‹ì´ë©°, ê° í•­ëª©ì€ "
            "{term, why, where_to_add, expected_effect, cautions} í‚¤ë¥¼ ê°€ì§„ë‹¤. " # insertion_example ì œê±°
            "where_to_addëŠ” ë°˜ë“œì‹œ ['ì œëª©'] í•˜ë‚˜ì—¬ì•¼ í•œë‹¤. ('ì†Œì œëª©', 'ì²« 120ì' ë“± ë‹¤ë¥¸ ê°’ì€ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€) "
            "ë°˜ë“œì‹œ 'í›„ë³´ í’€'ì— ìˆëŠ” ë‹¨ì–´ë§Œ ì‚¬ìš©."
            "whyì™€ expected_effectëŠ” ì°½ì˜ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•œë‹¤." 
        )
        user_payload = {
            "goal": f"ì´ˆì•ˆ ë¬¸ë§¥ì„ ë³´ì¡´í•˜ë©° í›„ë³´ í’€ì—ì„œë§Œ Top-{topk} ì„ ë³„", # topkëŠ” ë¦¬ë­ì»¤ ëª¨ë“œì—ì„œ ì‚¬ìš©
            "constraints": [
                "í›„ë³´ ë°– ë‹¨ì–´/ë™ì˜ì–´ ê¸ˆì§€",
                "ë¬¸ë§¥ ì–´ê¸‹ë‚˜ëŠ” ì‚½ì… ì˜ˆì‹œ ê¸ˆì§€",
                "ì¤‘ë³µ ì˜ë¯¸ ì¶”ì²œ ìµœì†Œí™”",
                "where_to_addëŠ” 'ì œëª©'ë§Œ í—ˆìš©.",
                f"ë…ììˆ˜ì¤€={audience}",
                f"í†¤={tone}"
            ],
            "candidates": cand_unique,
            "draft": {
                "title": draft_title,
                "body": draft_body[:6000]
            },
            # ë¦¬ë­ì»¤ ëª¨ë“œì—ì„œëŠ” insertion_exampleì„ ìƒì„±í•˜ì§€ ì•Šë„ë¡ ìš”ì²­
            "return_format": [
                {"term":"...", "why":"ì°½ì˜ì ì¸ ì¶”ì²œ ì´ìœ ", "where_to_add":"ì œëª©",
                 "expected_effect":"ì°½ì˜ì ì¸ ì˜ˆìƒ íš¨ê³¼", "cautions":"..."}
            ]
        }
        
        model_name = "gpt-4o-mini-2024-07-18"
        resp = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role":"system","content": sys_prompt},
                {"role":"user","content": json.dumps(user_payload, ensure_ascii=False)}
            ],
            temperature=temperature, 
            response_format={"type": "json_object"},
        )

        raw = (resp.choices[0].message.content or "").strip()
        data_obj = _parse_json_safely(raw)
        data = data_obj.get("items") if isinstance(data_obj, dict) else data_obj
        if not isinstance(data, list):
            raise ValueError("JSON í˜•ì‹ ì˜¤ë¥˜: ë°°ì—´(items)ì´ ì•„ë‹˜")

        allowed = set(cand_unique)
        recs = []
        for item in data:
            term = str(item.get("term","")).strip()
            where = str(item.get("where_to_add","")).strip()
            # [ìˆ˜ì •] where_to_add ê²€ì¦ì„ 'ì œëª©'ë§Œ í—ˆìš©í•˜ë„ë¡ ë³€ê²½
            if not term or term not in allowed or where != 'ì œëª©': 
                continue
            recs.append({
                "term": term,
                "category": categorize_term(term),
                "why": str(item.get("why","")).strip(),
                "where_to_add": where,
                "insertion_example": "", # [ìˆ˜ì • 3] ì ìš© ì˜ˆì‹œ ì œê±° ìš”ì²­ ë°˜ì˜ (ë¦¬ë­ì»¤ ëª¨ë“œì—ì„œë„)
                "expected_effect": str(item.get("expected_effect","")).strip(),
                "cautions": str(item.get("cautions","")).strip(),
            })
            if len(recs) >= topk:
                break
        return recs # <-- ELSE ë¸”ë¡ ì¢…ë£Œ

# run_lda_topics_streaming (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def run_lda_topics_streaming(
    texts: List[str],
    n_topics: int = 10,
    max_features: int = 5000, 
    batch_size: int = 1000,
    n_epochs: int = 3,
    progress_callback=None 
):
    """ì˜¨ë¼ì¸ í•™ìŠµ ê¸°ë°˜ LDA ì£¼ì œ ë¶„ì„"""
    vect = CountVectorizer(
        min_df=0.01,
        max_df=0.90,
        stop_words=STOPWORDS_KO
    )
    X = vect.fit_transform([t if isinstance(t, str) else "" for t in texts])

    lda = LatentDirichletAllocation(
        n_components=n_topics, learning_method="online",
        batch_size=batch_size, max_iter=1, random_state=42, evaluate_every=0,
    )

    n_samples = X.shape[0]
    n_batches = int(np.ceil(n_samples / batch_size))
    total_steps = n_epochs * n_batches

    t0 = time.time(); step = 0
    prog = progress_callback(0.0, text="LDA ì£¼ì œ ë¶„ì„ í•™ìŠµ ì¤‘â€¦") if progress_callback else None

    idx_all = np.arange(n_samples)
    for epoch in range(n_epochs):
        idx_all = sk_shuffle(idx_all, random_state=42 + epoch)
        for b in range(n_batches):
            bs = idx_all[b * batch_size : (b + 1) * batch_size]
            Xb = X[bs]
            lda.partial_fit(Xb)

            step += 1
            if prog:
                frac = step / total_steps
                elapsed = time.time() - t0
                sec_per_step = elapsed / max(step, 1)
                remain = sec_per_step * (total_steps - step)
                prog.progress(
                    frac, text=f"LDA í•™ìŠµ {frac*100:.1f}% | ê²½ê³¼ {elapsed:,.0f}s | ë‚¨ìŒ ~{remain:,.0f}s"
                )

    W = lda.transform(X)
    if prog: prog.empty()
    df_topic = pd.DataFrame({"topic": W.argmax(axis=1)})
    return df_topic, vect, lda, W

# train_logreg_with_progress_wrapper (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def train_logreg_with_progress_wrapper(texts, labels, stoplist=None, ngram_range=(1,2),
                               epochs=3, batch_size=2000, k_show=20, seed=42):
    # train_logreg_with_progress í•¨ìˆ˜ê°€ train_quality_classifierë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤.
    raise NotImplementedError("train_logreg_with_progress_wrapperëŠ” train_quality_classifierë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤.")

# get_topic_top_words (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def get_topic_top_words(lda, vect, topn=8):
    """LDA ê²°ê³¼ì—ì„œ í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ"""
    vocab = np.array(vect.get_feature_names_out())
    topics = {}
    for k, comp in enumerate(lda.components_):
        idx = np.argsort(comp)[::-1][:topn]
        topics[f"Topic {k}"] = [str(vocab[i]) for i in idx]
    return topics

def _heuristic_topic_name(words: list[str]) -> dict:
    """íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ìœ¼ë¡œ í† í”½ ì´ë¦„ ì¶”ë¡ """
    w = " ".join(words)
    rules = [
        (["ì •ë¶€","êµ­íšŒ","ì˜ˆì‚°","ì •ì±…","ëŒ€í†µë ¹"], ("ì •ì¹˜/í–‰ì •", "ì •ë¶€Â·êµ­íšŒÂ·ì˜ˆì‚° ê´€ë ¨ ì´ìŠˆ")),
        (["ì†í¥ë¯¼","ë¦¬ê·¸","ê²½ê¸°","ê³¨","ì„ ìˆ˜","ìŠ¤í¬ì¸ "], ("ìŠ¤í¬ì¸ /ì¶•êµ¬", "ê²½ê¸°/ì„ ìˆ˜/ë¦¬ê·¸ ì¤‘ì‹¬ ê¸°ì‚¬")),
        (["AI","ì¸ê³µì§€ëŠ¥","ë¡œë´‡","ê¸°ìˆ ","ì‚°ì—…","ìë™í™”","ë°ì´í„°"], ("ê¸°ìˆ /AI", "AIÂ·ë¡œë´‡Â·ì‚°ì—… ìë™í™”")),
        (["ì£¼ì‹","í™˜ìœ¨","ë¶€ë™ì‚°","ê¸ˆë¦¬","ê²½ì œ"], ("ê²½ì œ/ê¸ˆìœµ", "ê±°ì‹œê²½ì œÂ·ì‹œì¥ ë™í–¥")),
        (["ì½”ë¡œë‚˜","ì˜ë£Œ","ê±´ê°•","ë³‘ì›"], ("ì˜ë£Œ/ê±´ê°•", "ì§ˆë³‘Â·ì˜ë£ŒÂ·í—¬ìŠ¤ì¼€ì–´")),
        (["ë„·í”Œë¦­ìŠ¤","ì˜í™”","ë“œë¼ë§ˆ","ì½˜í…ì¸ "], ("ë¬¸í™”/ì½˜í…ì¸ ", "ì˜í™”Â·ë°©ì†¡Â·í”Œë«í¼")),
    ]
    for keys, (nm, desc) in rules:
        if any(k in w for k in keys):
            return {"name": nm, "desc": desc}
    return {"name": "ì¼ë°˜/ì¢…í•©", "desc": "ê´‘ë²”ìœ„í•œ ì´ìŠˆ ë¬¶ìŒ"}

# llm_name_topics (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def llm_name_topics(topic_top_words: dict, model_name=MODEL_CHAT):
    """LLMì„ ì‚¬ìš©í•˜ê±°ë‚˜ íœ´ë¦¬ìŠ¤í‹±ì„ ì‚¬ìš©í•˜ì—¬ í† í”½ ì´ë¦„ ë¼ë²¨ë§"""
    if not USE_LLM or client is None or not LLM_OK:
        return {k: _heuristic_topic_name(v) for k, v in topic_top_words.items()}

    payload = {
        "topics": topic_top_words,
        "schema": {"Topic k": {"name": "ì§§ì€ ì´ë¦„", "desc": "í•œ ì¤„ ì„¤ëª…"}},
        "instruction": "ìœ„ 'topics'ì˜ ìƒìœ„ ë‹¨ì–´ë¥¼ ë³´ê³  ê° í† í”½ì— ëŒ€í•´ {name, desc}ë¥¼ ìƒì„±. "
                       "JSON ê°ì²´ë¡œ { 'Topic 0': {'name':'..','desc':'..'}, ... } í˜•ì‹ë§Œ ë°˜í™˜. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€."
    }
    try:
        r = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role":"system","content":"ë„ˆëŠ” ì£¼ì œ ë¼ë²¨ëŸ¬ë‹¤. JSON ê°ì²´ë§Œ ë°˜í™˜í•œë‹¤."},
                {"role":"user","content": json.dumps(payload, ensure_ascii=False)}
            ],
            temperature=0.2,
            response_format={"type": "json_object"},
        )
        txt = (r.choices[0].message.content or "").strip()
        data = _parse_json_safely(txt)
        if not isinstance(data, dict) or not data:
            raise ValueError("ë¹ˆ JSON")
        for k, words in topic_top_words.items():
            if k not in data or "name" not in data[k]:
                data[k] = _heuristic_topic_name(words)
        return data
    except Exception:
        return {k: _heuristic_topic_name(v) for k, v in topic_top_words.items()}

# compute_sentiment_SI (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def compute_sentiment_SI(df_work: pd.DataFrame, cv: CountVectorizer, lex: dict) -> pd.DataFrame:
    """ê°„ë‹¨ í† í° ê¸°ì¤€ í‰ê· ê°ì„± S, í‰ê· ì ˆëŒ€ê°ì„± I (CV, Lexicon ì™¸ë¶€ ì£¼ì…)"""
    df = df_work.copy()
    texts = (df["title"].fillna("") + " " + df["content"].fillna("")).tolist()

    X = cv.transform(texts)
    vocab = np.array(cv.get_feature_names_out())

    rows, cols = X.nonzero()
    tok_by_row: Dict[int, List[str]] = {}
    for r, c in zip(rows, cols):
        tok_by_row.setdefault(r, []).append(vocab[c])

    S, I = [], []
    for r in range(X.shape[0]):
        vals = [lex.get(t, 0.0) for t in tok_by_row.get(r, [])]
        if vals:
            S.append(float(np.mean(vals)))
            I.append(float(np.mean(np.abs(vals))))
        else:
            S.append(0.0); I.append(0.0)
    df["S"], df["I"] = S, I
    return df

# get_sentiment_for_text (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def get_sentiment_for_text(txt: str, senti_pack: dict) -> Tuple[float, float]:
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•´ S/I ì ìˆ˜ ê³„ì‚°"""
    if not senti_pack or not senti_pack.get('cv') or not senti_pack.get('lex') or not txt:
        return 0.0, 0.0

    try:
        cv = senti_pack['cv']
        lex = senti_pack['lex']

        X = cv.transform([txt])
        vocab = np.array(cv.get_feature_names_out())

        rows, cols = X.nonzero()
        if not np.any(cols):
            return 0.0, 0.0

        vals = [lex.get(vocab[c], 0.0) for c in cols]
        if vals:
            s = float(np.mean(vals))
            i = float(np.mean(np.abs(vals)))
            return s, i
    except Exception:
        return 0.0, 0.0
    return 0.0, 0.0

# get_recent_popular_words (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def get_recent_popular_words(df_all_data: pd.DataFrame,
                             end_date: datetime.date,
                             topic_id: int = None,
                             k: int = 10) -> List[str]:
    """íŠ¹ì • í† í”½/ê¸°ê°„/Goodë“±ê¸‰ ë¬¸ì„œì—ì„œ Top-K ë¹ˆë„ ë‹¨ì–´ ì¶”ì¶œ"""
    if df_all_data is None or df_all_data.empty or 'date' not in df_all_data.columns or 'topic' not in df_all_data.columns or 'quality_label' not in df_all_data.columns:
        return []

    try:
        df = df_all_data.copy()
        if not pd.api.types.is_datetime64_any_dtype(df['date']):
             df['date'] = pd.to_datetime(df['date'], errors='coerce')

        # ë‚ ì§œ í•„í„°ë§
        end_date_pd = pd.to_datetime(end_date)
        start_date_pd = end_date_pd - pd.Timedelta(days=30)

        df_filtered = df[
            (df['date'] >= start_date_pd) &
            (df['date'] <= end_date_pd) &
            (df['quality_label'] == 'good')
        ]

        if topic_id is not None:
            df_filtered = df_filtered[df_filtered['topic'] == topic_id]

        if df_filtered.empty:
            return []

        texts = (df_filtered["title"].fillna("") + " " + df_filtered["content"].fillna("")).tolist()

        cv_recent = CountVectorizer(max_features=2000, stop_words=STOPWORDS_KO)
        X_recent = cv_recent.fit_transform(texts)

        word_counts = X_recent.sum(axis=0)
        words_freq = [(word, word_counts[0, idx]) for word, idx in cv_recent.vocabulary_.items()]
        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

        return [word for word, freq in words_freq[:k]]

    except Exception as e:
        return []

# fit_ols (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def fit_ols(y, X):
    """OLS íšŒê·€ ëª¨ë¸ ì í•©"""
    X = X.apply(pd.to_numeric, errors='coerce').fillna(0)
    y = y.apply(pd.to_numeric, errors='coerce').fillna(0)

    valid_idx = (y != 0) | (X != 0).any(axis=1)
    y_valid = y[valid_idx]
    X_valid = X[valid_idx]

    if len(y_valid) < 2:
        raise ValueError("ìœ íš¨í•œ ë°ì´í„° í¬ì¸íŠ¸ê°€ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. íšŒê·€ ë¶„ì„ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    Xc = sm.add_constant(X_valid, has_constant="add")
    model = sm.OLS(y_valid.astype(float), Xc, missing="drop")
    return model.fit()

# tidy_summary (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def tidy_summary(res: sm.regression.linear_model.RegressionResultsWrapper, max_rows=200):
    """OLS ê²°ê³¼ë¥¼ ê¹”ë”í•œ DataFrameìœ¼ë¡œ ë³€í™˜"""
    s = []
    for name, coef, se, t, p in zip(res.params.index, res.params.values, res.bse.values, res.tvalues, res.pvalues):
        s.append({"term": name, "coef": float(coef), "se": float(se), "t": float(t), "p": float(p)})
    df = pd.DataFrame(s)
    if len(df) > max_rows:
        return df.head(max_rows)
    return df

# get_suspected_stopwords (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def get_suspected_stopwords(df_all_data: pd.DataFrame, k: int = 50) -> List[str]:
    """í† í”½/ì„±ê³¼ì™€ ë¬´ê´€í•˜ê²Œ ê°€ì¥ ìì£¼ ì“°ì´ëŠ” ì¼ë°˜ ë‹¨ì–´(ë¶ˆìš©ì–´ í›„ë³´) ì¶”ì¶œ"""
    if df_all_data is None or df_all_data.empty:
        return []
    try:
        texts = (df_all_data["title"].fillna("") + " " + df_all_data["content"].fillna("")).tolist()

        cv_nostop = CountVectorizer(max_features=k,
                                    min_df=0.1,
                                    ngram_range=(1,1))
        cv_nostop.fit(texts)
        common_words = cv_nostop.get_feature_names_out()

        final_suspects = [w for w in common_words if w not in STOPWORDS_KO]
        return final_suspects
    except Exception as e:
        return []

# ì‹œê°í™” í•¨ìˆ˜ (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def create_sentiment_gauge_S(s_val: float, s_target: float, lexicon_max: float = 1.0):
    """Plotlyì˜ Indicatorë¥¼ ì‚¬ìš©í•´ ê°ì„± ì ìˆ˜(S) ê²Œì´ì§€ ìƒì„±"""
    fig = go.Figure()
    fig.add_trace(go.Indicator(
        mode = "gauge+number",
        value = s_val,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "ğŸ’– ê°ì„± ì ìˆ˜ (S)", 'font': {'size': 18}},
        number = {'font': {'size': 24}},
        gauge = {
            'axis': {'range': [-lexicon_max, lexicon_max], 'tickwidth': 1, 'tickcolor': "darkblue"},
            'bar': {'color': "#0072F0" if s_val >= 0 else "#E63946", 'thickness': 0.4},
            'bgcolor': "white",
            'borderwidth': 2,
            'bordercolor': "#CCCCCC",
            'steps': [
                {'range': [-lexicon_max, -0.05], 'color': 'rgba(230, 57, 70, 0.1)'},
                {'range': [-0.05, 0.05], 'color': 'rgba(200, 200, 200, 0.2)'},
                {'range': [0.05, lexicon_max], 'color': 'rgba(0, 114, 240, 0.1)'}
            ],
            'threshold': {
                'line': {'color': "green", 'width': 3},
                'thickness': 0.75,
                'value': s_target
            }
        }
    ))
    fig.update_layout(height=180, margin=dict(l=20, r=20, t=40, b=10))
    return fig

def create_sentiment_gauge_I(i_val: float, i_target: float, lexicon_max: float = 1.0):
    """Plotlyì˜ Indicatorë¥¼ ì‚¬ìš©í•´ ê°ì„± ê°•ë„(I) ê²Œì´ì§€ ìƒì„±"""
    fig = go.Figure()
    fig.add_trace(go.Indicator(
        mode = "gauge+number",
        value = i_val,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "ğŸ’– ê°ì„± ê°•ë„ (I)", 'font': {'size': 18}},
        number = {'font': {'size': 24}},
        gauge = {
            'axis': {'range': [0, lexicon_max], 'tickwidth': 1, 'tickcolor': "darkblue"},
            'bar': {'color': "#F4A261", 'thickness': 0.4},
            'bgcolor': "white",
            'borderwidth': 2,
            'bordercolor': "#CCCCCC",
            'steps': [
                {'range': [0, lexicon_max * 0.33], 'color': 'rgba(200, 200, 200, 0.2)'},
                {'range': [lexicon_max * 0.33, lexicon_max * 0.66], 'color': 'rgba(244, 162, 97, 0.1)'},
                {'range': [lexicon_max * 0.66, lexicon_max], 'color': 'rgba(244, 162, 97, 0.2)'},
            ],
            'threshold': {
                'line': {'color': "green", 'width': 3},
                'thickness': 0.75,
                'value': i_target
            }
        }
    ))
    fig.update_layout(height=180, margin=dict(l=20, r=20, t=40, b=10))
    return fig

# @st.cache_resourceë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë˜í¼ í•¨ìˆ˜ (Streamlit progress barë¥¼ ì „ë‹¬)
@st.cache_resource(show_spinner=False)
def cached_lda_run_wrapper(texts_tuple, n_topics, max_features, batch_size, n_epochs):
    # run_lda_topics_streaming ë‚´ë¶€ì—ì„œ st.progressë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” progress_callbackì„ st.progressë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
    return run_lda_topics_streaming(
        list(texts_tuple), n_topics=n_topics,
        max_features=None,
        batch_size=batch_size, n_epochs=n_epochs,
        progress_callback=st.progress
    )

# train_logreg_with_progress_wrapper (ì´ì „ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
def train_logreg_with_progress_wrapper(texts, labels, stoplist=None, ngram_range=(1,2),
                               epochs=3, batch_size=2000, k_show=20, seed=42):
    # train_logreg_with_progress í•¨ìˆ˜ê°€ train_quality_classifierë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤.
    raise NotImplementedError("train_logreg_with_progress_wrapperëŠ” train_quality_classifierë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤.")

# analytics_core.py íŒŒì¼ì— ì•„ë˜ í•¨ìˆ˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. (ê¸°ì¡´ evaluate_comparison_models ê·¼ì²˜ì— ë‘ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.)

def evaluate_baseline_models(df_full: pd.DataFrame, 
                             models: List[str] = ["SGDClassifier", "LogisticRegression", "RandomForestClassifier"]):
    """
    Baseline ëª¨ë“œ (ìˆ˜ì¹˜ í”¼ì²˜ë§Œ)ë¥¼ ì‚¬ìš©í•˜ì—¬ StratifiedKFold êµì°¨ ê²€ì¦ìœ¼ë¡œ 3ê°€ì§€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.
    """
    df_trainable = df_full[df_full['quality_label'] != 'medium'].copy().reset_index(drop=True)
    if df_trainable.empty:
        return {"error": "í‰ê°€ ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆê±°ë‚˜ í† í”½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."} # í† í”½ì´ ì—†ì–´ë„ Baselineì€ ê°€ëŠ¥í•˜ì§€ë§Œ, ë™ì¼í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì‚¬ìš©

    # 1. ì›ë³¸ ë°ì´í„° ì¤€ë¹„ (ìˆ˜ì¹˜ í”¼ì²˜ë§Œ)
    y = df_trainable["quality_label"].values
    X_num_raw = df_trainable[BASELINE_FEATURES].values # BASELINE_FEATURESë§Œ ì‚¬ìš©
    
    X_full = X_num_raw # X_fullì€ ìˆ˜ì¹˜ í”¼ì²˜ë§Œ í¬í•¨

    # 2. Stratified K-Fold ì„¤ì • (5-Fold ì‚¬ìš©)
    N_SPLITS = 5
    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)
    
    model_metrics = {m: {"Accuracy": [], "F1_Good": [], "CM_Total": np.zeros((2, 2))} for m in models}
    
    # 3. K-Fold ë°˜ë³µ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ë¡œì§ ì‹¤í–‰)
    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X_full, y)):
        
        X_train_fold, X_test_fold = X_full[train_idx].copy(), X_full[test_idx].copy()
        y_train_fold, y_test_fold = y[train_idx], y[test_idx]
        
        # â˜…â˜…â˜… Fold ë‚´ë¶€ì—ì„œ ìˆ˜ì¹˜ í”¼ì²˜ë§Œ ìŠ¤ì¼€ì¼ë§ (í•„ìˆ˜) â˜…â˜…â˜…
        # Baselineì€ X_full ì „ì²´ê°€ ìˆ˜ì¹˜ í”¼ì²˜ì´ë¯€ë¡œ ì „ì²´ì— ìŠ¤ì¼€ì¼ë§ ì ìš©
        scaler = RobustScaler()
        scaler.fit(X_train_fold)
        
        X_train_fold = scaler.transform(X_train_fold)
        X_test_fold = scaler.transform(X_test_fold)
        
        # 4. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (Advancedì™€ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©)
        for model_name in models:
            try:
                if model_name == "SGDClassifier":
                    clf = SGDClassifier(loss="log_loss", alpha=1e-5, random_state=42 + fold_idx, class_weight='balanced')
                elif model_name == "LogisticRegression":
                    clf = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42 + fold_idx, class_weight='balanced')
                elif model_name == "RandomForestClassifier":
                    clf = RandomForestClassifier(n_estimators=100, random_state=42 + fold_idx, class_weight='balanced')
                else:
                    continue
                    
                clf.fit(X_train_fold, y_train_fold)
                y_pred = clf.predict(X_test_fold)
                
                # ë©”íŠ¸ë¦­ ëˆ„ì 
                acc = accuracy_score(y_test_fold, y_pred)
                report = classification_report(y_test_fold, y_pred, output_dict=True, zero_division=0)
                f1_good = report['good']['f1-score']
                cm_fold = confusion_matrix(y_test_fold, y_pred, labels=['good', 'bad'])
                
                model_metrics[model_name]["Accuracy"].append(acc)
                model_metrics[model_name]["F1_Good"].append(f1_good)
                model_metrics[model_name]["CM_Total"] += cm_fold
                
            except Exception as e:
                pass 
                
    # 5. ìµœì¢… ê²°ê³¼ ì •ë¦¬
    final_results = {}
    for model_name, metrics in model_metrics.items():
        if metrics["Accuracy"]:
            final_results[model_name] = {
                "Accuracy_Mean": np.mean(metrics["Accuracy"]),
                "F1_Good_Mean": np.mean(metrics["F1_Good"]),
                # ìƒì„¸ ë¦¬í¬íŠ¸ëŠ” Advancedì—ì„œë§Œ ì¶œë ¥í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” í‰ê· ë§Œ ë°˜í™˜
            }
        else:
            final_results[model_name] = {"error": "êµì°¨ ê²€ì¦ ì¤‘ í•™ìŠµëœ Foldê°€ ì—†ìŠµë‹ˆë‹¤."}
            
    return final_results

# ================== [ì‹ ê·œ] ìë™ íŒŒì¸íŠœë‹ ë¡œì§ ==================

def generate_jsonl_content(df_analysis: pd.DataFrame, topic_labels: dict) -> str:
    """
    [ìˆ˜ì • ì™„ë£Œ] df_analysisë¥¼ ê¸°ë°˜ìœ¼ë¡œ GPT Fine-tuningìš© 'messages' í˜•ì‹ì˜ JSONL ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.
    (OpenAI APIì˜ ìµœì‹  íŒŒì¸íŠœë‹ ìš”êµ¬ í˜•ì‹ì— ë§ì¶¤)
    """
    
    # 1. 'good' í’ˆì§ˆ ì½˜í…ì¸ ë§Œ í•„í„°ë§
    df_good = df_analysis[df_analysis['quality_label'] == 'good'].copy()
    if df_good.empty:
        raise ValueError("Good ì½˜í…ì¸ ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ í•™ìŠµ ë°ì´í„°ì…‹ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    # 2. í† í”½ ì´ë¦„ ë§¤í•‘
    def map_topic_name(topic_id):
        key = f"Topic {topic_id}"
        return topic_labels.get(key, {}).get('name', 'ì¼ë°˜')

    df_good['topic_name'] = df_good['topic'].apply(map_topic_name)
    
    jsonl_data = []
    
    for index, row in df_good.iterrows():
        topic_name = row['topic_name']
        content = str(row['content']) if pd.notna(row['content']) else ''
        title = str(row['title']) if pd.notna(row['title']) else ''
        
        # 3. GPT Fine-tuningì„ ìœ„í•œ 'messages' í˜•ì‹ìœ¼ë¡œ ë°ì´í„° êµ¬ì¡°í™”
        messages_array = [
            {"role": "system", "content": SYSTEM_PROMPT_FT},                             # ì‹œìŠ¤í…œ ì—­í• 
            {"role": "user", "content": f"ì£¼ì œ: {topic_name}\në³¸ë¬¸: {content}"},         # ì‚¬ìš©ì ì…ë ¥ (í”„ë¡¬í”„íŠ¸)
            {"role": "assistant", "content": title}                                     # ëª¨ë¸ì˜ ê¸°ëŒ€ ì¶œë ¥ (ì™„ì„±)
        ]
        
        jsonl_data.append({"messages": messages_array})

    # 4. JSON Lines ë¬¸ìì—´ë¡œ ë³€í™˜ (to_json ëŒ€ì‹  ìˆ˜ë™ìœ¼ë¡œ ë³€í™˜)
    jsonl_str = "\n".join([json.dumps(item, ensure_ascii=False) for item in jsonl_data])
    
    return jsonl_str


def run_finetuning_job(df_analysis: pd.DataFrame, topic_labels: dict, base_model: str) -> str:
    """
    GPT íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ì„ ìƒì„±, ì—…ë¡œë“œí•˜ê³  í•™ìŠµ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.
    ì„±ê³µ ì‹œ job_idë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not client or not LLM_OK:
        raise RuntimeError("OpenAI API í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì¸ì¦ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
    
    # 1. JSONL ë°ì´í„° ìƒì„±
    jsonl_content = generate_jsonl_content(df_analysis, topic_labels)
    
    # 2. íŒŒì¼ì„ IO ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ì—…ë¡œë“œ
    file_io = io.BytesIO(jsonl_content.encode('utf-8'))
    file_io.name = "llm_training_data.jsonl"
    
    # API í˜¸ì¶œ: íŒŒì¼ ì—…ë¡œë“œ
    st.info("ğŸš€ 1/2 ë‹¨ê³„: í•™ìŠµ ë°ì´í„°ì…‹ì„ OpenAIì— ì—…ë¡œë“œ ì¤‘...")
    
    # try-except ë¸”ë¡ì„ ì‚¬ìš©í•˜ì—¬ API ì˜¤ë¥˜ë¥¼ í¬ì°©í•©ë‹ˆë‹¤.
    try:
        uploaded_file = client.files.create(
            file=file_io,
            purpose="fine-tune"
        )
        file_id = uploaded_file.id
        st.success(f"âœ… í•™ìŠµ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ (ID: {file_id})")
    except APIError as e:
        raise APIError(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
    except Exception as e:
        raise Exception(f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    
    # API í˜¸ì¶œ: í•™ìŠµ ì‘ì—… ì‹œì‘
    st.info(f"â³ 2/2 ë‹¨ê³„: {base_model} ëª¨ë¸ íŒŒì¸íŠœë‹ í•™ìŠµ ì‘ì—… ì‹œì‘ ì¤‘...")
    try:
        job = client.fine_tuning.jobs.create(
            training_file=file_id, 
            model=base_model
        )
        return job.id
    except APIError as e:
        # ğŸš« ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ì—¬ ì›ë˜ì˜ ì˜ˆì™¸ë¥¼ ê·¸ëŒ€ë¡œ ì „íŒŒí•©ë‹ˆë‹¤.
        #    OpenAI SDKì˜ APIErrorëŠ” ì¸ìë¥¼ ìë™ìœ¼ë¡œ ì±„ìš°ë¯€ë¡œ, ì¸ì ì—†ì´ ë‹¤ì‹œ raise í•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.
        #    (ë˜ëŠ” raise eë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.)
        raise e  # ì›ë˜ ë°œìƒí•œ ì˜ˆì™¸ë¥¼ ê·¸ëŒ€ë¡œ ì „íŒŒ
    except Exception as e:
        # ğŸš« ì´ ë¶€ë¶„ë„ APIError ì¸ì ëˆ„ë½ ë¬¸ì œë¥¼ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì¼ë°˜ Exceptionìœ¼ë¡œ ì „íŒŒí•©ë‹ˆë‹¤.
        raise Exception(f"í•™ìŠµ ì‘ì—… ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

# analytics_core.py ì— ì¶”ê°€ (ê¸°ì¡´ build_topic_term_bank_logreg í•¨ìˆ˜ ëŒ€ì‹  ì‚¬ìš© ê°€ëŠ¥)
from sklearn.pipeline import Pipeline # Pipeline import ì¶”ê°€

def build_topic_term_bank_rf_logratio(df_all: pd.DataFrame,
                                      topn: int = 50,
                                      min_samples_block: int = 10) -> dict:
    """ëœë¤ í¬ë ˆìŠ¤íŠ¸ í”¼ì²˜ ì¤‘ìš”ë„ì™€ ë¡œê·¸ ë¹„ìœ¨ì„ ì‚¬ìš©í•˜ì—¬ í† í”½ë³„ ë‹¨ì–´ ì€í–‰ êµ¬ì¶•"""
    bank = {}

    if 'topic' not in df_all.columns:
        return bank

    valid_topics = df_all["topic"].dropna().unique()
    unique_topics = sorted([t for t in valid_topics if pd.notna(t)])

    for t in unique_topics:
        try:
            topic_int = int(t)
        except ValueError:
            continue

        df_topic = df_all[df_all["topic"] == t]
        df_train = df_topic[df_topic["quality_label"] != "medium"].copy() # copy() ì¶”ê°€

        if len(df_train) < min_samples_block:
            bank[topic_int] = {
                "status": "error",
                "message": f"ìƒ˜í”Œ ì™„ì „ ë¶€ì¡± (N={len(df_train)}, ìµœì†Œ {min_samples_block} í•„ìš”)"
            }
            continue

        texts = (df_train["content"].fillna("")).tolist() # ì œëª©ì„ ì œì™¸í•˜ê³  ë³¸ë¬¸ë§Œ ì‚¬ìš©
        y = (df_train["quality_label"] == "good").astype(int).values

        try:
            if len(np.unique(y)) < 2:
                bank[topic_int] = {"status": "error", "message": "ë‹¨ì¼ ë¼ë²¨ë§Œ ì¡´ì¬"}
                continue

            # 1. TF-IDFì™€ RandomForest ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸
            tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1), max_features=5000, min_df=3, stop_words=STOPWORDS_KO)
            
            # í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ì–»ê¸° ìœ„í•´ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
            clf_rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', max_depth=10)
            
            pipeline = Pipeline([
                ('tfidf', tfidf_vectorizer),
                ('clf', clf_rf)
            ])
            pipeline.fit(texts, y)

            # 2. í”¼ì²˜ ì¤‘ìš”ë„ ì¶”ì¶œ (RandomForest)
            importances = pipeline['clf'].feature_importances_
            vocab = np.array(pipeline['tfidf'].get_feature_names_out())

            # 3. ë¡œê·¸ ë¹„ìœ¨(Log Ratio)ì„ ê³„ì‚°í•˜ì—¬ ë°©í–¥ì„± ë¶€ì—¬ (Good vs Bad)
            X_count_good = pipeline['tfidf'].transform(df_train[df_train['quality_label'] == 'good']['content'].fillna("")).sum(axis=0)
            X_count_bad = pipeline['tfidf'].transform(df_train[df_train['quality_label'] == 'bad']['content'].fillna("")).sum(axis=0)
            
            # ê° í‚¤ì›Œë“œê°€ Good/Badì—ì„œ ë‚˜íƒ€ë‚œ íšŸìˆ˜
            N_good = X_count_good.A1 + 1 # +1 ìŠ¤ë¬´ë”©
            N_bad = X_count_bad.A1 + 1 # +1 ìŠ¤ë¬´ë”©
            
            # ì „ì²´ ë¬¸ì„œ ìˆ˜
            D_good = len(df_train[df_train['quality_label'] == 'good'])
            D_bad = len(df_train[df_train['quality_label'] == 'bad'])

            # Log Ratio (í™•ë¥  ë¹„ìœ¨ì˜ ë¡œê·¸)
            # log_ratio > 0: Goodì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ë” ìì£¼ ë“±ì¥
            # log_ratio < 0: Badì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ë” ìì£¼ ë“±ì¥
            log_ratio = np.log((N_good / D_good) / (N_bad / D_bad))

            # 4. ì¤‘ìš”ë„ì™€ ë¡œê·¸ ë¹„ìœ¨ì„ ê²°í•©í•˜ì—¬ ìˆœìœ„ ê²°ì •
            # ì¤‘ìš”ë„ê°€ ë†’ê³  (RF) ë¡œê·¸ ë¹„ìœ¨ì´ ì–‘ìˆ˜ì¸ (Good ì„ í˜¸) ë‹¨ì–´ ìˆœìœ„
            combined_score_good = importances * (log_ratio > 0)
            order_good = np.argsort(combined_score_good)[::-1]
            
            # ì¤‘ìš”ë„ê°€ ë†’ê³  (RF) ë¡œê·¸ ë¹„ìœ¨ì´ ìŒìˆ˜ì¸ (Bad ì„ í˜¸) ë‹¨ì–´ ìˆœìœ„
            combined_score_bad = importances * (log_ratio < 0)
            order_bad = np.argsort(combined_score_bad)[::-1]

            # 5. ìµœì¢… ëª©ë¡ ìƒì„± (Log Ratio ê°’ì„ Scoreë¡œ ì‚¬ìš©)
            good_terms = []
            for i in order_good:
                if combined_score_good[i] > 0: # ê¸ì • ë°©í–¥ì„±ì„ ê°€ì§„ ë‹¨ì–´ë§Œ
                    good_terms.append((vocab[i], float(log_ratio[i])))
                if len(good_terms) >= topn: break

            bad_terms = []
            for i in order_bad:
                if combined_score_bad[i] > 0: # ë¶€ì • ë°©í–¥ì„±ì„ ê°€ì§„ ë‹¨ì–´ë§Œ
                    bad_terms.append((vocab[i], float(log_ratio[i])))
                if len(bad_terms) >= topn: break
            
            # ì „ì²´ ë¹ˆë„ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            cv_all = CountVectorizer(max_features=topn, min_df=3, stop_words=STOPWORDS_KO)
            X_all = cv_all.fit_transform(df_train["content"].fillna(""))
            counts = np.asarray(X_all.sum(axis=0)).ravel()
            vocab_all = np.array(cv_all.get_feature_names_out())
            order_all = np.argsort(counts)[::-1]
            all_terms = [(vocab_all[i], float(counts[i])) for i in order_all]

            bank[topic_int] = {
                "good": good_terms,
                "bad": bad_terms,
                "all": all_terms,
                "status": "ok",
                "message": f"ì„±ê³µ (N={len(df_train)})",
                "warning": f"ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì¤‘ìš”ë„ ê¸°ë°˜ ì¶”ì¶œ. ScoreëŠ” Log Ratio ê°’ì…ë‹ˆë‹¤."
            }
        except Exception as e:
            bank[topic_int] = {
                "status": "error",
                "message": f"ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}"
            }
    return bank