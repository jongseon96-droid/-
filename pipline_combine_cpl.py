# -*- coding: utf-8 -*-
"""
[ìˆ˜ì •] Streamlit App v2.9.3 â€” íŒŒì¸íŠœë‹ íŒŒíŠ¸ (TAB3) ì œê±° ë° ìµœì¢… ë°ëª¨ ëª¨ë“œ ì „í™˜
- CONFIG: [ì„¤ì •] íŒŒì¸íŠœë‹ ëª¨ë¸ IDê°€ ì´ë¯¸ ì„¤ì •ë˜ì—ˆë‹¤ê³  ê°€ì •í•˜ê³ , 'AI ì œëª© ìƒì„±ê¸°' ëª¨ë“œë¡œ ì „í™˜ (USE_FINETUNED_MODEL = True).
- TAB ë¦¬ìŠ¤íŠ¸: [ìˆ˜ì •] TAB3(íŒŒì¸íŠœë‹)ì„ ì œê±°í•˜ê³ , TAB4(ëª¨ë¸ ê´€ë¦¬ì)ë¥¼ TAB3ìœ¼ë¡œ ë³€ê²½.
- TAB1: [ìˆ˜ì •] 'AI ì œëª© ìƒì„±ê¸°' ëª¨ë“œë¡œ ê³ ì •ë˜ì–´ ì‘ë™.
- TAB3 (ëª¨ë¸ ê´€ë¦¬ì): [ìœ ì§€] ê¸°ì¡´ TAB4ì˜ ëª¨ë“  ëª¨ë¸ ê´€ë¦¬ ë¡œì§ ìœ ì§€.
"""

import os, io, json, time, re as regx
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px # ì‹œê°í™” ì¶”ê°€
import plotly.graph_objects as go # ê²Œì´ì§€ ì°¨íŠ¸ìš©
import datetime 
from typing import List, Dict, Tuple

# ==== OpenAI SDK ====
from openai import OpenAI
from openai import APIError, RateLimitError

# ==== ML / NLP ====
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.preprocessing import RobustScaler
from sklearn.utils import shuffle as sk_shuffle
from sklearn.feature_selection import chi2
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split

# ==== íšŒê·€ ====
import statsmodels.api as sm

# [ìˆ˜ì •] í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì •ì˜ (ëª¨ë“  ì•/ë’¤ íŠ¹ìˆ˜ ê³µë°± ì œê±°)
STOPWORDS_KO = [
    # ì¡°ì‚¬/ì–´ë¯¸ (ë§¤ìš° ë¹ˆë²ˆ)
    "ì…ë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ê°™ìŠµë‹ˆë‹¤", "ìˆìŠµë‹ˆë‹¤", "ìˆëŠ”", "ê²ƒì…ë‹ˆë‹¤", "í–ˆë‹¤", "ë“±", "ì´", "ê·¸", "ì €",
    "ìˆ˜", "ê²ƒ", "ë°", "A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z",
    "ì—ì„œ", "ìœ¼ë¡œ", "í•˜ëŠ”", "ì„", "ë¥¼", "ì€", "ëŠ”", "ì´", "ê°€", "ì˜", "ì—", "ì™€", "ê³¼", "ë„", "ê³ ", "ë¼ëŠ”",

    # ì¼ë°˜ ëª…ì‚¬ (ì‹ í˜¸ ë°©í•´)
    "ë¸”ë¡œê·¸", "í¬ìŠ¤íŒ…", "ì˜¤ëŠ˜", "ì´ë²ˆ", "ë‹¤ì–‘í•œ", "ê´€ë ¨", "ë‚´ìš©", "ì •ë³´", "ì •ë§", "ë°”ë¡œ", "ì§€ê¸ˆ", "ìƒê°",
    "ê²½ìš°", "ëŒ€í•´", "ëŒ€ë¶€ë¶„", "ë•Œë¬¸", "ê°€ì§€", "í†µí•´", "ìœ„í•´", "ëŒ€í•œ", "í†µí•œ", "ë”°ë¼","gt", "https", "lt", "ê°€ì¥", "ê°™ì€", "ê²ƒìœ¼ë¡œ", "ê²ƒì€", "ê²ƒì´", "ê²ƒì´ë‹¤", 
    "ê´‘ê³ ", "ê·¸ë¦¬ê³ ", "ê¸°ì‚¬", "ê¸°ì‚¬ë¥¼", "ë‰´ìŠ¤", "ë‹¤ë¥¸", "ë§ì€", "ì•„ë‹ˆë¼", 
    "ì–´ë–¤", "ì–¸ë¡ ", "ì‹ ë¬¸ê³¼ë°©ì†¡", "ì´ëŸ¬í•œ", "ì´ëŸ°", "ì´ë¥¼", "ìˆë‹¤", "ìˆì—ˆë‹¤", 
    "ì§€ë‚œ", "ì§€ì—­", "ì½˜í…ì¸ ", "ì½˜í…ì¸ ë¥¼", "í•˜ì§€ë§Œ", "í•œë‹¤","ë§Œë‚˜ë³´ì„¸ìš”", "2025", "ì—†ë‹¤", "ìœ„í•œ", "the", "com", "www", "of", "news", "and", "to", "2022" ,"uk" ,"2020", "in", "1ë©´", "ë†’ì€", "ë˜í•œ", "ë‚˜íƒ€ë‚¬ë‹¤", "ë§ì´",
    "naver", "í•œëˆˆì—", "2020ë…„", "ëŠ˜ì–´ë‚œ", "ëŒ“ê¸€", "íŠ¹íˆ", "ê·¸ë¦¼", "ëŒ€ë¹„", "ë•Œë¬¸ì—", "ì—†ëŠ”", "ê²ƒì„", "ë•Œë¬¸ì´ë‹¤", "ê·¸ëŸ¬ë‚˜", "ìˆë‹¤ëŠ”", "ë¬´ìŠ¨ì¼ì´", "ë¼ê³ ", "í•¨ê»˜", "í•˜ê³ ", "ë“±ì„",
    "ì–´ë–»ê²Œ", "í™œìš©", "ë§í–ˆë‹¤", "ap", "niemanlab", "esg", "ì£¼ëª©ë°›ëŠ”", "ê°•ì¡°í•œ", "ê·¸ëŠ”", "ìˆìœ¼ë©°",
    "blog", "nft", "kpfjra", "ì—ì„œë„", "quibi", "fast", "ì´í›„", "êµ¬ë¶„", "ë¹„í•´", "ë†’ì•˜ë‹¤", "2021","1ì›”", "2ì›”", "3ì›”", "4ì›”", "5ì›”", "6ì›”", 
    "7ì›”", "8ì›”", "9ì›”", "10ì›”", "11ì›”", "12ì›”",
    "1990", "1991", "1992", "1993", "1994", "1995", "1996", "1997", "1998", "1999",
    "2000", "2001", "2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009",
    "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019",
    "2020","ë³´ë‹ˆ", "ìˆê³ ", "ë¼ëŠ”" ,"ì•Šì•˜ë‹¤", "ì—¬ëŸ¬", "ëë‹¤", "ìš°ë¦¬ê°€", "ì—†ì—ˆë‹¤", "ì¢‹ì€", "ë‚˜ëŠ”","ê³µì˜ë°©ì†¡ì‚¬ì˜", "ê¸°ì‚¬ëŠ”", "ì‹ ë¬¸ê³¼", "ë°©ì†¡",
    "1990ë…„", "1991ë…„", "1992ë…„", "1993ë…„", "1994ë…„", "1995ë…„", "1996ë…„", "1997ë…„", "1998ë…„", "1999ë…„",
    "2000ë…„", "2001ë…„", "2002ë…„", "2003ë…„", "2004ë…„", "2005ë…„", "2006ë…„", "2007ë…„", "2008ë…„", "2009ë…„",
    "2010ë…„", "2011ë…„", "2012ë…„", "2013ë…„", "2014ë…„", "2015ë…„", "2016ë…„", "2017ë…„", "2018ë…„", "2019ë…„",
    "2020ë…„", "2021ë…„", "2022ë…„", "2023ë…„", "2024ë…„", "2025ë…„"
    
    # ìŠ¤í¬ë¦°ìƒ·ì—ì„œ ë³´ì¸ ë¬¸ì œ ë‹¨ì–´ë“¤
    "2024", "2023", "ai", "2024ë…„", "2023ë…„", "ã…‹ã…‹", "ã…ã…"
]


# ================== CONFIG ==================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
USE_LLM = len(OPENAI_API_KEY) > 0
client = OpenAI(api_key=OPENAI_API_KEY) if USE_LLM else None
MODEL_CHAT = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o-mini-2024-07-18")

# [ì‹ ê·œ] íŒŒì¸íŠœë‹ ì„¤ì • (ì™„ë£Œ ê°€ì •)
# TODO: íŒŒì¸íŠœë‹ ì™„ë£Œ ì‹œ ì—¬ê¸°ì— ëª¨ë¸ IDë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.
#FINETUNED_MODEL_ID = "ft:gpt-4o-mini-2024-07-18:::CWPoHwfK"  # <-- ì—¬ê¸°ì— ì‹¤ì œ ID ì…ë ¥ (íŒŒì¸íŠœë‹ ì™„ë£Œ ê°€ì •)
USE_FINETUNED_MODEL = True # [ìˆ˜ì •] íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
FINETUNED_MODEL_ID = "ft:gpt-4o-mini-2024-07-18:::CWPoHwfK"
LLM_OK = False
if USE_LLM and client:
    try:
        client.models.list()
        LLM_OK = True
    except Exception:
        LLM_OK = False

# ========= Robust CSV Loader =========
def read_csv_robust(src, **kwargs) -> pd.DataFrame:
    """UploadedFile/bytes/path/file-like ëª¨ë‘ ì§€ì›. ì¸ì½”ë”©ê³¼ êµ¬ë¶„ì ìë™ ì¬ì‹œë„."""
    encodings = ["utf-8", "utf-8-sig", "cp949", "euc-kr", "latin1"]
    seps = [None, ",", "\t", ";"]

    # bytesë¡œ ì•ˆì „ ë³µì‚¬
    if hasattr(src, "getvalue"):        # Streamlit UploadedFile
        raw = src.getvalue()
    elif isinstance(src, (bytes, bytearray)):
        raw = bytes(src)
    elif isinstance(src, str):          # ê²½ë¡œ
        with open(src, "rb") as f:
            raw = f.read()
    else:                               # file-like
        raw = src.read()
        try:
            src.seek(0)
        except Exception:
            pass

    last_err = None
    for enc in encodings:
        for sep in seps:
            try:
                return pd.read_csv(io.BytesIO(raw), encoding=enc, sep=sep, engine="python", **kwargs)
            except Exception as e:
                last_err = e
                continue
    raise RuntimeError(f"CSV ë””ì½”ë”© ì‹¤íŒ¨: ë§ˆì§€ë§‰ ì˜¤ë¥˜={last_err}")

# ========= JSON íŒŒì„œ =========
def _parse_json_safely(txt: str):
    """ì½”ë“œíœìŠ¤/ì•ë’¤ ì“°ë ˆê¸°/í•œê¸€ BOM ì œê±°, ì²« JSON ê°ì²´/ë°°ì—´ë§Œ íŒŒì‹±"""
    if not isinstance(txt, str):
        raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
    t = txt.strip().lstrip("\ufeff")
    if t.startswith("```"):
        parts = t.split("```")
        if len(parts) >= 3:
            cand = parts[1] if parts[1].strip().startswith(("{","[")) else parts[2]
            t = cand
        else:
            t = t.replace("```","").strip()
    si, sj = t.find("["), t.rfind("]")
    oi, oj = t.find("{"), t.rfind("}")
    if 0 <= si < sj:
        return json.loads(t[si:sj+1])
    if 0 <= oi < oj:
        return json.loads(t[oi:oj+1])
    raise ValueError("LLM ì‘ë‹µì—ì„œ JSONì„ ì°¾ì§€ ëª»í•¨")

# ====== Candidate templates (í´ë°±) ======
NUM_RE  = regx.compile(r"\b(\d+|top\s*\d+|[0-9]+ë¶„)\b", regx.I)
TIME_BANK = ["ì˜¤ëŠ˜", "ì´ë²ˆ ì£¼", "ì£¼ë§", "ì§€ê¸ˆ", "ë°©ê¸ˆ", "ì´ë²ˆ ë‹¬", "10ì›”", "11ì›”", "12ì›”"]
HOWTO_BANK = ["ë°©-step", "ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤"]
ACTION_BANK = ["ì •ë¦¬", "ë¹„êµ", "ë¶„ì„", "ì„¤ëª…", "ì¶”ì²œ", "ì ê²€", "ì‹¤ë²•", "ê°€ì´ë“œ", "ì²´í¬ë¦¬ìŠ¤íŠ¸", "íŠœí† ë¦¬ì–¼", "Step-byí—˜"]
CTA_BANK = ["ì§ˆë¬¸", "ëŒ“ê¸€", "êµ¬ë…", "ê³µìœ ", "ì•Œë¦¼", "ì°¸ì—¬"]
LIST_BANK = ["Top 5", "Top 7", "3ê°€ì§€", "5ë¶„ ìš”ì•½", "í•œëˆˆì—"]
BRAND_HINT = ["í•œì–‘ëŒ€", "ì˜¤í”ˆAI", "ì¹´ì¹´ì˜¤", "êµ¬ê¸€", "MS", "ë„¤ì´ë²„"]
DEFAULT_CANDIDATES = TIME_BANK + HOWTO_BANK + ACTION_BANK + CTA_BANK + LIST_BANK + BRAND_HINT

# ========= Utility =========
def categorize_term(t: str) -> str:
    t_low = t.lower()
    if NUM_RE.search(t_low) or any(x in t for x in LIST_BANK): return "ìˆ«ì/ë¦¬ìŠ¤íŠ¸"
    if any(k in t for k in TIME_BANK): return "ì‹œê°„í‘œí˜„"
    if any(k in t for k in HOWTO_BANK): return "How-to/ê°€ì´ë“œ"
    if any(k in t for k in CTA_BANK): return "ì§ˆë¬¸/CTA"
    if any(k in t for k in ACTION_BANK): return "í–‰ë™ë™ì‚¬/í–‰ìœ„"
    if regx.match(r"[A-Z][a-zA-Z0-9]+", t) or "ëŒ€" in t or "ëŒ€í•™" in t or any(b in t for b in BRAND_HINT):
        return "ê³ ìœ ëª…ì‚¬/ë¸Œëœë“œ"
    return "ê¸°íƒ€"

# ==== article_id ìŠ¤í‚¤ë§ˆ ë³´ì •(ìµœì†Œ íŒ¨ì¹˜) ====
_CTRL = regx.compile(r"[\x00-\x1f\x7f]")

def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ì»¬ëŸ¼ëª… BOM/ê³µë°± ì œê±° + ì†Œë¬¸ì + ìŠ¤í˜ì´ìŠ¤â†’ì–¸ë”ìŠ¤ì½”ì–´"""
    df = df.copy()
    df.columns = (
        pd.Index(df.columns)
        .map(lambda c: str(c).lstrip("\ufeff").strip().lower().replace(" ", "_"))
    )
    return df

def coerce_article_id(df: pd.DataFrame) -> pd.DataFrame:
    """
    - ì»¬ëŸ¼ëª… ì •ê·œí™”
    - article_id ë³„ì¹­(id, doc_id, post_id ë“±)ì„ article_idë¡œ í†µì¼
    - ê°’ì˜ BOM/ì œì–´ë¬¸ì/ê³µë°± ì œê±° + ë¬¸ìì—´í™”
    """
    df = _normalize_columns(df.copy())
    aliases = ["article_id", "id", "doc_id", "post_id", "review_id", "news_id", "content_id"]
    found = None
    for a in aliases:
        if a in df.columns:
            found = a
            break
    if found is None:
        raise KeyError(f"CSVì— article_id ê³„ì—´ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (ì»¬ëŸ¼={list(df.columns)[:12]})")
    if found != "article_id":
        df = df.rename(columns={found: "article_id"})

    df["article_id"] = (
        df["article_id"]
        .astype(str)
        .str.replace("\ufeff", "", regex=False)
        .apply(lambda x: _CTRL.sub("", x))
        .str.strip()
    )
    return df

# ====== [ìˆ˜ì •] í† í”½ ë‹¨ì–´ ì€í–‰ (LogReg ê³„ìˆ˜ ê¸°ë°˜ + ê²½ê³  ë¡œì§) ======
def build_topic_term_bank_logreg(df_all: pd.DataFrame, 
                                 topn: int = 50,
                                 min_samples_warn: int = 50, 
                                 min_samples_block: int = 10) -> dict: 
    """
    [ìˆ˜ì •] Logistic Regression ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í”½ë³„ ë‹¨ì–´ ì€í–‰ êµ¬ì¶•
    - min_samples_block(10) ë¯¸ë§Œ: 'ìƒ˜í”Œ ì—†ìŒ' ì—ëŸ¬
    - min_samples_warn(50) ë¯¸ë§Œ: 'ì‹ ë¢°ë„ ë‚®ìŒ' ê²½ê³ ì™€ í•¨ê»˜ ë¶„ì„ ì‹¤í–‰
    """
    bank = {}
    
    if 'topic' not in df_all.columns:
        return bank
        
    unique_topics = sorted(df_all["topic"].unique())
    
    for t in unique_topics:
        df_topic = df_all[df_all["topic"] == t]
        df_train = df_topic[df_topic["quality_label"] != "medium"]
        
        # [ìˆ˜ì •] ìƒ˜í”Œ ìˆ˜ ì²´í¬ ë¡œì§ ë³€ê²½
        if len(df_train) < min_samples_block:
            bank[int(t)] = {
                "status": "error", 
                "message": f"ìƒ˜í”Œ ì™„ì „ ë¶€ì¡± (N={len(df_train)}, ìµœì†Œ {min_samples_block} í•„ìš”)"
            }
            continue 
            
        # [ì‹ ê·œ] ê²½ê³  ë©”ì‹œì§€ ì„¤ì •
        warning_msg = None
        if len(df_train) < min_samples_warn:
            warning_msg = f"ìƒ˜í”Œ ìˆ˜(N={len(df_train)})ê°€ ê¶Œì¥({min_samples_warn})ë³´ë‹¤ ì ì–´ í†µê³„ì  ì‹ ë¢°ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        texts = (df_train["title"].fillna("") + " " + df_train["content"].fillna("")).tolist()
        y = (df_train["quality_label"] == "good").astype(int).values
        
        try:
            # Good/Bad ë¼ë²¨ì´ ëª¨ë‘ ìˆì–´ì•¼ í•¨
            if len(np.unique(y)) < 2:
                bank[int(t)] = {"status": "error", "message": f"ë‹¨ì¼ ë¼ë²¨ë§Œ ì¡´ì¬ (N={len(df_train)})"}
                continue

            tfidf = TfidfVectorizer(ngram_range=(1,1), max_features=5000, min_df=3, stop_words=STOPWORDS_KO)
            X = tfidf.fit_transform(texts)
            
            clf = LogisticRegression(max_iter=1000, solver="liblinear", random_state=42, class_weight='balanced')
            clf.fit(X, y)
            
            if not hasattr(clf, "coef_"):
                bank[int(t)] = {"status": "error", "message": "ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨ (ê³„ìˆ˜ ì—†ìŒ)"}
                continue

            coefs = clf.coef_[0]
            vocab = np.array(tfidf.get_feature_names_out())
            order = np.argsort(coefs)
            
            good_terms = [(vocab[i], float(coefs[i])) for i in order[::-1] if coefs[i] > 0][:topn]
            bad_terms = [(vocab[i], float(coefs[i])) for i in order if coefs[i] < 0][:topn]
            
            cv_all = CountVectorizer(max_features=topn, min_df=3, stop_words=STOPWORDS_KO)
            X_all = cv_all.fit_transform(texts)
            counts = np.asarray(X_all.sum(axis=0)).ravel()
            vocab_all = np.array(cv_all.get_feature_names_out())
            order_all = np.argsort(counts)[::-1]
            all_terms = [(vocab_all[i], float(counts[i])) for i in order_all]

            bank[int(t)] = {
                "good": good_terms, 
                "bad": bad_terms, 
                "all": all_terms,
                "status": "ok",
                "message": f"ì„±ê³µ (N={len(df_train)})",
                "warning": warning_msg # [ì‹ ê·œ] ê²½ê³  ë©”ì‹œì§€ ì €ì¥
            }
        except Exception as e:
            bank[int(t)] = {
                "status": "error",
                "message": f"ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}"
            }
    return bank


# ====== Draft â†’ Topic ì¶”ë¡  ======
def infer_topic_for_text(txt: str,
                         vect: CountVectorizer,
                         lda_model: LatentDirichletAllocation) -> Tuple[int, np.ndarray]:
    Xd = vect.transform([txt if isinstance(txt, str) else ""])
    dist = lda_model.transform(Xd)[0]
    return int(dist.argmax()), dist

def get_topic_keywords_from_bank(bank: dict, topic_id: int, k_each: int = 30) -> Dict[str, List[Tuple[str, float]]]:
    """ì£¼ì œ IDì— í•´ë‹¹í•˜ëŠ” 'good'/'all' í‚¤ì›Œë“œë¥¼ (ë‹¨ì–´, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    if topic_id not in bank or bank[topic_id].get("status") != "ok": # [ìˆ˜ì •] status check
        return {"good": [], "all": []}
    
    goods = [(w, s) for w,s in bank[topic_id].get("good", [])[:k_each]]
    alls = [(w, s) for w,s in bank[topic_id].get("all", [])[:max(1, k_each//2)]]
    
    seen = set()
    unique_goods = []
    for w,s in goods:
        if w not in seen:
            unique_goods.append((w,s)); seen.add(w)
            
    unique_alls = []
    for w,s in alls:
        if w not in seen:
            unique_alls.append((w,s)); seen.add(w)

    return {"good": unique_goods, "all": unique_alls}


# ====== LLM Reranker/Generator ======
def llm_rerank_or_generate(
    draft_title: str, 
    draft_body: str,  
    candidates: List[str],
    topic_name: str, # [ì‹ ê·œ] ì£¼ì œ ì´ë¦„ ì¶”ê°€
    topk: int = 8,
    audience: str = "í˜¼í•©",
    tone: str = "ë¶„ì„ì ",
    temperature: float = 0.2,
    use_finetuned: bool = False, # [ì‹ ê·œ] í”Œë˜ê·¸
    ft_model_id: str = MODEL_CHAT # [ì‹ ê·œ] íŒŒì¸íŠœë‹ ëª¨ë¸ ID
) -> List[Dict]:
    if not USE_LLM or client is None or not LLM_OK:
        raise RuntimeError("APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: OPENAI_API_KEY/ë„¤íŠ¸ì›Œí¬/ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")

    if use_finetuned and ft_model_id.startswith("ft:"):
        # ===== 1. íŒŒì¸íŠœë‹ ëª¨ë¸ (ì œëª© ìƒì„±) ë¡œì§ =====
        system_prompt = "ë‹¹ì‹ ì€ ì œì‹œëœ ì£¼ì œì™€ ë³¸ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ë…ìì˜ ì°¸ì—¬ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” ì„±ê³¼í˜• ì œëª©ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤. ëª…ì‚¬í˜•ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."
        user_prompt = f"ì£¼ì œ: {topic_name}, ë³¸ë¬¸: {draft_body}"
        
        # ëª¨ë¸ í˜¸ì¶œ
        resp = client.chat.completions.create(
            model=ft_model_id, 
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            n=topk, 
            temperature=0.7 
        )

        recs = []
        for i, choice in enumerate(resp.choices, 1):
            title_text = choice.message.content.strip()
            if title_text:
                recs.append({
                    "term": title_text,
                    "category": f"AI ìƒì„± ì œëª© {i}",
                    "why": "íŒŒì¸íŠœë‹ ëª¨ë¸ì´ í•™ìŠµí•œ 'Good' ì½˜í…ì¸  íŒ¨í„´ ê¸°ë°˜.",
                    "where_to_add": "ì œëª©",
                    "insertion_example": f"AI ìƒì„± ì œëª©: {title_text}",
                    "expected_effect": "ê³¼ê±° ì„±ê³¼ ë°ì´í„°ë¥¼ ë°˜ì˜í•˜ì—¬ CTR/ì°¸ì—¬ìœ¨ ê·¹ëŒ€í™”.",
                    "cautions": "ì›ë³¸ ëª¨ë¸ì˜ ì°½ì˜ì„±ì´ ë°˜ì˜ë˜ì–´ ë¬¸ë§¥ì„ ì¬ê²€í† í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                })
        return recs

    else:
        # ===== 2. ê¸°ë³¸ LLM (ë¦¬ë­ì»¤) ë¡œì§ =====
        cand = [c.strip() for c in candidates if str(c).strip()]
        cand_unique = list(dict.fromkeys(cand))[:500]
        if not cand_unique:
            raise RuntimeError("í›„ë³´ ë‹¨ì–´ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (í†µê³„ ê¸°ë°˜ ì¶”ì²œ ë‹¨ì–´ ì—†ìŒ)")

        sys_prompt = (
            "ë„ˆëŠ” í•œêµ­ì–´ ì½˜í…ì¸  í¸ì§‘ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. ë°˜ë“œì‹œ JSON ê°ì²´ë§Œ ì¶œë ¥í•œë‹¤. "
            "ì´ˆì•ˆì€ {'title': '...', 'body': '...'} JSON ê°ì²´ë¡œ ì œê³µëœë‹¤. 'title'ê³¼ 'body'ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ë¶„ì„í•´ì•¼ í•œë‹¤. "
            "ê°ì²´ëŠ” {'items': [...]} í˜•ì‹ì´ë©°, ê° í•­ëª©ì€ "
            "{term, why, where_to_add, insertion_example, expected_effect, cautions} í‚¤ë¥¼ ê°€ì§„ë‹¤. "
            "where_to_addëŠ” ë°˜ë“œì‹œ ['ì œëª©', 'ë³¸ë¬¸'] ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•œë‹¤. ('ì†Œì œëª©', 'ì²« 120ì' ë“± ë‹¤ë¥¸ ê°’ì€ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€) "
            "ë°˜ë“œì‹œ 'í›„ë³´ í’€'ì— ìˆëŠ” ë‹¨ì–´ë§Œ ì‚¬ìš©."
            "insertion_exampleì€ ë°˜ë“œì‹œ ì´ˆì•ˆì˜ ì‹¤ì œ ë¬¸ì¥ì„ ì°¾ì•„ 'ê¸°ì¡´/ìˆ˜ì •' (Before/After) í˜•ì‹ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•´ì•¼ í•œë‹¤."
        )
        user_payload = {
            "goal": f"ì´ˆì•ˆ ë¬¸ë§¥ì„ ë³´ì¡´í•˜ë©° í›„ë³´ í’€ì—ì„œë§Œ Top-{topk} ì„ ë³„",
            "constraints": [
                "í›„ë³´ ë°– ë‹¨ì–´/ë™ì˜ì–´ ê¸ˆì§€",
                "ë¬¸ë§¥ ì–´ê¸‹ë‚˜ëŠ” ì‚½ì… ì˜ˆì‹œ ê¸ˆì§€",
                "ì¤‘ë³µ ì˜ë¯¸ ì¶”ì²œ ìµœì†Œí™”",
                "where_to_addëŠ” 'ì œëª©' ë˜ëŠ” 'ë³¸ë¬¸'ë§Œ í—ˆìš©. 'ì†Œì œëª©' ê¸ˆì§€.",
                f"ë…ììˆ˜ì¤€={audience}",
                f"í†¤={tone}"
            ],
            "candidates": cand_unique,
            "draft": {
                "title": draft_title,
                "body": draft_body[:6000] 
            },
            "return_format": [
                {"term":"...", "why":"'ì œëª©' ë˜ëŠ” 'ë³¸ë¬¸'ì˜ ë¬¸ë§¥ê³¼ ì—°ê´€ì§€ì–´ ì„¤ëª…", "where_to_add":"ì œëª©",
                 "insertion_example":"ì˜ˆ: 'ê¸°ì¡´: AI íŠ¸ë Œë“œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.' -> 'ìˆ˜ì •: [ì¶”ì²œë‹¨ì–´] AI íŠ¸ë Œë“œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.'", 
                 "expected_effect":"...", "cautions":"..."}
            ]
        }
        model_name = "gpt-4o-mini-2024-07-18"
        resp = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role":"system","content": sys_prompt},
                {"role":"user","content": json.dumps(user_payload, ensure_ascii=False)}
            ],
            temperature=temperature,
            response_format={"type": "json_object"},
        )
        
        raw = (resp.choices[0].message.content or "").strip()
        st.session_state["llm_meta_last"] = {
            "model": getattr(resp, "model", model_name),
            "id": getattr(resp, "id", ""),
            "usage": getattr(resp, "usage", None),
            "raw": raw,
            "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
        }
        data_obj = _parse_json_safely(raw)
        data = data_obj.get("items") if isinstance(data_obj, dict) else data_obj
        if not isinstance(data, list):
            raise ValueError("JSON í˜•ì‹ ì˜¤ë¥˜: ë°°ì—´(items)ì´ ì•„ë‹˜")
        
        allowed = set(cand_unique)
        recs = []
        for item in data:
            term = str(item.get("term","")).strip()
            where = str(item.get("where_to_add","")).strip()
            if not term or term not in allowed or where not in ['ì œëª©', 'ë³¸ë¬¸']: 
                continue
            recs.append({
                "term": term,
                "category": categorize_term(term),
                "why": str(item.get("why","")).strip(),
                "where_to_add": where,
                "insertion_example": str(item.get("insertion_example","")).strip(),
                "expected_effect": str(item.get("expected_effect","")).strip(),
                "cautions": str(item.get("cautions","")).strip(),
            })
            if len(recs) >= topk:
                break
        return recs


# ========= ì„±ê³¼ ê¸°ë°˜ ë¶„ì„ (RobustScaler) =========
def build_engagement(df: pd.DataFrame, w_views=0.4, w_likes=0.4, w_comments=0.2) -> pd.DataFrame:
    """
    RobustScaler(ì¤‘ì•™ê°’/IQR) ê¸°ë°˜ ì •ê·œí™”:
      x_rob = (x - median) / IQR
    â€» ë²”ìœ„ê°€ [0,1]ë¡œ ê³ ì •ë˜ì§€ ì•ŠìŒ(ì´ìƒì¹˜ ì˜í–¥ ê°ì†Œ ëª©ì ).
    """
    df = df.copy()
    cols = ["views_total", "likes", "comments"]
    for c in cols:
        if c not in df.columns:
            df[c] = 0
        df[c] = df[c].fillna(0)

    scaler = RobustScaler(quantile_range=(25.0, 75.0))
    for c in cols:
        df[c + "_rob"] = scaler.fit_transform(df[[c]]).ravel()

    df["engagement"] = (
        w_views   * df["views_total_rob"] +
        w_likes   * df["likes_rob"] +
        w_comments* df["comments_rob"]
    )
    return df

def label_quality_by_quantile(df: pd.DataFrame, col="engagement", low_q=0.33, high_q=0.66) -> pd.DataFrame:
    df = df.copy()
    q_low, q_high = df[col].quantile([low_q, high_q])
    def _label(x):
        if x >= q_high: return "good"
        if x <= q_low: return "bad"
        return "medium"
    df["quality_label"] = df[col].apply(_label)
    return df

# ========= LDA(ì˜¨ë¼ì¸) =========
def run_lda_topics_streaming(
    texts: List[str],
    n_topics: int = 10,
    max_features: int = 5000,
    batch_size: int = 1000,
    n_epochs: int = 3,
):
    vect = CountVectorizer(
        min_df=0.01,  # [ìˆ˜ì •] max_features ëŒ€ì‹  min_df/max_df ì‚¬ìš©
        max_df=0.90,  
        stop_words=STOPWORDS_KO 
    )
    X = vect.fit_transform([t if isinstance(t, str) else "" for t in texts])

    lda = LatentDirichletAllocation(
        n_components=n_topics, learning_method="online",
        batch_size=batch_size, max_iter=1, random_state=42, evaluate_every=0,
    )

    n_samples = X.shape[0]
    n_batches = int(np.ceil(n_samples / batch_size))
    total_steps = n_epochs * n_batches

    prog = st.progress(0.0, text="LDA ì£¼ì œ ë¶„ì„ í•™ìŠµ ì¤‘â€¦")
    t0 = time.time(); step = 0

    idx_all = np.arange(n_samples)
    for epoch in range(n_epochs):
        idx_all = sk_shuffle(idx_all, random_state=42 + epoch)
        for b in range(n_batches):
            bs = idx_all[b * batch_size : (b + 1) * batch_size]
            Xb = X[bs]
            lda.partial_fit(Xb)

            step += 1
            frac = step / total_steps
            elapsed = time.time() - t0
            sec_per_step = elapsed / max(step, 1)
            remain = sec_per_step * (total_steps - step)
            prog.progress(
                frac, text=f"LDA í•™ìŠµ {frac*100:.1f}% | ê²½ê³¼ {elapsed:,.0f}s | ë‚¨ìŒ ~{remain:,.0f}s"
            )

    W = lda.transform(X)        # ë¬¸ì„œ-í† í”½ ë¶„í¬
    prog.empty()
    df_topic = pd.DataFrame({"topic": W.argmax(axis=1)})
    return df_topic, vect, lda, W

# --------- n-gram ì¤‘ìš”ë„ ---------
def top_ngrams_by_logreg(texts: List[str], labels: List[str], ngram_range=(1,2), k: int = 20) -> Dict[str, List[Tuple[str, float]]]:
    y = np.array([1 if l=="good" else 0 for l in labels])
    tfidf = TfidfVectorizer(
        ngram_range=ngram_range, 
        max_features=30000, 
        min_df=2,
        stop_words=STOPWORDS_KO 
    )
    X = tfidf.fit_transform([t if isinstance(t,str) else "" for t in texts])
    clf = LogisticRegression(max_iter=1000, solver="liblinear")
    clf.fit(X, y)
    coefs = clf.coef_[0]
    idx_sorted = np.argsort(coefs)
    vocab = np.array(tfidf.get_feature_names_out())
    top_pos = [(vocab[i], float(coefs[i])) for i in idx_sorted[::-1][:k]]
    top_neg = [(vocab[i], float(coefs[i])) for i in idx_sorted[:k]]
    return {"good_terms": top_pos, "bad_terms": top_neg}

# --------- ë¶ˆìš©ì–´/ETA í•™ìŠµ ìœ í‹¸ ---------
def train_logreg_with_progress(texts, labels, stoplist=None, ngram_range=(1,2),
                               epochs=3, batch_size=2000, k_show=20, seed=42):
    y = np.array([1 if l=="good" else 0 for l in labels])
    
    final_stop_words = list(STOPWORDS_KO)
    if stoplist:
        if isinstance(stoplist, (set, tuple, list)):
            final_stop_words.extend(list(stoplist))
        else:
            try: final_stop_words.extend(list(stoplist))
            except Exception: pass
    final_stop_words = list(set(final_stop_words)) 

    tfidf = TfidfVectorizer(
        ngram_range=ngram_range, 
        min_df=5, 
        max_df=0.80, 
        stop_words=final_stop_words 
    )
    X = tfidf.fit_transform([t if isinstance(t,str) else "" for t in texts])

    n = X.shape[0]
    idx_all = np.arange(n)
    n_batches = int(np.ceil(n / batch_size))
    total_steps = epochs * n_batches

    clf = SGDClassifier(loss="log_loss", learning_rate="optimal",
                        alpha=1e-5, random_state=seed, warm_start=True)
    classes = np.array([0,1])

    prog = st.progress(0.0, text="ì½˜í…ì¸  ë“±ê¸‰ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì¤‘â€¦")
    t0 = time.time(); step = 0

    for ep in range(epochs):
        idx_all = sk_shuffle(idx_all, random_state=seed + ep)
        for b in range(n_batches):
            bs = idx_all[b*batch_size : (b+1)*batch_size]
            Xb = X[bs]; yb = y[bs]
            clf.partial_fit(Xb, yb, classes=classes)

            step += 1
            frac = step/total_steps
            elapsed = time.time() - t0
            sec_per = elapsed / max(step,1)
            remain = sec_per * (total_steps - step)
            prog.progress(frac, text=f"ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ {frac*100:.1f}% | ê²½ê³¼ {elapsed:,.0f}s | ë‚¨ìŒ ~{remain:,.0f}s")

    prog.empty()

    coefs = clf.coef_[0]
    vocab = np.array(tfidf.get_feature_names_out())
    order = np.argsort(coefs)
    good_terms = [(vocab[i], float(coefs[i])) for i in order[::-1][:k_show]]
    bad_terms  = [(vocab[i], float(coefs[i]))  for i in order[:k_show]]

    return {
        "tfidf": tfidf, "clf": clf,
        "good_terms": good_terms, "bad_terms": bad_terms,
        "last_eta_info": {"elapsed": time.time()-t0}
    }

# === í† í”½ ìƒìœ„ë‹¨ì–´ & ë¼ë²¨ ===
def get_topic_top_words(lda, vect, topn=8):
    vocab = np.array(vect.get_feature_names_out())
    topics = {}
    for k, comp in enumerate(lda.components_):
        idx = np.argsort(comp)[::-1][:topn]
        topics[f"Topic {k}"] = [str(vocab[i]) for i in idx]
    return topics

def _heuristic_topic_name(words: list[str]) -> dict:
    w = " ".join(words)
    rules = [
        (["ì •ë¶€","êµ­íšŒ","ì˜ˆì‚°","ì •ì±…","ëŒ€í†µë ¹"], ("ì •ì¹˜/í–‰ì •", "ì •ë¶€Â·êµ­íšŒÂ·ì˜ˆì‚° ê´€ë ¨ ì´ìŠˆ")),
        (["ì†í¥ë¯¼","ë¦¬ê·¸","ê²½ê¸°","ê³¨","ì„ ìˆ˜","ìŠ¤í¬ì¸ "], ("ìŠ¤í¬ì¸ /ì¶•êµ¬", "ê²½ê¸°/ì„ ìˆ˜/ë¦¬ê·¸ ì¤‘ì‹¬ ê¸°ì‚¬")),
        (["AI","ì¸ê³µì§€ëŠ¥","ë¡œë´‡","ê¸°ìˆ ","ì‚°ì—…","ìë™í™”","ë°ì´í„°"], ("ê¸°ìˆ /AI", "AIÂ·ë¡œë´‡Â·ì‚°ì—… ìë™í™”")),
        (["ì£¼ì‹","í™˜ìœ¨","ë¶€ë™ì‚°","ê¸ˆë¦¬","ê²½ì œ"], ("ê²½ì œ/ê¸ˆìœµ", "ê±°ì‹œê²½ì œÂ·ì‹œì¥ ë™í–¥")),
        (["ì½”ë¡œë‚˜","ì˜ë£Œ","ê±´ê°•","ë³‘ì›"], ("ì˜ë£Œ/ê±´ê°•", "ì§ˆë³‘Â·ì˜ë£ŒÂ·í—¬ìŠ¤ì¼€ì–´")),
        (["ë„·í”Œë¦­ìŠ¤","ì˜í™”","ë“œë¼ë§ˆ","ì½˜í…ì¸ "], ("ë¬¸í™”/ì½˜í…ì¸ ", "ì˜í™”Â·ë°©ì†¡Â·í”Œë«í¼")),
    ]
    for keys, (nm, desc) in rules:
        if any(k in w for k in keys):
            return {"name": nm, "desc": desc}
    return {"name": "ì¼ë°˜/ì¢…í•©", "desc": "ê´‘ë²”ìœ„í•œ ì´ìŠˆ ë¬¶ìŒ"}

def llm_name_topics(topic_top_words: dict, model_name=MODEL_CHAT):
    if not USE_LLM or client is None or not LLM_OK:
        return {k: _heuristic_topic_name(v) for k, v in topic_top_words.items()}

    payload = {
        "topics": topic_top_words,
        "schema": {"Topic k": {"name": "ì§§ì€ ì´ë¦„", "desc": "í•œ ì¤„ ì„¤ëª…"}},
        "instruction": "ìœ„ 'topics'ì˜ ìƒìœ„ ë‹¨ì–´ë¥¼ ë³´ê³  ê° í† í”½ì— ëŒ€í•´ {name, desc}ë¥¼ ìƒì„±. "
                       "JSON ê°ì²´ë¡œ { 'Topic 0': {'name':'..','desc':'..'}, ... } í˜•ì‹ë§Œ ë°˜í™˜. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€."
    }
    try:
        r = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role":"system","content":"ë„ˆëŠ” ì£¼ì œ ë¼ë²¨ëŸ¬ë‹¤. JSON ê°ì²´ë§Œ ë°˜í™˜í•œë‹¤."},
                {"role":"user","content": json.dumps(payload, ensure_ascii=False)}
            ],
            temperature=0.2,
            response_format={"type": "json_object"},
        )
        txt = (r.choices[0].message.content or "").strip()
        data = _parse_json_safely(txt)
        if not isinstance(data, dict) or not data:
            raise ValueError("ë¹ˆ JSON")
        for k, words in topic_top_words.items():
            if k not in data or "name" not in data[k]:
                data[k] = _heuristic_topic_name(words)
        return data
    except Exception:
        return {k: _heuristic_topic_name(v) for k, v in topic_top_words.items()}

# [ìˆ˜ì •] ê°ì„± S/I ê³„ì‚° í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½ (cv, lexë¥¼ ì¸ìë¡œ ë°›ë„ë¡)
def compute_sentiment_SI(df_work: pd.DataFrame, cv: CountVectorizer, lex: dict) -> pd.DataFrame:
    """ê°„ë‹¨ í† í° ê¸°ì¤€ í‰ê· ê°ì„± S, í‰ê· ì ˆëŒ€ê°ì„± I (CV, Lexicon ì™¸ë¶€ ì£¼ì…)"""
    df = df_work.copy()
    texts = (df["title"].fillna("") + " " + df["content"].fillna("")).tolist()

    X = cv.transform(texts) 
    vocab = np.array(cv.get_feature_names_out())

    rows, cols = X.nonzero()
    tok_by_row: Dict[int, List[str]] = {}
    for r, c in zip(rows, cols):
        tok_by_row.setdefault(r, []).append(vocab[c])

    S, I = [], []
    for r in range(X.shape[0]):
        vals = [lex.get(t, 0.0) for t in tok_by_row.get(r, [])]
        if vals:
            S.append(float(np.mean(vals)))
            I.append(float(np.mean(np.abs(vals))))
        else:
            S.append(0.0); I.append(0.0)
    df["S"], df["I"] = S, I
    return df

# [ìˆ˜ì •] TAB1ì—ì„œ ì´ˆì•ˆ í…ìŠ¤íŠ¸ì˜ ê°ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ í—¬í¼ í•¨ìˆ˜
def get_sentiment_for_text(txt: str, senti_pack: dict) -> Tuple[float, float]:
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•´ S/I ì ìˆ˜ ê³„ì‚°"""
    if not senti_pack or not senti_pack.get('cv') or not senti_pack.get('lex') or not txt:
        return 0.0, 0.0
    
    try:
        cv = senti_pack['cv']
        lex = senti_pack['lex']
        
        X = cv.transform([txt])
        vocab = np.array(cv.get_feature_names_out())
        
        rows, cols = X.nonzero()
        if not np.any(cols): 
            return 0.0, 0.0
            
        vals = [lex.get(vocab[c], 0.0) for c in cols]
        if vals:
            s = float(np.mean(vals))
            i = float(np.mean(np.abs(vals)))
            return s, i
    except Exception:
        return 0.0, 0.0
    return 0.0, 0.0

# [ì‹ ê·œ] TAB1ì—ì„œ ìµœê·¼ 30ì¼ ì¸ê¸° ë‹¨ì–´ë¥¼ ë™ì ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def get_recent_popular_words(df_all_data: pd.DataFrame, 
                             end_date: datetime.date, 
                             topic_id: int = None, 
                             k: int = 10) -> List[str]:
    """íŠ¹ì • í† í”½/ê¸°ê°„/Goodë“±ê¸‰ ë¬¸ì„œì—ì„œ Top-K ë¹ˆë„ ë‹¨ì–´ ì¶”ì¶œ"""
    if df_all_data is None or df_all_data.empty or 'date' not in df_all_data.columns or 'topic' not in df_all_data.columns or 'quality_label' not in df_all_data.columns:
        return []
    
    try:
        df = df_all_data.copy()
        if not pd.api.types.is_datetime64_any_dtype(df['date']):
             df['date'] = pd.to_datetime(df['date'], errors='coerce')
        
        # ë‚ ì§œ í•„í„°ë§
        end_date_pd = pd.to_datetime(end_date)
        start_date_pd = end_date_pd - pd.Timedelta(days=30)
        
        df_filtered = df[
            (df['date'] >= start_date_pd) & 
            (df['date'] <= end_date_pd) &
            (df['quality_label'] == 'good')
        ]
        
        if topic_id is not None:
            df_filtered = df_filtered[df_filtered['topic'] == topic_id]
        
        if df_filtered.empty:
            return []
        
        texts = (df_filtered["title"].fillna("") + " " + df_filtered["content"].fillna("")).tolist()
        
        cv_recent = CountVectorizer(max_features=2000, stop_words=STOPWORDS_KO)
        X_recent = cv_recent.fit_transform(texts)
        
        word_counts = X_recent.sum(axis=0)
        words_freq = [(word, word_counts[0, idx]) for word, idx in cv_recent.vocabulary_.items()]
        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
        
        return [word for word, freq in words_freq[:k]]
        
    except Exception as e:
        st.error(f"[ìµœê·¼ ë‹¨ì–´ ë¶„ì„ ì˜¤ë¥˜] {e}. (ë°ì´í„°ì— 'topic' ë˜ëŠ” 'date' ì»¬ëŸ¼ì´ ì˜¬ë°”ë¥´ê²Œ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.)")
        return []


# ========= íšŒê·€ ìœ í‹¸ =========
def fit_ols(y, X):
    X = X.apply(pd.to_numeric, errors='coerce').fillna(0)
    y = y.apply(pd.to_numeric, errors='coerce').fillna(0)
    
    valid_idx = (y != 0) | (X != 0).any(axis=1)
    y_valid = y[valid_idx]
    X_valid = X[valid_idx]

    if len(y_valid) < 2:
        raise ValueError("ìœ íš¨í•œ ë°ì´í„° í¬ì¸íŠ¸ê°€ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. íšŒê·€ ë¶„ì„ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    Xc = sm.add_constant(X_valid, has_constant="add")
    model = sm.OLS(y_valid.astype(float), Xc, missing="drop")
    return model.fit()

def tidy_summary(res: sm.regression.linear_model.RegressionResultsWrapper, max_rows=200):
    s = []
    for name, coef, se, t, p in zip(res.params.index, res.params.values, res.bse.values, res.tvalues, res.pvalues):
        s.append({"term": name, "coef": float(coef), "se": float(se), "t": float(t), "p": float(p)})
    df = pd.DataFrame(s)
    if len(df) > max_rows:
        return df.head(max_rows)
    return df

# [ìˆ˜ì •] TAB4 'ë¶ˆìš©ì–´ ì˜ì‹¬ ë‹¨ì–´' ë¡œì§ ë³€ê²½: 'Bad'ê°€ ì•„ë‹Œ 'Generic' ë‹¨ì–´ ì¶”ì¶œ
def get_suspected_stopwords(df_all_data: pd.DataFrame, k: int = 50) -> List[str]:
    """í† í”½/ì„±ê³¼ì™€ ë¬´ê´€í•˜ê²Œ ê°€ì¥ ìì£¼ ì“°ì´ëŠ” ì¼ë°˜ ë‹¨ì–´(ë¶ˆìš©ì–´ í›„ë³´) ì¶”ì¶œ"""
    if df_all_data is None or df_all_data.empty:
        return []
    try:
        texts = (df_all_data["title"].fillna("") + " " + df_all_data["content"].fillna("")).tolist()
        
        cv_nostop = CountVectorizer(max_features=k, 
                                    min_df=0.1, 
                                    ngram_range=(1,1)) 
        cv_nostop.fit(texts)
        common_words = cv_nostop.get_feature_names_out()
        
        final_suspects = [w for w in common_words if w not in STOPWORDS_KO]
        return final_suspects
    except Exception as e:
        st.error(f"[ë¶ˆìš©ì–´ ì˜ì‹¬ ë‹¨ì–´ ì¶”ì¶œ ì˜¤ë¥˜] {e}")
        return []

# [ì‹ ê·œ] TAB1 ê°ì„± ê²Œì´ì§€ ì°¨íŠ¸ ìƒì„± í•¨ìˆ˜ (S/I ë¶„ë¦¬)
def create_sentiment_gauge_S(s_val: float, s_target: float, lexicon_max: float = 1.0):
    """Plotlyì˜ Indicatorë¥¼ ì‚¬ìš©í•´ ê°ì„± ì ìˆ˜(S) ê²Œì´ì§€ ìƒì„±"""
    fig = go.Figure()
    fig.add_trace(go.Indicator(
        mode = "gauge+number",
        value = s_val,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "ğŸ’– ê°ì„± ì ìˆ˜ (S)", 'font': {'size': 18}},
        number = {'font': {'size': 24}},
        gauge = {
            'axis': {'range': [-lexicon_max, lexicon_max], 'tickwidth': 1, 'tickcolor': "darkblue"},
            'bar': {'color': "#0072F0" if s_val >= 0 else "#E63946", 'thickness': 0.4},
            'bgcolor': "white",
            'borderwidth': 2,
            'bordercolor': "#CCCCCC",
            'steps': [
                {'range': [-lexicon_max, -0.05], 'color': 'rgba(230, 57, 70, 0.1)'}, 
                {'range': [-0.05, 0.05], 'color': 'rgba(200, 200, 200, 0.2)'}, 
                {'range': [0.05, lexicon_max], 'color': 'rgba(0, 114, 240, 0.1)'}
            ],
            'threshold': {
                'line': {'color': "green", 'width': 3},
                'thickness': 0.75,
                'value': s_target
            }
        }
    ))
    fig.update_layout(height=180, margin=dict(l=20, r=20, t=40, b=10))
    return fig

def create_sentiment_gauge_I(i_val: float, i_target: float, lexicon_max: float = 1.0):
    """Plotlyì˜ Indicatorë¥¼ ì‚¬ìš©í•´ ê°ì„± ê°•ë„(I) ê²Œì´ì§€ ìƒì„±"""
    fig = go.Figure()
    fig.add_trace(go.Indicator(
        mode = "gauge+number",
        value = i_val,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "ğŸ’– ê°ì„± ê°•ë„ (I)", 'font': {'size': 18}},
        number = {'font': {'size': 24}},
        gauge = {
            'axis': {'range': [0, lexicon_max], 'tickwidth': 1, 'tickcolor': "darkblue"},
            'bar': {'color': "#F4A261", 'thickness': 0.4},
            'bgcolor': "white",
            'borderwidth': 2,
            'bordercolor': "#CCCCCC",
            'steps': [
                {'range': [0, lexicon_max * 0.33], 'color': 'rgba(200, 200, 200, 0.2)'},
                {'range': [lexicon_max * 0.33, lexicon_max * 0.66], 'color': 'rgba(244, 162, 97, 0.1)'},
                {'range': [lexicon_max * 0.66, lexicon_max], 'color': 'rgba(244, 162, 97, 0.2)'},
            ],
            'threshold': {
                'line': {'color': "green", 'width': 3},
                'thickness': 0.75,
                'value': i_target
            }
        }
    ))
    fig.update_layout(height=180, margin=dict(l=20, r=20, t=40, b=10))
    return fig


# ==================== (â˜…) MODE_CFG [ìˆ˜ì •] ====================
# [ìˆ˜ì •] max_featuresë¥¼ ì œê±° (min_df/max_df ì‚¬ìš©ìœ¼ë¡œ ëŒ€ì²´)
MODE_CFG = {
    "quick": {
        "sample_n": 5000,
        "lda_topics": 0,
        # "max_features": 3000, # [ìˆ˜ì •] ì œê±°
        "batch_size": 500,
        "n_epochs": 2,
        "clf_epochs": 1,
        "clf_batch": 500,
        "ngram_range": (1, 2),
    },
    "full": {
        "sample_n": None,
        "lda_topics": 0,
        # "max_features": 5000, # [ìˆ˜ì •] ì œê±°
        "batch_size": 1000,
        "n_epochs": 3,
        "clf_epochs": 3,
        "clf_batch": 1000,
        "ngram_range": (1, 3),
    },
}
# ==================================================================

# ========= Streamlit UI =========
st.set_page_config(page_title="ë¬¸ë§¥í˜• ì¶”ì²œ + ì„±ê³¼ ë¶„ì„ + ê°ì„±/íšŒê·€", page_icon="ğŸ“", layout="wide")
st.title("ğŸ“ ë¬¸ë§¥í˜• ìš©ì–´ ì¶”ì²œ + ğŸ“ˆ ì„±ê³¼ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

with st.sidebar:
    st.subheader("ê³µí†µ ì„¤ì •")
    audience = st.selectbox("ì£¼ìš” ë…ì ìˆ˜ì¤€", ["ì…ë¬¸ì", "ì „ë¬¸ê°€", "í˜¼í•©"], index=2)
    tone = st.selectbox("ì½˜í…ì¸  í†¤/ìŠ¤íƒ€ì¼", ["ì¹œê·¼", "ê³µì‹", "ë¶„ì„ì "], index=2)
    if LLM_OK: st.success("LLM ìƒíƒœ: âœ… ì—°ê²° OK")
    elif USE_LLM: st.error("LLM ìƒíƒœ: âŒ ì¸ì¦/ê¶Œí•œ/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜")
    else: st.error("LLM ìƒíƒœ: âŒ OPENAI_API_KEY ë¯¸ì„¤ì •")

def require_llm():
    if not LLM_OK:
        st.error("APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: OPENAI_API_KEY/ë„¤íŠ¸ì›Œí¬/ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

# [ìˆ˜ì •] TAB3 (íŒŒì¸íŠœë‹) ì œê±°, TAB4 -> TAB3ìœ¼ë¡œ ë³€ê²½
TAB1, TAB2, TAB3 = st.tabs([
    "ğŸ’¡ ë¬¸ë§¥í˜• ìš©ì–´ ì¶”ì²œ", 
    "ğŸ“ˆ ì„±ê³¼/ì£¼ì œ/ê°ì„± ë¶„ì„", 
    "ğŸ”¬ ëª¨ë¸ ê´€ë¦¬ì (Admin)" # êµ¬ TAB4
])

# ================= TAB1 =================
with TAB1:
    st.header("1) ì´ˆì•ˆ í…ìŠ¤íŠ¸ ì…ë ¥")
    
    draft_title = st.text_input("ì œëª© (ì„ íƒ)", placeholder="ì˜ˆ: ì´ë²ˆ ì£¼ AI íŠ¸ë Œë“œ Top 5")
    draft_body = st.text_area("ë³¸ë¬¸ (ì´ˆì•ˆ)", height=220,
                         placeholder="ì˜ˆ) 1. ì˜¤í”ˆAIì˜ ìƒˆ ëª¨ë¸ì´...")
    
    full_draft = draft_title.strip() + " " + draft_body.strip()

    c_date, c_check = st.columns([1, 1])
    with c_date:
        ref_date = st.date_input("ê¸°ì¤€ ë‚ ì§œ", datetime.date.today())
    with c_check:
        st.write("") 
        st.write("")
        all_dates = st.checkbox("ëª¨ë“  ë‚ ì§œ ì„ íƒí•˜ê¸° (ì „ì²´ ê¸°ê°„ ë¶„ì„)", value=True) 

    # [ìˆ˜ì •] 'LLM í† í° ë³´ê¸°' ì œê±°

    for _k, _v in [
        ("last_recs", None),
        ("last_recs_time", None),
        ("last_draft", ""),
        ("last_candidates", []),
        ("sentiment_pack", None), 
        ("df_for_analysis", None),
        ("analysis_done", False), 
        ("ft_model_id", "ft:gpt-4o-mini-2024-07-18:::CWPoHwfK"), # [ì‹ ê·œ] íŒŒì¸íŠœë‹ ID ì €ì¥
    ]:
        st.session_state.setdefault(_k, _v)

    candidates = list(DEFAULT_CANDIDATES)
    topic_id_for_draft, topic_dist = None, None
    topic_name = "ë¯¸ë¶„ë¥˜" 

    topic_bank = st.session_state.get("topic_term_bank")
    lda_vect   = st.session_state.get("lda_vect")
    lda_model  = st.session_state.get("lda_model")
    clf_pack   = st.session_state.get("clf_pack")
    senti_pack = st.session_state.get('sentiment_pack')
    df_all_data = st.session_state.get('df_for_analysis') 
    DUMMY_ID = "ft:gpt-4o-mini-DUMMY_ID_INIT"
    # [ìˆ˜ì •] íŒŒì¸íŠœë‹ ëª¨ë¸ IDëŠ” ì„¸ì…˜ì—ì„œ ê°€ì ¸ì˜´
    FINETUNED_MODEL_ID = st.session_state.get('ft_model_id', DUMMY_ID) 
    # ëª¨ë¸ IDê°€ ì„¤ì •ë˜ì—ˆê³ , 'ft:'ë¡œ ì‹œì‘í•˜ë©´ íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©
    is_ft_model_ready = FINETUNED_MODEL_ID.startswith("ft:") and FINETUNED_MODEL_ID != DUMMY_ID

    # [ì‹ ê·œ] ë°ì´í„° ë¯¸ë¡œë“œ ì‹œ ê²½ê³ 
    if not st.session_state['analysis_done']:
        st.warning("âš ï¸ **ë°ì´í„° ë¯¸ë¡œë“œ:** TAB2ì—ì„œ CSV ì—…ë¡œë“œ ë° ë¶„ì„ì„ ì‹¤í–‰í•˜ë©´ ê³¼ê±° ë°ì´í„° ê¸°ë°˜ì˜ í™•ë¥ , ì£¼ì œ, í‚¤ì›Œë“œ ì¶”ì²œì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

    
    # --- 1. í† í”½ ì¶”ë¡  ë° íƒœê·¸ í‘œì‹œ ---
    if full_draft.strip() and topic_bank and lda_vect is not None and lda_model is not None:
        topic_id_for_draft, topic_dist = infer_topic_for_text(full_draft, lda_vect, lda_model)
        
        topic_name = f"í† í”½ {topic_id_for_draft}"
        topic_desc = "ë¶„ì„ëœ ì£¼ì œ"
        lbls = st.session_state.get("topic_labels", {})
        if f"Topic {topic_id_for_draft}" in lbls:
            meta = lbls[f"Topic {topic_id_for_draft}"]
            topic_name = meta.get('name', topic_name)
            topic_desc = meta.get('desc', topic_desc)

        st.markdown(f"**ì´ˆì•ˆì˜ ì˜ˆìƒ ì£¼ì œ:** <span style='background-color: #0072F0; color: white; padding: 3px 8px; border-radius: 15px; font-size: 0.9em; margin-left: 10px;'>{topic_name}</span>", unsafe_allow_html=True)
        st.caption(f"â”” {topic_desc} (í›„ë³´ ë‹¨ì–´ {len(candidates)}ê°œ)")
    
    # --- 2. ë“±ê¸‰/í™•ë¥  ë° í‚¤ì›Œë“œ ì¶”ì²œ (ë¶„ë¥˜ê¸° ë¡œë“œ ì‹œ) ---
    if full_draft.strip() and clf_pack is not None:
        tfidf = clf_pack["tfidf"]; clf = clf_pack["clf"]
        Xd = tfidf.transform([full_draft])
        proba_good = float(clf.predict_proba(Xd)[0,1])
        label = "ìƒ (Good)" if proba_good >= 0.5 else "í•˜ (Bad)"
        
        c1, c2 = st.columns(2)
        c1.metric("ì˜ˆìƒ ì½˜í…ì¸  ë§¤ë ¥ ë“±ê¸‰", label)
        c2.metric("ğŸ“ˆ ê³¼ê±° ë°ì´í„° ê¸°ë°˜ ì„±ê³¼ í™•ë¥ ", f"{proba_good*100:.1f}%")
        
        st.caption("â”” ê³¼ê±° ë°ì´í„°(TAB2)ë¡œ í•™ìŠµí•œ í†µê³„ ëª¨ë¸ì˜ ì˜ˆì¸¡ì¹˜ì…ë‹ˆë‹¤. LLM ì¶”ì²œ(ë¬¸ë§¥/í’ˆì§ˆ ì¤‘ì‹¬)ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # í‚¤ì›Œë“œ ì¶”ì²œ ì„¹ì…˜ (íŒŒì¸íŠœë‹ ëª¨ë“œì—ì„œëŠ” ìˆ¨ê¹€)
        if not is_ft_model_ready:
            # [ìˆ˜ì •] 'ëª¨ë“  ë‚ ì§œ' ì²´í¬ë°•ìŠ¤ì— ë”°ë¼ ë™ì  í‚¤ì›Œë“œ ì¶”ì²œ
            if all_dates:
                # --- "ëª¨ë“  ë‚ ì§œ" ì„ íƒ ì‹œ (ê¸°ì¡´ ë¡œì§) ---
                if topic_id_for_draft is not None and topic_bank:
                    topic_keywords_data = get_topic_keywords_from_bank(topic_bank, int(topic_id_for_draft), k_each=10)
                    good_topic_terms = [w for w,s in topic_keywords_data.get("good", [])]
                    if good_topic_terms:
                        with st.expander(f"âœ… **'{topic_name}' ì£¼ì œ**ì˜ **ì „ì²´ ê¸°ê°„** ì„±ê³¼ ìš°ìˆ˜ ë‹¨ì–´ (ì¶”ì²œ)"):
                            st.markdown(f"**ì´ìœ :** ê³¼ê±° ì´ ì£¼ì œ(`{topic_name}`)ì˜ ì½˜í…ì¸  ì¤‘ **ë†’ì€ ì„±ê³¼**ë¥¼ ë‚¸ ë¬¸ì„œì—ì„œ ìì£¼ ë°œê²¬ëœ ë‹¨ì–´ë“¤ì…ë‹ˆë‹¤.")
                            st.info(", ".join(good_topic_terms))
                        candidates = list(dict.fromkeys(good_topic_terms + candidates)) 

                good_terms_list = clf_pack.get("good_terms", [])
                top_good = [t for t,_ in good_terms_list]
                missing  = [t for t in top_good if t not in full_draft][:8]
                if missing:
                    st.info(f"ğŸ’¡ **ì „ì²´ ê¸°ê°„** ì„±ê³¼ ìš°ìˆ˜ ë‹¨ì–´ (ì´ˆì•ˆì— ì—†ìœ¼ë©´ ì¶”ê°€ ê³ ë ¤): \n\n" + ", ".join(missing))
                    st.caption("â”” ì´ìœ : ì£¼ì œì™€ ê´€ê³„ì—†ì´ ì „ë°˜ì ìœ¼ë¡œ ë†’ì€ ì„±ê³¼ë¥¼ ë‚¸ ì½˜í…ì¸ ì˜ ê³µí†µ í‚¤ì›Œë“œì…ë‹ˆë‹¤.")
                    candidates = list(dict.fromkeys(list(missing) + candidates))
            
            else:
                # --- "íŠ¹ì • ë‚ ì§œ" ì„ íƒ ì‹œ (ìµœê·¼ 30ì¼) ---
                if topic_id_for_draft is not None and (df_all_data is not None and not df_all_data.empty):
                    # 1. ì£¼ì œë³„ ìµœê·¼ ë‹¨ì–´
                    recent_topic_words = get_recent_popular_words(df_all_data, ref_date, topic_id=topic_id_for_draft, k=10)
                    if recent_topic_words:
                        with st.expander(f"ğŸ“ˆ **'{topic_name}' ì£¼ì œ**ì˜ **ìµœê·¼ 30ì¼** ì¸ê¸° ë‹¨ì–´ (Good ì½˜í…ì¸ )"):
                            st.markdown(f"**ê¸°ì¤€:** `{ref_date - datetime.timedelta(days=30)}` ~ `{ref_date}` ê¸°ê°„ ë™ì•ˆ ì„±ê³¼ê°€ ì¢‹ì•˜ë˜ ë¬¸ì„œ ê¸°ì¤€")
                            st.warning(", ".join(recent_topic_words))
                        candidates = list(dict.fromkeys(recent_topic_words + candidates))
                    
                    # 2. ì „ì²´ ìµœê·¼ ë‹¨ì–´
                    recent_all_words = get_recent_popular_words(df_all_data, ref_date, topic_id=None, k=10)
                    missing_recent = [w for w in recent_all_words if w not in full_draft][:8]
                    if missing_recent:
                        st.info(f"ğŸ“ˆ **ì „ì²´ ì£¼ì œ**ì˜ **ìµœê·¼ 30ì¼** ì¸ê¸° ë‹¨ì–´ (ì´ˆì•ˆì— ì—†ìœ¼ë©´ ì¶”ê°€ ê³ ë ¤): \n\n" + ", ".join(missing_recent))
                        st.caption(f"â”” ì´ìœ : `{ref_date}` ê¸°ì¤€ ìµœê·¼ 30ì¼ê°„ ì„±ê³¼ê°€ ì¢‹ì•˜ë˜ ëª¨ë“  ì½˜í…ì¸ ì˜ ê³µí†µ í‚¤ì›Œë“œì…ë‹ˆë‹¤.")
                        candidates = list(dict.fromkeys(list(missing_recent) + candidates))
                    
                    if not recent_topic_words and not missing_recent:
                        st.caption(f"â„¹ï¸ `{ref_date}` ê¸°ì¤€ ìµœê·¼ 30ì¼ê°„ì˜ ì¸ê¸° ë‹¨ì–´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                elif df_all_data is None or df_all_data.empty:
                    st.caption("â„¹ï¸ TAB2ì—ì„œ ë¶„ì„ì„ ì‹¤í–‰í•˜ë©´ 'ìµœê·¼ 30ì¼ ì¸ê¸° ë‹¨ì–´'ê°€ í™œì„±í™”ë©ë‹ˆë‹¤.")

        st.divider()
        
        # --- 3. ê°ì„± ì ìˆ˜ (ê°ì„± ì‚¬ì „ ë¡œë“œ ì‹œ) ---
        if senti_pack and senti_pack.get('cv') and senti_pack.get('lex'):
             senti_s, senti_i = get_sentiment_for_text(full_draft, senti_pack)
             target_s = senti_pack.get('target_s')
             target_i = senti_pack.get('target_i')

             # [ì‹ ê·œ] ê²Œì´ì§€ ì°¨íŠ¸ë¥¼ 2ì—´ë¡œ ê°•ì œ ë¶„ë¦¬
             col1, col2 = st.columns(2)
             with col1:
                 st.plotly_chart(create_sentiment_gauge_S(senti_s, target_s), use_container_width=True)
             with col2:
                 st.plotly_chart(create_sentiment_gauge_I(senti_i, target_i), use_container_width=True)
             
             # [ì‹ ê·œ] ìº¡ì…˜ì— ëª©í‘œ ì ìˆ˜ í…ìŠ¤íŠ¸ ì¶”ê°€
             if target_s is not None and target_i is not None:
                st.markdown(f"**ğŸ¯ ëª©í‘œ ì ìˆ˜** (Good ì½˜í…ì¸  í‰ê· ): **S (ì ìˆ˜): {target_s:.2f}** | **I (ê°•ë„): {target_i:.2f}**")
             
             st.caption("""
             * **ê°ì„± ì ìˆ˜ (S):** ê¸ì •(>0) ë˜ëŠ” ë¶€ì •(<0)ì˜ ì •ë„. 0ì€ ì¤‘ë¦½. (ë²”ìœ„: -1 ~ 1)
             * **ê°ì„± ê°•ë„ (I):** ê°ì„±ì´ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ í‘œí˜„ë˜ì—ˆëŠ”ì§€. 0ì€ ê°ì„± ë‹¨ì–´ ì—†ìŒ. (ë²”ìœ„: 0 ~ 1)
             * **ì´ˆë¡ìƒ‰ ì„ :** ì„±ê³¼ê°€ ì¢‹ì•˜ë˜(Good) ê¸€ë“¤ì˜ í‰ê·  'ëª©í‘œ' ì ìˆ˜ì…ë‹ˆë‹¤.
             * **ê°€ì •:** ì´ ì°¨íŠ¸ëŠ” ì‚¬ìš©ëœ ê°ì„± ì‚¬ì „ì˜ ì ìˆ˜ê°€ -1ì—ì„œ +1 ì‚¬ì´ì„ì„ ê°€ì •í•©ë‹ˆë‹¤.
             """)
             
             # [ì‹ ê·œ] ë™ì  ê°ì„± ì¶”ì²œ
             if target_s is not None and target_i is not None:
                recs = []
                if abs(senti_s - target_s) > 0.05:
                    if senti_s < target_s:
                        recs.append(f"ì„±ê³¼ ì¢‹ì€ ê¸€(ëª©í‘œ {target_s:.2f})ì€ **ë” ê¸ì •ì **ì…ë‹ˆë‹¤. (ê¸ì • ë‹¨ì–´ ì¶”ê°€)")
                    else:
                        recs.append(f"ì„±ê³¼ ì¢‹ì€ ê¸€(ëª©í‘œ {target_s:.2f})ì€ **ë” ì°¨ë¶„(ì¤‘ë¦½/ë¶€ì •)**í•©ë‹ˆë‹¤. (ê¸ì • ë‹¨ì–´ ê°ì†Œ)")
                if abs(senti_i - target_i) > 0.1:
                    if senti_i < target_i:
                        recs.append(f"ì„±ê³¼ ì¢‹ì€ ê¸€(ëª©í‘œ {target_i:.2f})ì€ **ê°ì„± í‘œí˜„ì´ ë” ê°•í•©ë‹ˆë‹¤.** (ê°ì„± ë‹¨ì–´ ì¶”ê°€)")
                    else:
                        recs.append(f"ì„±ê³¼ ì¢‹ì€ ê¸€(ëª©í‘œ {target_i:.2f})ì€ **ë” ê°ê´€ì (ê°ì„± ê°•ë„ ë‚®ìŒ)**ì…ë‹ˆë‹¤. (ê°ì„± ë‹¨ì–´ ê°ì†Œ)")
                
                if recs:
                    st.info("ğŸ’¡ **[ê°ì„± ì¶”ì²œ]** " + " ".join(recs))

        else:
             st.info("ğŸ’– ê°ì„± ì ìˆ˜ (S/I)\n\nTAB2ì—ì„œ 'ê°ì„± ì‚¬ì „'ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹¤í–‰í•˜ë©´ í™œì„±í™”ë©ë‹ˆë‹¤.")
        
        st.divider()

    # --- 4. LLM ì¶”ì²œ/ìƒì„± (ë©”ì¸ ë¡œì§ ìŠ¤ìœ„ì¹˜) ---
    
    # [ìˆ˜ì •] íŒŒì¸íŠœë‹ ëª¨ë“œ ì‹œ UI/ë²„íŠ¼ ë³€ê²½
    if is_ft_model_ready:
        st.subheader("2) ğŸ¤– AI ì œëª© ìƒì„±ê¸° (íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš© ì¤‘)")
        st.caption(f"íŒŒì¸íŠœë‹ëœ ëª¨ë¸({FINETUNED_MODEL_ID[:20]}...)ì´ ë¶„ì„ ëŒ€ì‹  **ì œëª©ì„ ì§ì ‘ ìƒì„±**í•©ë‹ˆë‹¤.")
        topk = st.slider("ìƒì„±í•  ì œëª© ê°œìˆ˜ (Top-K)", 3, 10, 5) # ì¶”ì²œ ê°œìˆ˜ë¥¼ ì œëª© ê°œìˆ˜ë¡œ ë³€ê²½
        btn_label = "âœ¨ AI ì œëª© ìƒì„± ì‹œì‘"
    else:
        st.subheader("2) LLM ë¦¬ë­ì»¤ (í†µê³„ ê¸°ë°˜ í›„ë³´ ì‚¬ìš© ì¤‘)")
        topk = st.slider("ì¶”ì²œ ê°œìˆ˜ (Top-K)", 3, 15, 8)
        btn_label = "âœ¨ ë¬¸ë§¥í˜• ìš©ì–´ ì¶”ì²œ ìƒì„±"
        
    btn = st.button(btn_label, disabled=not LLM_OK)

    if btn:
        require_llm()
        if not full_draft.strip():
            st.warning("ì œëª©ì´ë‚˜ ë³¸ë¬¸ ì´ˆì•ˆ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            with st.spinner("LLMì´ ì œëª©ì„ ìƒì„±/ì„ ë³„ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    if is_ft_model_ready:
                        # ===== [ì‹ ê·œ] íŒŒì¸íŠœë‹ ëª¨ë¸ í˜¸ì¶œ ë¡œì§ =====
                        topic_name_current = topic_name if topic_name != "ë¯¸ë¶„ë¥˜" else "ì¼ë°˜"
                        recs = []
                        system_prompt = ("ë‹¹ì‹ ì€ ì œì‹œëœ ì£¼ì œì™€ ë³¸ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ë…ìì˜ ì°¸ì—¬ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” ì„±ê³¼í˜• ì œëª©ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤. "
                        "ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ë‹¤ìŒ 3ê°€ì§€ ìŠ¤íƒ€ì¼ì„ í˜¼í•©í•˜ì—¬ ìƒì„±í•˜ê³ , ë²ˆí˜¸ë§Œ ë¶™ì—¬ ì¶œë ¥í•˜ì„¸ìš”:\n"
                            "1. í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬ì˜ ê°„ê²°í•œ ëª…ì‚¬í˜• ì œëª©\n"
                            "2. í˜¸ê¸°ì‹¬ì„ ìœ ë°œí•˜ëŠ” ì§ˆë¬¸í˜• ì œëª©\n"
                            "3. ìˆ«ì/ë¦¬ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ìš”ì•½í˜• ì œëª©")
                        user_prompt = f"ì£¼ì œ: {topic_name_current}, ë³¸ë¬¸: {draft_body}\n\nìš”ì²­ ê°œìˆ˜: {topk}ê°œ"
                        
                        resp = client.chat.completions.create(
                            model=FINETUNED_MODEL_ID, # â˜…â˜…â˜… íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ID ì‚¬ìš©
                            messages=[
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": user_prompt}
                            ],
                            n=1, # ìŠ¬ë¼ì´ë”ë¡œ ë°›ì€ Kê°œ ìƒì„± ìš”ì²­
                            temperature=0.7 # ì°½ì˜ì„±ì„ ìœ„í•´ ì˜¨ë„ë¥¼ ì•½ê°„ ë†’ì„
                        )

# [ì‹ ê·œ] LLM ì‘ë‹µì„ í…ìŠ¤íŠ¸ë¡œ ê°€ì ¸ì™€ì„œ ë²ˆí˜¸/ì‰¼í‘œ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
                        full_response_text = resp.choices[0].message.content.strip()
                        lines = [t.strip() for t in full_response_text.split('\n') if t.strip()]
                        potential_titles = []
                        for line in lines:
                            # ì‰¼í‘œ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬ ì‹œë„ (LLMì´ í•œ ì¤„ì— ì—¬ëŸ¬ ê°œ ë¶™ì—¬ ë„£ëŠ” ê²½ìš° ëŒ€ë¹„)
                            if ',' in line:
                                potential_titles.extend([t.strip() for t in line.split(',') if t.strip()])
                            else:
                                potential_titles.append(line.strip())
                        CLEAN_PATTERN = regx.compile(r'^\s*[\d\.\:]\s*|^\s*\[.*?\]\s*|\s*[\d\.\:]\s*')
                        for title_text_raw in potential_titles:
                            title_text = title_text_raw.strip()
                            
                            # ë²ˆí˜¸ ë° ë…¸ì´ì¦ˆ ì œê±° íŒ¨í„´ ì ìš©
                            title_text = CLEAN_PATTERN.sub('', title_text)
                            if title_text and len(recs) < topk: # ìµœëŒ€ topk ê°œìˆ˜ë§Œ ì‚¬ìš©
                                recs.append({
                                    "term": title_text,
                                    "category": f"AI ìƒì„± ì œëª© {len(recs)+1}",
                                    "why": "íŒŒì¸íŠœë‹ ëª¨ë¸ì´ í•™ìŠµí•œ 'Good' ì½˜í…ì¸  íŒ¨í„´ ê¸°ë°˜. (ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ í˜¼í•©)",
                                    "where_to_add": "ì œëª©",
                                    "insertion_example": f"AI ìƒì„± ì œëª©: {title_text}",
                                    "expected_effect": "ê³¼ê±° ì„±ê³¼ ë°ì´í„°ë¥¼ ë°˜ì˜í•˜ì—¬ CTR/ì°¸ì—¬ìœ¨ ê·¹ëŒ€í™”.",
                                    "cautions": "ì›ë³¸ ëª¨ë¸ì˜ ì°½ì˜ì„±ì´ ë°˜ì˜ë˜ì–´ ë¬¸ë§¥ì„ ì¬ê²€í† í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                                })
                        
                        # [ë³´ì™„ ë¡œì§] ì •ê·œ í‘œí˜„ì‹ì— ì‹¤íŒ¨í–ˆê±°ë‚˜ LLMì´ JSONìœ¼ë¡œ ì‘ë‹µí–ˆì„ ê²½ìš° ì²˜ë¦¬
                        if not recs and full_response_text:
                            # ì‰¼í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” í´ë°± ë¡œì§ (LLMì´ ë²ˆí˜¸ ì—†ì´ ì‰¼í‘œë¡œë§Œ êµ¬ë¶„í–ˆì„ ë•Œ)
                            fallback_titles = [t.strip() for t in full_response_text.split(',') if t.strip()]
                            for i, title_text in enumerate(fallback_titles[:topk], 1):
                                if title_text:
                                    recs.append({
                                        "term": title_text.strip(),
                                        "category": f"AI ìƒì„± ì œëª© {len(recs)+1}",
                                        "why": "LLM ì‘ë‹µì„ ì‰¼í‘œ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•¨ (ëª¨ë¸ ì¶œë ¥ í˜•ì‹ ì˜¤ë¥˜ ë³´ì •).",
                                        "where_to_add": "ì œëª©",
                                        "insertion_example": f"AI ìƒì„± ì œëª©: {title_text}",
                                        "expected_effect": "LLM ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ ë³´ì •.",
                                        "cautions": "ì›ë³¸ ëª¨ë¸ì˜ ì°½ì˜ì„±ì´ ë°˜ì˜ë˜ì–´ ë¬¸ë§¥ì„ ì¬ê²€í† í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                                    })
                        if len(recs) != topk:
                            st.warning(f"âš ï¸ ëª¨ë¸ì´ ìš”ì²­ëœ ì œëª© ê°œìˆ˜({topk}ê°œ)ë¥¼ ëª¨ë‘ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì‹¤ì œ ìƒì„±: {len(recs)}ê°œ). í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•˜ê±°ë‚˜ í•™ìŠµ ë°ì´í„°ë¥¼ ë³´ê°•í•˜ì„¸ìš”.")
                        st.session_state["last_recs"] = recs
                        st.session_state["last_recs_time"] = time.strftime("%Y-%m-%d %H:%M:%S")

                    else:
                        # ===== ê¸°ì¡´ LLM ë¦¬ë­ì»¤ í˜¸ì¶œ ë¡œì§ =====
                        recs = llm_rerank_or_generate(
                            draft_title=draft_title, 
                            draft_body=draft_body, 
                            candidates=candidates, 
                            topk=topk, 
                            audience=audience, 
                            tone=tone,
                            # [ìˆ˜ì •] temperature ì¸ìë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
                            temperature=0.2, # 0.2ëŠ” ì•ˆì „í•œ ê¸°ë³¸ê°’ì…ë‹ˆë‹¤.
                            topic_name=topic_name, 
                            use_finetuned=False,
                            ft_model_id=FINETUNED_MODEL_ID
                        )
                        st.session_state["last_recs"] = recs
                        st.session_state["last_recs_time"] = time.strftime("%Y-%m-%d %H:%M:%S")

                    st.success("ì¶”ì²œ ì™„ë£Œ!")
                    st.session_state["last_draft"] = full_draft
                    st.session_state["last_candidates"] = list(candidates)
                except Exception as e:
                    st.error(str(e))
            st.markdown("---")
            st.subheader("DEBUG: last_recs ì„¸ì…˜ ê°’")
            # st.write()ëŠ” print()ì²˜ëŸ¼ ë³€ìˆ˜ì˜ ë‚´ìš©ì„ í™”ë©´ì— ì¶œë ¥í•´ì¤ë‹ˆë‹¤.
            st.write(st.session_state.get("last_recs")) 
            st.markdown("---")
# [ìˆ˜ì •í•  ìœ„ì¹˜] TAB1 ì„¹ì…˜, if st.session_state.get("last_recs"): ë¸”ë¡ ì „ì²´ êµì²´

            if st.session_state.get("last_recs"):
                
                # [ìˆ˜ì •] ëª¨ë“œì— ê´€ê³„ì—†ì´ ê²°ê³¼ë¥¼ ì¤„ê¸€ ë¦¬ìŠ¤íŠ¸ë¡œ ë°”ë¡œ ì¶œë ¥
                if is_ft_model_ready:
                    st.subheader("âœ… AI ìƒì„± ì œëª© í›„ë³´ (Top-K)")
                    result_label = "AI ìƒì„± ì œëª© í›„ë³´"
                else:
                    st.subheader("âœ… LLM ë¦¬ë­ì»¤ ì¶”ì²œ ë‹¨ì–´")
                    result_label = "ë¬¸ë§¥ ì¶”ì²œ ë‹¨ì–´ í›„ë³´"

                st.markdown(f"**ì´ {len(st.session_state['last_recs'])}ê°œì˜ {result_label}ê°€ ìˆìŠµë‹ˆë‹¤.**")
                st.markdown("---")
                
                # [ì‹ ê·œ] ê²°ê³¼ë¥¼ ë²ˆí˜¸ ë§¤ê¸´ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥
                output_lines = []
                for i, r in enumerate(st.session_state["last_recs"], 1):
                    term_text = r.get('term', '(ìš©ì–´)').strip()
                    category_text = r.get('category', '')
                    
                    main_line = f"**{i}. {term_text}**"
                    
                    # ë¦¬ë­ì»¤ ëª¨ë“œì¼ ê²½ìš°ì—ë§Œ ë¶„ë¥˜ ì •ë³´ë¥¼ ì¶”ê°€
                    if not is_ft_model_ready:
                        main_line += f" (`{category_text}`)"

                    output_lines.append(main_line)

                # ì¤„ê¸€ ë¦¬ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì¶œë ¥
                st.markdown('\n'.join(output_lines))

                st.markdown("---")
                st.caption("â€¢ ìì„¸í•œ ì¶”ì²œ ì´ìœ  ë° ì˜ˆì‹œëŠ” LLM í˜¸ì¶œ ì‹œ ë©”íƒ€ë°ì´í„°ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

            else:
                st.info("ì•„ì§ ìƒì„±ëœ ì¶”ì²œì´ ì—†ìŠµë‹ˆë‹¤. ìœ„ ë²„íŠ¼ìœ¼ë¡œ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")

            st.markdown("---")
            st.caption("â€¢ ì¶”ì²œ ìš©ì–´ëŠ” TAB2ì—ì„œ ë¶„ì„í•œ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ 'í† í”½ë³„ í•µì‹¬ ë‹¨ì–´' í’€ì—ì„œ ì„ ë³„ë©ë‹ˆë‹¤.")

        st.markdown("---")
        st.caption("â€¢ ì¶”ì²œ ìš©ì–´ëŠ” TAB2ì—ì„œ ë¶„ì„í•œ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ 'í† í”½ë³„ í•µì‹¬ ë‹¨ì–´' í’€ì—ì„œ ì„ ë³„ë©ë‹ˆë‹¤.")

# ================= TAB2 =================
with TAB2:
    st.header("ğŸ“Š ë°ì´í„° ì—…ë¡œë“œ ë° ì„±ê³¼ ë¶„ì„")
    st.caption("ì½˜í…ì¸  ì„±ê³¼ë¥¼ ë¶„ì„í•˜ë ¤ë©´ ì•„ë˜ ë‘ ê°€ì§€ CSV íŒŒì¼ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤.")
    c1, c2 = st.columns(2)
    with c1:
        f_content = st.file_uploader("ğŸ“ (1) ì½˜í…ì¸  ìƒì„¸ CSV (article_id, title, content, date í¬í•¨)", type=["csv"], key="content")
    with c2:
        f_metrics = st.file_uploader("ğŸ“ˆ (2) ì„±ê³¼ ì¸¡ì • CSV (article_id, views_total, likes, comments, period í¬í•¨)", type=["csv"], key="metrics")

    st.markdown("---")
    st.subheader("âš™ï¸ ë¶„ì„ ì„¤ì •")
    c3, c4, c5 = st.columns(3)
    lda_topics = c3.number_input("ì£¼ì œ ë¶„ë¥˜ ê°œìˆ˜ (LDA í† í”½ ìˆ˜)", min_value=5, max_value=40, value=10, step=1)
    c4.markdown("**ì½˜í…ì¸  ë§¤ë ¥ ì ìˆ˜ ê°€ì¤‘ì¹˜** (ì´í•© 1.0)")
    wv = c4.slider("ì¡°íšŒìˆ˜ ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.4, 0.05, key="wv_slider")
    wl = c4.slider("ì¢‹ì•„ìš” ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.4, 0.05, key="wl_slider")
    wc = c4.slider("ëŒ“ê¸€ ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.2, 0.05, key="wc_slider")
    
    f_lex = c5.file_uploader("ğŸ’– ê°ì„± ì‚¬ì „ CSV (ì„ íƒ: word,score)", type=["csv"], key="lex")

    st.session_state.setdefault('analysis_done', False)

    def prepare_by_mode(df_in: pd.DataFrame, mode_cfg: dict, lda_topics_ui: int):
        if mode_cfg["sample_n"]:
            n = min(mode_cfg["sample_n"], len(df_in))
            df_work = df_in.sample(n=n, random_state=42).reset_index(drop=True)
            st.info(f"ìƒ˜í”Œë§ ì‚¬ìš©: **{n}í–‰**ìœ¼ë¡œ ì¶•ì†Œ(ì›ë³¸ {len(df_in)}).")
        else:
            df_work = df_in.copy()
        
        n_topics = mode_cfg["lda_topics"] if mode_cfg["lda_topics"] > 0 else int(lda_topics_ui) 

        lda_kwargs = dict(
            n_topics=n_topics,
            max_features=mode_cfg.get("max_features"), # [ìˆ˜ì •] .get()ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
            batch_size=mode_cfg["batch_size"],
            n_epochs=mode_cfg["n_epochs"],
        )
        clf_kwargs = dict(
            epochs=mode_cfg["clf_epochs"],
            batch_size=mode_cfg["clf_batch"],
            ngram_range=mode_cfg["ngram_range"],
        )
        return df_work, lda_kwargs, clf_kwargs

    @st.cache_resource(show_spinner=False)
    def cached_lda_run(texts_tuple, n_topics, max_features, batch_size, n_epochs):
        # [ìˆ˜ì •] max_featuresëŠ” í˜„ì¬ ì‚¬ìš© ì•ˆí•¨ (min_df/max_df)
        return run_lda_topics_streaming(
            list(texts_tuple), n_topics=n_topics, 
            max_features=None, 
            batch_size=batch_size, n_epochs=n_epochs
        )

    if f_content is not None and f_metrics is not None:
        try:
            # [ìˆ˜ì •] íŒŒì¼ì´ ë°”ë€Œë©´ analysis_doneì„ Falseë¡œ ë¦¬ì…‹
            is_new_file = False
            if (st.session_state.get('f_content_name') != f_content.name) or \
               (st.session_state.get('f_metrics_name') != f_metrics.name):
                st.session_state['analysis_done'] = False
                st.session_state['f_content_name'] = f_content.name
                st.session_state['f_metrics_name'] = f_metrics.name
                is_new_file = True
                st.info("ìƒˆë¡œìš´ íŒŒì¼ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¶„ì„ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

            # [ìˆ˜ì •] ë¶„ì„ì´ ì•„ì§ ì•ˆ ëê±°ë‚˜, ìƒˆ íŒŒì¼ì´ë©´ df_fullì„ ë¡œë“œí•˜ê³  ì„¸ì…˜ì— ì €ì¥
            if not st.session_state['analysis_done'] or is_new_file:
                st.info("ìƒˆë¡œìš´ íŒŒì¼ ê°ì§€. ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤...")
                df_c = coerce_article_id(read_csv_robust(f_content))
                df_m_raw = coerce_article_id(read_csv_robust(f_metrics)) 
                df_c["article_id"] = df_c["article_id"].astype(str)
                df_m_raw["article_id"] = df_m_raw["article_id"].astype(str)

                st.info("ì„±ê³¼ CSV(2)ê°€ ê¸°ê°„ë³„ ë°ì´í„°(long format)ì…ë‹ˆë‹¤. article_id ê¸°ì¤€ìœ¼ë¡œ ì„±ê³¼(views, likes, comments)ë¥¼ **í•©ì‚°(sum)**í•©ë‹ˆë‹¤.")
                metric_cols = ["views_total", "likes", "comments"]
                
                for col in metric_cols:
                    if col in df_m_raw.columns:
                        df_m_raw[col] = pd.to_numeric(df_m_raw[col], errors='coerce').fillna(0)
                
                df_m = df_m_raw.groupby("article_id")[metric_cols].sum().reset_index()
                df = pd.merge(df_c, df_m, on="article_id", how="inner")
                
                if 'date' in df.columns:
                    df['date'] = pd.to_datetime(df['date'], errors='coerce')
                
                # ì›ë³¸ + ì„±ê³¼ + ë¼ë²¨ì„ ì„¸ì…˜ì— ì €ì¥
                df = build_engagement(df, w_views=wv, w_likes=wl, w_comments=wc)
                df = label_quality_by_quantile(df, col="engagement", low_q=0.33, high_q=0.66)
                
                st.session_state['df_for_analysis'] = df.copy() 
                st.session_state['df_m_raw_for_viz'] = df_m_raw.copy() # ì‹œê°í™”ìš© ì›ë³¸ ì €ì¥
                st.success(f"ë°ì´í„° ë³‘í•© ë° ë¼ë²¨ë§ ì™„ë£Œ: {len(df)} ê±´. (ë¶„ì„ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”)")
            
            # [ìˆ˜ì •] ì„¸ì…˜ì—ì„œ df_fullì„ ë¡œë“œí•˜ì—¬ ë“±ê¸‰ í™•ì¸ì„ í‘œì‹œ (ë§¤ë²ˆ)
            df_full_display = st.session_state.get('df_for_analysis')
            if df_full_display is not None and not df_full_display.empty:
                st.subheader("1. ì½˜í…ì¸  ë“±ê¸‰ í™•ì¸")
                st.caption("ì½˜í…ì¸  ë§¤ë ¥ ì ìˆ˜(Total Engagement)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 33%ëŠ” 'ìƒ (good)', í•˜ìœ„ 33%ëŠ” 'í•˜ (bad)'ë¡œ ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤.")
                grade_counts = df_full_display["quality_label"].value_counts().rename({"good": "ìƒ (Good)", "medium": "ì¤‘ (Medium)", "bad": "í•˜ (Bad)"})
                st.dataframe(grade_counts.to_frame(name="ì½˜í…ì¸  ìˆ˜"), use_container_width=True)
            else:
                st.info("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë°ì´í„° ë“±ê¸‰ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


            colm1, colm2 = st.columns(2)
            do_quick = colm1.button("âš¡ï¸ ë¹ ë¥¸ ë¶„ì„ (ìƒ˜í”Œ/ê²½ëŸ‰ ëª¨ë¸)", use_container_width=True)
            do_full = colm2.button("ğŸ”¬ ì •ë°€ ë¶„ì„ (ì „ì²´/ê³ ì •ë°€ ëª¨ë¸)", use_container_width=True)

            if do_quick or do_full:
                # ----------------- ë¶„ì„ ì‹¤í–‰ ë¸”ë¡ ì‹œì‘ -----------------
                mode = "quick" if do_quick else "full"
                cfg = MODE_CFG[mode] 
                
                # [ìˆ˜ì •] df_workëŠ” df_full(ì„¸ì…˜)ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
                df_full_for_prep = st.session_state.get('df_for_analysis')
                if df_full_for_prep is None or df_full_for_prep.empty:
                    st.error("ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: df_for_analysisê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ì„¸ìš”.")
                    st.stop()

                df_work, lda_kw, clf_kw = prepare_by_mode(df_full_for_prep, cfg, lda_topics)

                # ===== LDA =====
                st.subheader("2. ì£¼ì œ(í† í”½) ë¶„ë¥˜ ë° ë¶„ì„")
                with st.spinner(f"LDA({mode}) ì£¼ì œ ë¶„ì„ ì‹¤í–‰ ì¤‘â€¦ (ë¶ˆìš©ì–´ ë° min_df/max_df ì ìš©)"):
                    df_sig = tuple(df_work["content"].fillna("").tolist())
                    df_topic, vect, lda, W = cached_lda_run(tuple(df_sig), **lda_kw)
                df_work["topic"] = df_topic["topic"]
                st.write("ì£¼ì œ ë¶„ë¥˜ ê²°ê³¼ (ìƒ˜í”Œ):", df_work[["article_id","topic","title"]].head(10))

                topics_top_words = get_topic_top_words(lda, vect, topn=8)
                st.info("í† í”½ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM/íœ´ë¦¬ìŠ¤í‹±ì„ ì‚¬ìš©í•´ ì£¼ì œ ì´ë¦„ê³¼ ì„¤ëª…ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.")
                with st.spinner("LLM/íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í† í”½ ë¼ë²¨ë§ ì¤‘..."):
                    topic_labels = llm_name_topics(topics_top_words)

                st.session_state["topic_labels"] = topic_labels
                st.session_state["lda_vect"] = vect 
                st.session_state["lda_model"] = lda
                
                if 'df_for_analysis' in st.session_state and st.session_state['df_for_analysis'] is not None:
                    full_texts = (st.session_state['df_for_analysis']["title"].fillna("") + " " + st.session_state['df_for_analysis']["content"].fillna("")).tolist()
                    full_X = vect.transform(full_texts)
                    full_topics = lda.transform(full_X).argmax(axis=1)
                    st.session_state['df_for_analysis']['topic'] = full_topics # ì„¸ì…˜ì— 'topic' ì»¬ëŸ¼ ì¶”ê°€
                    st.info("ì „ì²´ ë°ì´í„°(TAB1/TAB4ìš©)ì— í† í”½ ë¶„ë¥˜ ì ìš© ì™„ë£Œ.")
                else:
                    st.warning("ì „ì²´ ë°ì´í„°(df_for_analysis)ê°€ ì„¸ì…˜ì— ì—†ì–´ í† í”½ì„ í• ë‹¹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


                # ===== ë¶„ë¥˜ê¸° í•™ìŠµ =====
                st.subheader("3. ì½˜í…ì¸  ë“±ê¸‰ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
                df_train = df_work[df_work["quality_label"] != "medium"]
                st.info(f"í•™ìŠµ ë°ì´í„°ì…‹: 'ìƒ(Good)'ê³¼ 'í•˜(Bad)' {len(df_train)}ê±´ ì‚¬ìš©.")
                
                with st.spinner(f"SGD ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘â€¦ (ë¶ˆìš©ì–´ ì ìš©)"):
                    clf_pack = train_logreg_with_progress(
                        texts = df_train["title"].fillna("") + " " + df_train["content"].fillna(""),
                        labels = df_train["quality_label"],
                        stoplist=None, 
                        **clf_kw
                    )
                st.session_state["clf_pack"] = clf_pack
                st.success("ë“±ê¸‰ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
                
                # ===== [ìˆ˜ì •] í† í”½ ë‹¨ì–´ ì€í–‰ êµ¬ì¶• (LogReg) =====
                st.subheader("4. í† í”½ë³„ í•µì‹¬ ë‹¨ì–´ ì€í–‰ êµ¬ì¶• (TAB1 ì¶”ì²œ ê¸°ë°˜)")
                with st.spinner("í† í”½ë³„ ì„±ê³¼ ìš°ìˆ˜/ì €ì¡° ë‹¨ì–´ ë¶„ì„ ì¤‘â€¦ (LogReg ê³„ìˆ˜ ì ìš©)"):
                    df_full_with_topic = st.session_state.get('df_for_analysis') # 'topic'ì´ ë°©ê¸ˆ ì¶”ê°€ë¨
                    if df_full_with_topic is not None and 'topic' in df_full_with_topic.columns:
                        topic_term_bank = build_topic_term_bank_logreg(df_full_with_topic, topn=50)
                    else:
                        # df_full_with_topicì´ ì—†ê±°ë‚˜ 'topic'ì´ ì—†ìœ¼ë©´, ìƒ˜í”Œë§ëœ df_workë¡œ ëŒ€ì‹  ì‹¤í–‰ (ì •í™•ë„ ë‚®ìŒ)
                        st.warning("ì „ì²´ ë°ì´í„°ì— í† í”½ì´ ì—†ì–´, ìƒ˜í”Œë§ëœ ë°ì´í„°ë¡œ ë‹¨ì–´ ì€í–‰ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. (ì •í™•ë„ ì €í•˜)")
                        topic_term_bank = build_topic_term_bank_logreg(df_work, topn=50) 
                    st.session_state["topic_term_bank"] = topic_term_bank
                st.success("í† í”½ ê¸°ë°˜ ìš©ì–´ ì€í–‰(LogReg) êµ¬ì¶• ì™„ë£Œ! (TAB1ì—ì„œ í™œìš© ê°€ëŠ¥)")

                # [ìˆ˜ì •] ê°ì„± ë¶„ì„ê¸° ìƒì„± ë¡œì§ (TAB1ìš©)
                if f_lex is not None:
                    st.subheader("5. ê°ì„± ë¶„ì„ê¸° ìƒì„± (TAB1ìš©)")
                    with st.spinner("ê°ì„± ì‚¬ì „ì„ ì²˜ë¦¬í•˜ì—¬ TAB1ì—ì„œ ì‚¬ìš©í•  ë¶„ì„ê¸°ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                        try:
                            f_lex.seek(0)
                            lex_df = read_csv_robust(f_lex)
                            if not set(["word","score"]).issubset(lex_df.columns):
                                st.warning("ê°ì„± ì‚¬ì „ì— 'word', 'score' ì»¬ëŸ¼ì´ ì—†ì–´ S/I ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                                st.session_state['sentiment_pack'] = None
                            else:
                                lex_dict = dict(zip(lex_df["word"].astype(str), lex_df["score"].astype(float)))
                                senti_cv = CountVectorizer(min_df=1, stop_words=STOPWORDS_KO)
                                texts = (df_full_for_prep["title"].fillna("") + " " + df_full_for_prep["content"].fillna("")).tolist() 
                                senti_cv.fit(texts) 
                                
                                # [ìˆ˜ì •] 'Good' ê¸€ì˜ í‰ê·  S/I ì ìˆ˜ ê³„ì‚° ë° NaN ë°©ì–´
                                df_work_senti = compute_sentiment_SI(df_work, senti_cv, lex_dict)
                                avg_s_good = df_work_senti[df_work_senti['quality_label'] == 'good']['S'].mean()
                                avg_i_good = df_work_senti[df_work_senti['quality_label'] == 'good']['I'].mean()

                                # [ìˆ˜ì •] NaNì¸ ê²½ìš° 0.0ìœ¼ë¡œ í´ë°±
                                target_s_val = float(avg_s_good) if pd.notna(avg_s_good) else 0.0
                                target_i_val = float(avg_i_good) if pd.notna(avg_i_good) else 0.0
                                
                                st.session_state['sentiment_pack'] = {
                                    'lex': lex_dict, 
                                    'cv': senti_cv,
                                    'target_s': target_s_val, 
                                    'target_i': target_i_val
                                }
                                st.session_state['lex_file_name'] = f_lex.name
                                st.success(f"ê°ì„± ë¶„ì„ê¸°(S/I)ê°€ TAB1ì„ ìœ„í•´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ëª©í‘œ S: {target_s_val:.2f}, ëª©í‘œ I: {target_i_val:.2f})")
                        except Exception as e:
                            st.error(f"ê°ì„± ì‚¬ì „ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                            st.session_state['sentiment_pack'] = None
                else:
                    st.session_state['sentiment_pack'] = None 

                # [ìˆ˜ì •] ë¶„ì„ ì™„ë£Œ í”Œë˜ê·¸ ë° ì‹œê°í™”ìš© ë°ì´í„° ì €ì¥
                st.session_state['analysis_done'] = True
                st.session_state['df_work_for_viz'] = df_work.copy()
                # [ìˆ˜ì •] df_m_raw_for_vizëŠ” ë²„íŠ¼ ë°–ì—ì„œ ì´ë¯¸ ì €ì¥ë¨
                st.session_state['topic_labels_for_viz'] = topic_labels
                st.rerun() 
                
                # ----------------- ë¶„ì„ ì‹¤í–‰ ë¸”ë¡ ë -----------------

            # [ìˆ˜ì •] ì‹œê°í™” ë¸”ë¡ ì „ì²´ë¥¼ 'analysis_done' í”Œë˜ê·¸ ê¸°ë°˜ìœ¼ë¡œ ë°–ìœ¼ë¡œ ì´ë™
            if st.session_state.get('analysis_done', False):
                
                # [ìˆ˜ì •] ì„¸ì…˜ì—ì„œ ì‹œê°í™”ìš© ë°ì´í„° ë¡œë“œ
                df_work_viz = st.session_state.get('df_work_for_viz')
                df_m_raw_viz = st.session_state.get('df_m_raw_for_viz')
                topic_labels_viz = st.session_state.get('topic_labels_for_viz', {})
                clf_pack_viz = st.session_state.get('clf_pack') 
                senti_pack_viz = st.session_state.get('sentiment_pack')

                if df_work_viz is None or df_m_raw_viz is None or topic_labels_viz is None or clf_pack_viz is None:
                    st.error("ì‹œê°í™” ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. ë¶„ì„ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
                    st.stop()

                st.markdown("---")
                st.header("ğŸ”¬ ì¶”ê°€ ë¶„ì„ ì‹œê°í™”")

                # [ìˆ˜ì •] í† í”½ í•„í„°
                topic_names_list = ["ì „ì²´ (All)"] + [v.get('name', k) for k,v in topic_labels_viz.items()]
                filter_topic_name = st.selectbox("ğŸ”¬ ì‹œê°í™” í† í”½ í•„í„°", topic_names_list)

                # [ìˆ˜ì •] í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„± (df_viz)
                topic_names_map = {int(k.split(' ')[1]): v.get('name', k) for k, v in topic_labels_viz.items()}
                
                if 'topic_name' not in df_work_viz.columns:
                     df_work_viz['topic_name'] = df_work_viz['topic'].map(topic_names_map).fillna('ê¸°íƒ€')

                if filter_topic_name == "ì „ì²´ (All)":
                    df_viz = df_work_viz
                else:
                    df_viz = df_work_viz[df_work_viz['topic_name'] == filter_topic_name]

                # [ì‹œê°í™” 1] í† í”½ë³„ ì„±ê³¼ (ì´ ì°¨íŠ¸ëŠ” í•„í„° ì ìš© ì•ˆí•¨)
                st.subheader("A. ì£¼ì œë³„ ì„±ê³¼ ë¶„í¬")
                try:
                    fig_topic_box = px.box(
                        df_work_viz, 
                        x='topic_name', 
                        y='engagement', 
                        color='topic_name',
                        title='ì£¼ì œ(í† í”½)ë³„ ì½˜í…ì¸  ë§¤ë ¥ ì ìˆ˜(Total Engagement) ë¶„í¬',
                        labels={'topic_name': 'ì£¼ì œëª…', 'engagement': 'ì½˜í…ì¸  ë§¤ë ¥ ì ìˆ˜(ì´í•©)'}
                    )
                    st.plotly_chart(fig_topic_box, use_container_width=True)
                except Exception as e:
                    st.error(f"í† í”½ ì„±ê³¼ ì‹œê°í™” ì‹¤íŒ¨: {e}")

                # [ì‹œê°í™” 2] ì„±ê³¼ í•µì‹¬ í‚¤ì›Œë“œ (ì „ì²´ ì£¼ì œ ê¸°ì¤€)
                st.subheader("B. ì„±ê³¼ ì˜ˆì¸¡ í•µì‹¬ í‚¤ì›Œë“œ (ì „ì²´ ì£¼ì œ ê¸°ì¤€)")
                st.caption("â”” ì´ ì°¨íŠ¸ëŠ” 'ì „ì²´ ì£¼ì œ'ì— ëŒ€í•´ í•™ìŠµëœ **ë‹¨ì¼ ëª¨ë¸**ì˜ ê²°ê³¼ì´ë¯€ë¡œ í† í”½ í•„í„°ê°€ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                if clf_pack_viz: 
                    try:
                        good_df = pd.DataFrame(clf_pack_viz['good_terms'], columns=['term', 'score'])
                        bad_df = pd.DataFrame(clf_pack_viz['bad_terms'], columns=['term', 'score'])
                        drivers_df = pd.concat([good_df, bad_df])
                        drivers_df['type'] = np.where(drivers_df['score'] > 0, 'ì„±ê³¼ ìš°ìˆ˜ (Good)', 'ì„±ê³¼ ì €ì¡° (Bad)')

                        fig_drivers = px.bar(
                            drivers_df.sort_values('score'), 
                            x='score', 
                            y='term', 
                            color='type',
                            orientation='h', 
                            title='ì½˜í…ì¸  ë“±ê¸‰ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œ',
                            labels={'term': 'í‚¤ì›Œë“œ', 'score': 'ì˜í–¥ë ¥ ì ìˆ˜ (ê³„ìˆ˜)'},
                            color_discrete_map={'ì„±ê³¼ ìš°ìˆ˜ (Good)': 'blue', 'ì„±ê³¼ ì €ì¡° (Bad)': 'red'}
                        )
                        fig_drivers.update_layout(yaxis_title="í‚¤ì›Œë“œ")
                        st.plotly_chart(fig_drivers, use_container_width=True)
                    except Exception as e:
                        st.error(f"í‚¤ì›Œë“œ ì‹œê°í™” ì‹¤íŒ¨: {e}")

                # [ì‹œê°í™” 3] ê°ì„±ê³¼ ì„±ê³¼ (í•„í„° ì ìš©)
                st.subheader(f"C. ê°ì„±(S/I)ê³¼ ì„±ê³¼ ({filter_topic_name})")
                if f_lex is not None:
                    if senti_pack_viz and senti_pack_viz.get('cv') and senti_pack_viz.get('lex'):
                        if 'S' not in df_viz.columns:
                            with st.spinner(f"({filter_topic_name}) ê°ì„±(S/I) ì ìˆ˜ ê³„ì‚° ì¤‘..."):
                                df_viz = compute_sentiment_SI(df_viz, senti_pack_viz['cv'], senti_pack_viz['lex'])
                        
                        if 'S' in df_viz.columns and df_viz['S'].abs().sum() > 0:
                            fig_senti_scatter = px.scatter(
                                df_viz, 
                                x='S', 
                                y='engagement', 
                                color='quality_label',
                                title=f'ì½˜í…ì¸  ê°ì„±(S)ê³¼ ì„±ê³¼(Total Engagement) ê´€ê³„ ({filter_topic_name})',
                                labels={'S': 'í‰ê·  ê°ì„± ì ìˆ˜ (S)', 'engagement': 'ì½˜í…ì¸  ë§¤ë ¥ ì ìˆ˜(ì´í•©)'},
                                hover_data=['title']
                            )
                            st.plotly_chart(fig_senti_scatter, use_container_width=True)
                        else:
                            st.warning(f"'{filter_topic_name}' í† í”½ì—ì„œ ìœ íš¨í•œ ê°ì„± ì ìˆ˜(S)ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.caption("ğŸ’– 'ê°ì„± ì‚¬ì „ CSV'ë¥¼ ì—…ë¡œë“œí•˜ë©´ ê°ì„±-ì„±ê³¼ ê´€ê³„ ë¶„ì„ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")
                else:
                    st.caption("ğŸ’– 'ê°ì„± ì‚¬ì „ CSV'ë¥¼ ì—…ë¡œë“œí•˜ë©´ ê°ì„±-ì„±ê³¼ ê´€ê³„ ë¶„ì„ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")
                
                # [ì‹œê°í™” 4] ì‹œê°„ëŒ€ë³„ ì„±ê³¼ ì¶”ì„¸ (ì „ì²´ ì£¼ì œ ê¸°ì¤€)
                st.subheader("D. ê¸°ê°„ë³„ ì„±ê³¼ ì¶”ì„¸ (ì „ì²´ ì£¼ì œ ê¸°ì¤€)")
                st.caption("â”” ì´ ì°¨íŠ¸ëŠ” 'ì „ì²´ ì£¼ì œ'ì˜ **í‰ê· ** ì¶”ì„¸ì´ë©°, í† í”½ í•„í„°ê°€ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                if 'period' in df_m_raw_viz.columns: 
                    try:
                        df_trend = df_m_raw_viz.groupby('period')[['views_total', 'likes', 'comments']].mean().reset_index()
                        df_trend['parsed_period'] = pd.to_datetime(df_trend['period'], errors='coerce')
                        df_trend = df_trend.dropna(subset=['parsed_period']).sort_values('parsed_period')
                        
                        if not df_trend.empty:
                            df_trend_melted = df_trend.melt(
                                id_vars='parsed_period', 
                                value_vars=['views_total', 'likes', 'comments'],
                                var_name='Metric', 
                                value_name='Average Value'
                            )
                            
                            fig_time_trend = px.line(
                                df_trend_melted, 
                                x='parsed_period', 
                                y='Average Value', 
                                color='Metric', 
                                title='ê¸°ê°„(Period)ë³„ í‰ê·  ì„±ê³¼(ì¡°íšŒ/ì¢‹ì•„ìš”/ëŒ“ê¸€) ì¶”ì„¸',
                                labels={'parsed_period': 'ê¸°ê°„', 'Average Value': 'í‰ê·  ì„±ê³¼ ê°’', 'Metric': 'ì„±ê³¼ ì§€í‘œ'}
                            )
                            st.plotly_chart(fig_time_trend, use_container_width=True)
                        else:
                            st.warning("ìœ íš¨í•œ 'period' ì»¬ëŸ¼ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì‹œê°„ëŒ€ë³„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"ê¸°ê°„(period) ì»¬ëŸ¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                else:
                    st.caption("ğŸ“… ì„±ê³¼ ì¸¡ì • CSV(2)ì— 'period' ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì‹œê°„ëŒ€ë³„ ì¶”ì„¸ ë¶„ì„ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")
            
        except Exception as e:
            st.error(f"ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")


# ================= TAB3 =================
with TAB3:
    st.header("ğŸ› ï¸ LLM íŒŒì¸íŠœë‹ (ê³ ì„±ëŠ¥ ì œëª© ìƒì„±ê¸°)")
    st.info("ì´ íƒ­ì€ TAB2ì—ì„œ ë¶„ì„í•œ 'Good' ë“±ê¸‰ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´, ì„±ê³¼ê°€ ì¢‹ì€ ì œëª© ìŠ¤íƒ€ì¼ì„ gpt-4o-miniì— í•™ìŠµì‹œí‚µë‹ˆë‹¤.")
    
    # ì„¸ì…˜ì—ì„œ ë°ì´í„° ë¡œë“œ (TAB2ì—ì„œ ë¶„ì„ì´ ì™„ë£Œë˜ì–´ì•¼ í•¨)
    df_full = st.session_state.get('df_for_analysis')
    topic_labels = st.session_state.get('topic_labels', {})
    
    # [ìˆ˜ì •] 'analysis_done' í”Œë˜ê·¸ ë° 'topic' ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ë¡œ KeyError ë°©ì§€
    if not st.session_state.get('analysis_done', False) or df_full is None or df_full.empty or 'topic' not in df_full.columns:
        st.error("âš ï¸ TAB2ì—ì„œ ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•˜ì—¬ 'Good' ë°ì´í„°ì™€ 'í† í”½ ë¼ë²¨'ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ('topic' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.)")
    else:
        st.subheader("1. í•™ìŠµ ë°ì´í„° ì¤€ë¹„")
        
        # 1. 'Good' ë°ì´í„°ë§Œ í•„í„°ë§
        df_good = df_full[df_full['quality_label'] == 'good'].copy()
        
        # 2. í† í”½ ì´ë¦„ ë§¤í•‘
        topic_names_map = {int(k.split(' ')[1]): v.get('name', k) for k,v in topic_labels.items()}
        df_good['topic_name'] = df_good['topic'].map(topic_names_map).fillna('ì¼ë°˜')

        st.markdown(f"**{len(df_good)}** ê±´ì˜ **'Good'** ì½˜í…ì¸ ë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        st.dataframe(df_good[['title', 'topic_name', 'engagement']].head())

        if st.button("ğŸš€ íŒŒì¸íŠœë‹ ì‘ì—… ìƒì„± ë° ì‹œì‘ (JSONL ìƒì„± í¬í•¨)"):
            if len(df_good) < 10:
                st.error(f"í•™ìŠµ ìƒ˜í”Œì´ 10ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤ (í˜„ì¬ {len(df_good)}ê°œ). íŒŒì¸íŠœë‹ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            elif not LLM_OK:
                st.error("OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                try:
                    with st.spinner("1/4: 'Good' ë°ì´í„°ë¡œ JSONL í•™ìŠµ íŒŒì¼ ìƒì„± ì¤‘..."):
                        training_data = []
                        for idx, row in df_good.iterrows():
                            # [ìˆ˜ì •] NaN (float) ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ str()ë¡œ ê°ì‹¸ê³  ë¹ˆ ê°’ í™•ì¸
                            content_str = str(row.get('content', '') or '').strip()
                            title_str = str(row.get('title', '') or '').strip()
                            topic_str = str(row.get('topic_name', 'ì¼ë°˜'))

                            # [ìˆ˜ì •] ë³¸ë¬¸ì´ë‚˜ ì œëª©ì´ ë¹„ì–´ìˆìœ¼ë©´ í•™ìŠµì—ì„œ ì œì™¸
                            if not content_str or not title_str:
                                continue

                            system_prompt = "ë‹¹ì‹ ì€ ì œì‹œëœ ì£¼ì œì™€ ë³¸ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ë…ìì˜ ì°¸ì—¬(ì¡°íšŒìˆ˜, ëŒ“ê¸€)ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” ì„±ê³¼í˜• ì œëª©ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤."
                            user_prompt = f"ì£¼ì œ: {topic_str}, ë³¸ë¬¸: {content_str[:2000]}" # ë³¸ë¬¸ì€ 2000ìë¡œ ì œí•œ (í† í° ì ˆì•½)
                            assistant_completion = title_str

                            messages = [
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": user_prompt},
                                {"role": "assistant", "content": assistant_completion}
                            ]
                            training_data.append({"messages": messages})
                        
                        st.caption(f"âœ… ìµœì¢… í•™ìŠµ ë°ì´í„°: {len(training_data)}ê±´ ìƒì„± ì™„ë£Œ (ë¹ˆ ê°’/NaN ì œì™¸)")

                        # JSONL í˜•ì‹ìœ¼ë¡œ ì¸ë©”ëª¨ë¦¬ íŒŒì¼ ìƒì„±
                        jsonl_output = "\n".join([json.dumps(item, ensure_ascii=False) for item in training_data])
                        bytes_io = io.BytesIO(jsonl_output.encode('utf-8'))
                        bytes_io.name = "train_data.jsonl" # íŒŒì¼ëª… ì§€ì •
                    
                    with st.spinner("2/4: í•™ìŠµ íŒŒì¼(train_data.jsonl)ì„ OpenAIì— ì—…ë¡œë“œ ì¤‘..."):
                        file = client.files.create(
                            file=bytes_io, 
                            purpose='fine-tune'
                        )
                        st.write(f"âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ! (File ID: {file.id})")

                    with st.spinner("3/4: íŒŒì¸íŠœë‹ ì‘ì—…ì„ ìƒì„±í•˜ê³  ëŒ€ê¸°ì—´ì— ë„£ëŠ” ì¤‘..."):
                        job = client.fine_tuning.jobs.create(
                            training_file=file.id, 
                            model=MODEL_CHAT # "gpt-4o-mini-..."
                        )
                        st.session_state['ft_job_id'] = job.id
                        st.write(f"âœ… ì‘ì—… ìƒì„± ì™„ë£Œ! (Job ID: {job.id})")

                    with st.spinner("4/4: ì‘ì—… ìƒíƒœ í™•ì¸ ì¤‘... (ì‹¤ì œ íŠœë‹ì€ ëª‡ ë¶„~ëª‡ ì‹œê°„ ì†Œìš”)"):
                        job_status = client.fine_tuning.jobs.retrieve(job.id)
                        st.success(f"ğŸ‰ íŒŒì¸íŠœë‹ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤! (í˜„ì¬ ìƒíƒœ: {job_status.status})")
                        st.markdown("---")
                        st.markdown("**í–¥í›„ ì‘ì—…:**")
                        st.code(f"client.fine_tuning.jobs.retrieve('{job.id}')")
                        st.markdown("ìœ„ ì½”ë“œë¡œ ì‘ì—… ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”. ìƒíƒœê°€ 'succeeded'ê°€ ë˜ë©´ ì™„ë£Œëœ ê²ƒì…ë‹ˆë‹¤.")
                        st.markdown("ì™„ë£Œë˜ë©´ **[OpenAI íŒŒì¸íŠœë‹ í˜ì´ì§€](https://platform.openai.com/finetune)**ì—ì„œ 'Fine-tuned model' ID (ì˜ˆ: `ft:gpt-4o-mini...`)ë¥¼ ë³µì‚¬í•˜ì„¸ìš”.")

                except Exception as e:
                    st.error(f"íŒŒì¸íŠœë‹ ì‹¤íŒ¨: {e}")
                    
    if 'ft_job_id' in st.session_state:
        st.subheader("í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì‘ì—… ìƒíƒœ")
        job_id = st.session_state['ft_job_id']
        if st.button("ğŸ”„ ì‘ì—… ìƒíƒœ ìƒˆë¡œê³ ì¹¨"):
            try:
                job_status = client.fine_tuning.jobs.retrieve(job_id)
                st.json(job_status, expanded=False)
                if job_status.fine_tuned_model:
                    st.success(f"ğŸ‰ íŠœë‹ ì™„ë£Œ! ëª¨ë¸ ID: {job_status.fine_tuned_model}")
                    st.session_state['ft_model_id'] = job_status.fine_tuned_model
            except Exception as e:
                st.error(f"ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")

# ================= [ì‹ ê·œ] TAB4 =================
with TAB3:
    st.header("ğŸ”¬ ëª¨ë¸ ê´€ë¦¬ì (Admin)")
    st.info("ì´ íƒ­ì€ `TAB2`ì—ì„œ ë¶„ì„ì´ ì™„ë£Œëœ í›„ í™œì„±í™”ë©ë‹ˆë‹¤. í˜„ì¬ ì ìš©ëœ ëª¨ë¸ì˜ ìƒíƒœì™€ ì„±ëŠ¥ì„ ì ê²€í•©ë‹ˆë‹¤.")

    # ì„¸ì…˜ì—ì„œ ë°ì´í„° ë¡œë“œ
    df_full = st.session_state.get('df_for_analysis')
    topic_bank = st.session_state.get('topic_term_bank')
    clf_pack = st.session_state.get('clf_pack')
    lda_model = st.session_state.get('lda_model')
    lda_vect = st.session_state.get('lda_vect')
    topic_labels = st.session_state.get('topic_labels', {})

    # [ìˆ˜ì •] 'analysis_done' í”Œë˜ê·¸ ë° 'topic' ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ë¡œ KeyError ë°©ì§€
    if not st.session_state.get('analysis_done', False) or df_full is None or df_full.empty or 'topic' not in df_full.columns or clf_pack is None or lda_model is None or topic_bank is None:
        st.error("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. TAB2ì—ì„œ ë¨¼ì € 'ë¹ ë¥¸ ë¶„ì„' ë˜ëŠ” 'ì •ë°€ ë¶„ì„'ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")
    else:
        # --- 1. ë¶ˆìš©ì–´ ---
        st.subheader("1. ë¶ˆìš©ì–´(Stopwords) ê´€ë¦¬")
        with st.expander("í˜„ì¬ ì ìš© ì¤‘ì¸ ê¸°ë³¸ ë¶ˆìš©ì–´ ëª©ë¡ ë³´ê¸°"):
            st.text(f"ì´ {len(STOPWORDS_KO)}ê°œ ë‹¨ì–´:")
            st.json(STOPWORDS_KO)
        
        # [ìˆ˜ì •] 'ë¶ˆìš©ì–´ ì˜ì‹¬' ë¡œì§ ë³€ê²½ (ê³ ë¹ˆë„ ì¼ë°˜ì–´)
        with st.expander("ë¶ˆìš©ì–´ ì˜ì‹¬ ë‹¨ì–´ ë³´ê¸° (ê³ ë¹ˆë„ ì¼ë°˜ ë‹¨ì–´)"):
            st.markdown("í† í”½/ì„±ê³¼ì™€ ê´€ê³„ì—†ì´ **ëª¨ë“  ë¬¸ì„œì— ë„ˆë¬´ ìì£¼ ë“±ì¥**í•˜ëŠ” ë‹¨ì–´(ì˜ˆ: 10% ì´ìƒ)ì…ë‹ˆë‹¤. 'ë¯¸êµ­' ê°™ì€ ê³ ìœ ëª…ì‚¬ë³´ë‹¤ **'ê²ƒì´ë‹¤', 'ìˆë‹¤'** ê°™ì€ ì¼ë°˜ ë‹¨ì–´ê°€ ì—¬ê¸° ëœ¬ë‹¤ë©´ ë¶ˆìš©ì–´ ì¶”ê°€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
            with st.spinner("ëª¨ë“  ë¬¸ì„œì—ì„œ ê³ ë¹ˆë„ ì¼ë°˜ ë‹¨ì–´ë¥¼ ì¶”ì¶œ ì¤‘ì…ë‹ˆë‹¤..."):
                suspected = get_suspected_stopwords(df_full, k=50)
                if suspected:
                    st.warning("ì•„ë˜ ë‹¨ì–´ë“¤ì€ ì´ë¯¸ ê¸°ë³¸ ë¶ˆìš©ì–´(STOPWORDS_KO)ì— í¬í•¨ëœ ê²ƒì„ ì œì™¸í•œ ê³ ë¹ˆë„ ë‹¨ì–´ì…ë‹ˆë‹¤.")
                    st.text(", ".join(suspected))
                else:
                    st.info("ë¶ˆìš©ì–´ ì˜ì‹¬ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # --- 2. í† í”½ ë‹¨ì–´ ì€í–‰ ---
        st.subheader("2. í† í”½ë³„ í•µì‹¬ ë‹¨ì–´ ì€í–‰ (TAB1 ì¶”ì²œ ê¸°ë°˜)")
        # [ìˆ˜ì •] ë‹¨ì–´ ì€í–‰ ìƒì„± ë¡œì§ ë³€ê²½ ì•ˆë‚´ (LogReg)
        st.markdown("""
        [ìˆ˜ì •] ì´ ë‹¨ì–´ ì€í–‰ì€ `build_topic_term_bank_logreg` (ë¡œì§€ìŠ¤í‹± íšŒê·€) í•¨ìˆ˜ë¡œ ìƒì„±ë©ë‹ˆë‹¤.
        - **ì„±ê³¼ ìš°ìˆ˜ ë‹¨ì–´ (Good):** í•´ë‹¹ í† í”½ì˜ 'Good'ì„ ì˜ˆì¸¡í•˜ëŠ” **ë¡œì§€ìŠ¤í‹± íšŒê·€ ê³„ìˆ˜(ì–‘ìˆ˜)**ê°€ ë†’ì€ ë‹¨ì–´ì…ë‹ˆë‹¤. (ì¶”ì²œ)
        - **ì„±ê³¼ ì €ì¡° ë‹¨ì–´ (Bad):** í•´ë‹¹ í† í”½ì˜ 'Bad'ë¥¼ ì˜ˆì¸¡í•˜ëŠ” **ë¡œì§€ìŠ¤í‹± íšŒê·€ ê³„ìˆ˜(ìŒìˆ˜)**ê°€ ë†’ì€ ë‹¨ì–´ì…ë‹ˆë‹¤. (ë¹„ê¶Œì¥)
        - **ë‹¨ìˆœ ë¹ˆë„ ë‹¨ì–´ (All):** ì„±ê³¼ì™€ ë¬´ê´€í•˜ê²Œ í•´ë‹¹ í† í”½ì—ì„œ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ì…ë‹ˆë‹¤.
        """)
        st.caption("â”” ì´ ë‹¨ì–´ ì€í–‰ì€ TAB2ì—ì„œ 'ì „ì²´ ê¸°ê°„' ë°ì´í„°ë¡œ í•™ìŠµëœ **ê³ ì •ëœ ëª¨ë¸**ì˜ ê²°ê³¼ì…ë‹ˆë‹¤. (ìµœê·¼ í•œ ë‹¬ ë™ì  ë‹¨ì–´ëŠ” TAB1 ì°¸ì¡°)")
        
        if topic_labels:
            topic_names_map = {v.get('name', k): int(k.split(' ')[1]) for k,v in topic_labels.items()}
            selected_name = st.selectbox("í™•ì¸í•  í† í”½ ì„ íƒ", list(topic_names_map.keys()))
            
            if selected_name:
                selected_id = topic_names_map[selected_name]
                
                if selected_id not in topic_bank:
                     st.error(f"í† í”½ {selected_id}ê°€ ë‹¨ì–´ ì€í–‰ì— ì—†ìŠµë‹ˆë‹¤. (TAB2 ì¬ì‹¤í–‰ í•„ìš”)")
                else:
                    bank_data = topic_bank[selected_id]
                    # [ìˆ˜ì •] status í‚¤ë¥¼ í™•ì¸í•˜ì—¬ êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ í‘œì‹œ
                    if bank_data.get("status") == "ok":
                        # [ì‹ ê·œ] 'ìƒ˜í”Œ ë¶€ì¡±' ê²½ê³ ê°€ ìˆë‹¤ë©´ ë¨¼ì € í‘œì‹œ
                        if bank_data.get("warning"):
                            st.warning(bank_data.get("warning"))
                        
                        c_g, c_b, c_a = st.columns(3)
                        c_g.dataframe({"ì„±ê³¼ ìš°ìˆ˜ ë‹¨ì–´ (Good)": [w for w,s in bank_data['good'][:20]]})
                        c_b.dataframe({"ì„±ê³¼ ì €ì¡° ë‹¨ì–´ (Bad)": [w for w,s in bank_data['bad'][:20]]})
                        c_a.dataframe({"ë‹¨ìˆœ ë¹ˆë„ ë‹¨ì–´ (All)": [w for w,s in bank_data['all'][:20]]})
                    else:
                        st.error(f"'{selected_name}' í† í”½ì˜ ë‹¨ì–´ ì€í–‰ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n**ì‚¬ìœ :** {bank_data.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")


        # --- 3. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ---
        st.subheader("3. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
        st.info("ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì „ì²´ ë°ì´í„°ë¥¼ 80(í•™ìŠµ)/20(í…ŒìŠ¤íŠ¸)ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ì¬í‰ê°€í•©ë‹ˆë‹¤.")
        
        if st.button("ğŸš€ ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰ (80/20 ë¶„í• )"):
            
            # 1. ë¶„ë¥˜(Classifier) ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
            st.markdown("#### A. ì„±ê³¼ ì˜ˆì¸¡ ëª¨ë¸ (SGDClassifier) ì„±ëŠ¥")
            st.caption("ëª©í‘œ: 'Good' / 'Bad' ë¼ë²¨ì„ ì–¼ë§ˆë‚˜ ì˜ ë§ì¶”ëŠ”ê°€? (ë¶„ë¥˜ ëª¨ë¸)")
            
            with st.spinner("ì„±ê³¼ ì˜ˆì¸¡ ëª¨ë¸ì„ 80/20 ë°ì´í„°ë¡œ ì¬í•™ìŠµ ë° í‰ê°€ ì¤‘..."):
                try:
                    df_trainable = df_full[df_full['quality_label'] != 'medium'].copy()
                    texts = (df_trainable["title"].fillna("") + " " + df_trainable["content"].fillna("")).tolist()
                    labels = df_trainable["quality_label"].tolist()

                    X_train_txt, X_test_txt, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)
                    
                    tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.8, stop_words=STOPWORDS_KO) # [ìˆ˜ì •]
                    X_train_vec = tfidf.fit_transform(X_train_txt)
                    X_test_vec = tfidf.transform(X_test_txt)
                    
                    clf_test = SGDClassifier(loss="log_loss", learning_rate="optimal", alpha=1e-5, random_state=42)
                    clf_test.fit(X_train_vec, y_train)
                    
                    y_pred = clf_test.predict(X_test_vec)
                    
                    # [ìˆ˜ì •] classification_reportë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°›ì•„ DataFrameìœ¼ë¡œ ë³€í™˜
                    report_dict = classification_report(y_test, y_pred, output_dict=True)
                    report_df = pd.DataFrame(report_dict).transpose()
                    st.text("Classification Report (Test Set):")
                    st.dataframe(report_df.round(3)) # [ìˆ˜ì •] í‘œ(DataFrame)ë¡œ í‘œì‹œ
                    
                    cm = confusion_matrix(y_test, y_pred, labels=['good', 'bad'])
                    st.text("Confusion Matrix (Test Set):")
                    st.dataframe(pd.DataFrame(cm, index=['True: Good', 'True: Bad'], columns=['Pred: Good', 'Pred: Bad']))

                except Exception as e:
                    st.error(f"ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            
            st.markdown("---")
            
            # 2. í† í”½(LDA) ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
            st.markdown("#### B. ì£¼ì œ ë¶„ë¥˜ ëª¨ë¸ (LDA) ì„±ëŠ¥")
            st.caption("ëª©í‘œ: ë¬¸ì„œë¥¼ ì–¼ë§ˆë‚˜ ì¼ê´€ì„± ìˆëŠ” ì£¼ì œë¡œ ì˜ ë¬¶ì—ˆëŠ”ê°€? (ë¹„ì§€ë„ í•™ìŠµ)")
            st.warning("""
            **ì¤‘ìš”:** LDAëŠ” ì •ë‹µì´ ì—†ëŠ” ë¹„ì§€ë„ í•™ìŠµì´ë¯€ë¡œ **Accuracy(ì •í™•ë„)ë‚˜ F1-Scoreë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.** ëŒ€ì‹ , ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ 'í™•ì‹ ì„ ê°€ì§€ê³ ' ë¬¸ì„œë¥¼ ë¶„ë¥˜í–ˆëŠ”ì§€, ì£¼ì œê°€ ì–¼ë§ˆë‚˜ ëª…í™•íˆ êµ¬ë¶„ë˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” **Perplexity(í˜¼ì¡ë„)**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            - **Perplexity (í˜¼ì¡ë„):** **ë‚®ì„ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤.** ëª¨ë¸ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
            """)
            
            with st.spinner("LDA ëª¨ë¸ì˜ Perplexity(í˜¼ì¡ë„)ë¥¼ ê³„ì‚° ì¤‘..."):
                try:
                    texts = (df_full["title"].fillna("") + " " + df_full["content"].fillna("")).tolist()
                    X_full = lda_vect.transform(texts)
                    
                    perplexity = lda_model.perplexity(X_full)
                    st.metric("Perplexity (í˜¼ì¡ë„) - ì „ì²´ ë°ì´í„° ê¸°ì¤€", f"{perplexity:,.2f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
                except Exception as e:
                    st.error(f"LDA ì„±ëŠ¥ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")

            st.markdown("---")
            st.markdown("#### C. íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ (RÂ² / MSE)")
            st.error("ë³¸ ì•±ì€ 'Good/Bad'ë¥¼ ë§ì¶”ëŠ” **ë¶„ë¥˜(Classification) ëª¨ë¸**ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, RÂ²(Adjusted R-squared)ë‚˜ MSE ê°™ì€ **íšŒê·€(Regression) ì§€í‘œ**ëŠ” í•´ë‹¹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹  ìœ„ (A)ì˜ **ì •í™•ë„(Accuracy)**ì™€ **F1-Score**ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.")